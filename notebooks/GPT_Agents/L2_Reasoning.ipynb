{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe7aab2-ec07-425f-a884-6269ded96a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key set successfully.\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from helper import set_openai_key, test_openai_api, create_openai_client, print_pretty\n",
    "\n",
    "set_openai_key()\n",
    "\n",
    "test_openai_api()\n",
    "\n",
    "client = create_openai_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33014ae5-d31b-45f7-9516-9968baafeb6b",
   "metadata": {},
   "source": [
    "### Reasoning effort\n",
    "In the examples bellow, the `reasoning_effort` parameter (lovingly referred to as the \"juice\" during the development of these models) is used to give the model guidance on how many reasoning tokens it should generate before creating a response to the prompt. You can specify one of `low`, `medium`, or `high` for this parameter, where low will favor speed and economical token usage, and `high` will favor more complete reasoning at the cost of more tokens generated and slower responses. The default value is `medium`, which is a balance between speed and reasoning accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf90d068-484a-4e7e-a0a6-2a39e1488139",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Write a bash script that takes a matrix represented as a string with \n",
    "format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d3e58-a5f8-4b5d-ac8f-00c92131e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    reasoning_effort=\"medium\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc282a-a4dd-4a13-b8d3-1fe5285167f0",
   "metadata": {},
   "source": [
    "### How reasoning works\n",
    "\n",
    "Reasoning models introduce reasoning tokens in addition to input and output tokens. The models use these reasoning tokens to \"think\", breaking down their understanding of the prompt and considering multiple approaches to generating a response. After generating reasoning tokens, the model produces an answer as visible completion tokens, and discards the reasoning tokens from its context.\n",
    "\n",
    "Here is an example of a multi-step conversation between a user and an assistant. Input and output tokens from each step are carried over, while reasoning tokens are discarded.\n",
    "\n",
    "![img](https://cdn.openai.com/API/images/guides/reasoning_tokens.png)\n",
    "\n",
    "\n",
    "### Managing the context window\n",
    "\n",
    "It's important to ensure there's enough space in the context window for reasoning tokens when creating completions. Depending on the problem's complexity, the models may generate anywhere from a few hundred to tens of thousands of reasoning tokens. The exact number of reasoning tokens used is visible in the usage [object of the chat completion response object](https://platform.openai.com/docs/api-reference/chat/object), under `completion_tokens_details`:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 9,\n",
    "    \"completion_tokens\": 12,\n",
    "    \"total_tokens\": 21,\n",
    "    \"completion_tokens_details\": {\n",
    "      \"reasoning_tokens\": 0,\n",
    "      \"accepted_prediction_tokens\": 0,\n",
    "      \"rejected_prediction_tokens\": 0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93260f68-6349-4f1c-98fb-fa5fa5934d41",
   "metadata": {},
   "source": [
    "### Coding (refactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a3164-dcbf-4dec-a774-b8e5611cb7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Instructions:\n",
    "- Given the React component below, change it so that nonfiction books have red\n",
    "  text. \n",
    "- Return only the code in your reply\n",
    "- Do not include any additional formatting, such as markdown code blocks\n",
    "- For formatting, use four space tabs, and do not allow any lines of code to \n",
    "  exceed 80 columns\n",
    "\n",
    "const books = [\n",
    "  { title: 'Dune', category: 'fiction', id: 1 },\n",
    "  { title: 'Frankenstein', category: 'fiction', id: 2 },\n",
    "  { title: 'Moneyball', category: 'nonfiction', id: 3 },\n",
    "];\n",
    "\n",
    "export default function BookList() {\n",
    "  const listItems = books.map(book =>\n",
    "    <li>\n",
    "      {book.title}\n",
    "    </li>\n",
    "  );\n",
    "\n",
    "  return (\n",
    "    <ul>{listItems}</ul>\n",
    "  );\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d32dff-3c91-470e-9874-f7e843aff667",
   "metadata": {},
   "source": [
    "### Coding (planning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2e930-236a-46d3-a765-93816ceb0d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Instructions:\n",
    "- Given the React component below, change it so that nonfiction books have red\n",
    "  text. \n",
    "- Return only the code in your reply\n",
    "- Do not include any additional formatting, such as markdown code blocks\n",
    "- For formatting, use four space tabs, and do not allow any lines of code to \n",
    "  exceed 80 columns\n",
    "\n",
    "const books = [\n",
    "  { title: 'Dune', category: 'fiction', id: 1 },\n",
    "  { title: 'Frankenstein', category: 'fiction', id: 2 },\n",
    "  { title: 'Moneyball', category: 'nonfiction', id: 3 },\n",
    "];\n",
    "\n",
    "export default function BookList() {\n",
    "  const listItems = books.map(book =>\n",
    "    <li>\n",
    "      {book.title}\n",
    "    </li>\n",
    "  );\n",
    "\n",
    "  return (\n",
    "    <ul>{listItems}</ul>\n",
    "  );\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7c3a1-2aa3-40a1-af1d-8edbfb155dda",
   "metadata": {},
   "source": [
    "### STEM Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323bdc29-120d-4ae3-9ea7-1540b7826e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"\"\"\n",
    "What are three compounds we should consider investigating to \n",
    "advance research into new antibiotics? Why should we consider \n",
    "them?\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
