{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa14c59-9501-464e-aa86-ea20b9e7629e",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "\n",
    "Calculating the BLEU (Bilingual Evaluation Understudy) score involves comparing a generated text (usually a machine translation) to one or more reference texts (human translations). Here’s a step-by-step example to illustrate the calculation.\n",
    "\n",
    "### Example\n",
    "\n",
    "**Reference Sentences:**\n",
    "1. The cat is on the mat.\n",
    "2. A cat is sitting on the mat.\n",
    "\n",
    "**Generated Sentence:**\n",
    "The cat is sitting on the mat.\n",
    "\n",
    "### Step 1: Tokenization\n",
    "First, tokenize the sentences into words (or n-grams).\n",
    "\n",
    "**Reference 1:**  \n",
    "- Tokens: [\"The\", \"cat\", \"is\", \"on\", \"the\", \"mat.\"]\n",
    "\n",
    "**Reference 2:**  \n",
    "- Tokens: [\"A\", \"cat\", \"is\", \"sitting\", \"on\", \"the\", \"mat.\"]\n",
    "\n",
    "**Generated:**  \n",
    "- Tokens: [\"The\", \"cat\", \"is\", \"sitting\", \"on\", \"the\", \"mat.\"]\n",
    "\n",
    "### Step 2: Count n-grams\n",
    "For simplicity, let’s calculate BLEU with unigrams (1-grams) and bigrams (2-grams).\n",
    "\n",
    "#### Unigram Counts\n",
    "- **Generated Unigrams:**  \n",
    "  \"The\", \"cat\", \"is\", \"sitting\", \"on\", \"the\", \"mat.\"\n",
    "  \n",
    "- **Reference Unigrams:**  \n",
    "  From Reference 1: \"The\", \"cat\", \"is\", \"on\", \"the\", \"mat.\"  \n",
    "  From Reference 2: \"A\", \"cat\", \"is\", \"sitting\", \"on\", \"the\", \"mat.\"\n",
    "\n",
    "- **Matched Unigrams:**\n",
    "  - \"The\": 1\n",
    "  - \"cat\": 1\n",
    "  - \"is\": 1\n",
    "  - \"on\": 1\n",
    "  - \"the\": 1\n",
    "  - \"sitting\": 1 (only in generated)\n",
    "  - \"mat.\": 1\n",
    "  \n",
    "Total matched unigrams = 5.\n",
    "\n",
    "#### Bigram Counts\n",
    "- **Generated Bigrams:**  \n",
    "  (\"The\", \"cat\"), (\"cat\", \"is\"), (\"is\", \"sitting\"), (\"sitting\", \"on\"), (\"on\", \"the\"), (\"the\", \"mat.\")\n",
    "\n",
    "- **Reference Bigrams:**  \n",
    "  From Reference 1: (\"The\", \"cat\"), (\"cat\", \"is\"), (\"is\", \"on\"), (\"on\", \"the\"), (\"the\", \"mat.\")  \n",
    "  From Reference 2: (\"A\", \"cat\"), (\"cat\", \"is\"), (\"is\", \"sitting\"), (\"sitting\", \"on\"), (\"on\", \"the\"), (\"the\", \"mat.\")\n",
    "\n",
    "- **Matched Bigrams:**\n",
    "  - (\"The\", \"cat\"): 1\n",
    "  - (\"cat\", \"is\"): 1\n",
    "  - (\"is\", \"sitting\"): 1 (only in generated)\n",
    "  - (\"sitting\", \"on\"): 1 (only in generated)\n",
    "  - (\"on\", \"the\"): 1\n",
    "  - (\"the\", \"mat.\"): 1\n",
    "\n",
    "Total matched bigrams = 5.\n",
    "\n",
    "### Step 3: Calculate Precision\n",
    "- Total generated unigrams = 7  \n",
    "- Total matched unigrams = 5  \n",
    "  \\$\n",
    "  \\text{Unigram Precision} = \\frac{\\text{Matched Unigrams}}{\\text{Total Generated Unigrams}} = \\frac{5}{7} \\approx 0.714\n",
    "  \\$\n",
    "\n",
    "- Total generated bigrams = 6  \n",
    "- Total matched bigrams = 5  \n",
    "  \\$\n",
    "  \\text{Bigram Precision} = \\frac{\\text{Matched Bigrams}}{\\text{Total Generated Bigrams}} = \\frac{5}{6} \\approx 0.833\n",
    "  \\$\n",
    "\n",
    "### Step 4: Calculate BLEU Score\n",
    "The BLEU score is typically calculated using a geometric mean of the precision scores multiplied by a brevity penalty (BP) if the generated sentence is shorter than the reference sentences.\n",
    "\n",
    "For simplicity, let’s assume there’s no brevity penalty here.\n",
    "\n",
    "\\$\n",
    "\\text{BLEU} = \\exp\\left(\\frac{1}{N} \\sum_{n=1}^{N} \\log P_n\\right)\n",
    "\\$\n",
    "\n",
    "Where \\$ P_n \\$ is the precision for n-grams.\n",
    "\n",
    "For unigrams (N=1):\n",
    "\\$\n",
    "\\text{BLEU}_{1} = \\exp(\\log(0.714)) \\approx 0.714\n",
    "\\$\n",
    "\n",
    "For bigrams (N=2):\n",
    "\\$\n",
    "\\text{BLEU}_{2} = \\exp\\left(\\frac{1}{2}(\\log(0.714) + \\log(0.833))\\right) \\approx \\sqrt{0.714 \\times 0.833} \\approx 0.774\n",
    "\\$\n",
    "\n",
    "### Final BLEU Score\n",
    "For the combined score:\n",
    "\\$\n",
    "\\text{BLEU} \\approx 0.774\n",
    "\\$\n",
    "\n",
    "### Conclusion\n",
    "In this example, the generated sentence has a BLEU score of approximately 0.774 compared to the reference sentences, indicating a reasonable level of similarity. In practice, BLEU is typically calculated with more reference sentences and for n-grams up to 4-grams to get a comprehensive score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5711f-6cb0-473e-8743-f307fa6bfd70",
   "metadata": {},
   "source": [
    "### ROUGE\n",
    "\n",
    "Calculating the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score involves comparing a generated text (like a summary) with one or more reference texts. The most commonly used ROUGE metrics are ROUGE-N (for n-grams), ROUGE-L (for longest common subsequence), and ROUGE-W (weighted longest common subsequence).\n",
    "\n",
    "### Example\n",
    "\n",
    "**Reference Summaries:**\n",
    "1. The cat sat on the mat.\n",
    "2. A cat is resting on the mat.\n",
    "\n",
    "**Generated Summary:**\n",
    "The cat is on the mat.\n",
    "\n",
    "### Step 1: Tokenization\n",
    "First, tokenize the sentences into words.\n",
    "\n",
    "**Reference 1:**  \n",
    "- Tokens: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat.\"]\n",
    "\n",
    "**Reference 2:**  \n",
    "- Tokens: [\"A\", \"cat\", \"is\", \"resting\", \"on\", \"the\", \"mat.\"]\n",
    "\n",
    "**Generated:**  \n",
    "- Tokens: [\"The\", \"cat\", \"is\", \"on\", \"the\", \"mat.\"]\n",
    "\n",
    "### Step 2: Calculate ROUGE-N\n",
    "\n",
    "#### ROUGE-1 (Unigrams)\n",
    "\n",
    "1. **Generate Unigrams:**\n",
    "   - **Reference 1 Unigrams:** {\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat.\"}\n",
    "   - **Reference 2 Unigrams:** {\"A\", \"cat\", \"is\", \"resting\", \"on\", \"the\", \"mat.\"}\n",
    "   - **Generated Unigrams:** {\"The\", \"cat\", \"is\", \"on\", \"the\", \"mat.\"}\n",
    "\n",
    "2. **Count Matched Unigrams:**\n",
    "   - From the generated summary, the matched unigrams are: \"The\", \"cat\", \"on\", \"the\", \"mat.\"\n",
    "   - Matched Unigrams = 5\n",
    "\n",
    "3. **Calculate Precision, Recall, and F1-Score:**\n",
    "   - Total generated unigrams = 6\n",
    "   - Total reference unigrams = 7 (for both references, but we only need unique counts, which is 8)\n",
    "\n",
    "   **Precision:**\n",
    "   \\$\n",
    "   P = \\frac{\\text{Matched Unigrams}}{\\text{Total Generated Unigrams}} = \\frac{5}{6} \\approx 0.833\n",
    "   \\$\n",
    "\n",
    "   **Recall:**\n",
    "   \\$\n",
    "   R = \\frac{\\text{Matched Unigrams}}{\\text{Total Reference Unigrams}} = \\frac{5}{8} \\approx 0.625\n",
    "   \\$\n",
    "\n",
    "   **F1-Score:**\n",
    "   \\$\n",
    "   F1 = 2 \\times \\frac{P \\times R}{P + R} = 2 \\times \\frac{0.833 \\times 0.625}{0.833 + 0.625} \\approx 0.714\n",
    "   \\$\n",
    "\n",
    "#### ROUGE-2 (Bigrams)\n",
    "\n",
    "1. **Generate Bigrams:**\n",
    "   - **Reference 1 Bigrams:** {\"The cat\", \"cat sat\", \"sat on\", \"on the\", \"the mat.\"}\n",
    "   - **Reference 2 Bigrams:** {\"A cat\", \"cat is\", \"is resting\", \"resting on\", \"on the\", \"the mat.\"}\n",
    "   - **Generated Bigrams:** {\"The cat\", \"cat is\", \"is on\", \"on the\", \"the mat.\"}\n",
    "\n",
    "2. **Count Matched Bigrams:**\n",
    "   - Matched Bigrams = {\"on the\", \"the mat.\"} (2 matches)\n",
    "\n",
    "3. **Calculate Precision, Recall, and F1-Score:**\n",
    "   - Total generated bigrams = 5\n",
    "   - Total reference bigrams = 9 (considering both references)\n",
    "\n",
    "   **Precision:**\n",
    "   \\$\n",
    "   P = \\frac{2}{5} = 0.4\n",
    "   \\$\n",
    "\n",
    "   **Recall:**\n",
    "   \\$\n",
    "   R = \\frac{2}{9} \\approx 0.222\n",
    "   \\$\n",
    "\n",
    "   **F1-Score:**\n",
    "   \\$\n",
    "   F1 = 2 \\times \\frac{0.4 \\times 0.222}{0.4 + 0.222} \\approx 0.285\n",
    "   \\$\n",
    "\n",
    "### Step 3: Calculate ROUGE-L\n",
    "\n",
    "ROUGE-L evaluates the longest common subsequence (LCS).\n",
    "\n",
    "1. **Find LCS:**\n",
    "   - The LCS between \"The cat is on the mat.\" and both references is \"cat on the mat.\"\n",
    "\n",
    "   Length of LCS = 4\n",
    "\n",
    "2. **Calculate Precision and Recall:**\n",
    "   - Length of generated summary = 6\n",
    "   - Length of reference summary = 8 (for both)\n",
    "\n",
    "   **Precision:**\n",
    "   \\$\n",
    "   P = \\frac{LCS}{\\text{Length of Generated}} = \\frac{4}{6} \\approx 0.667\n",
    "   \\$\n",
    "\n",
    "   **Recall:**\n",
    "   \\$\n",
    "   R = \\frac{LCS}{\\text{Length of Reference}} = \\frac{4}{8} = 0.5\n",
    "   \\$\n",
    "\n",
    "   **F1-Score:**\n",
    "   \\$\n",
    "   F1 = 2 \\times \\frac{0.667 \\times 0.5}{0.667 + 0.5} \\approx 0.571\n",
    "   \\$\n",
    "\n",
    "### Summary of Scores\n",
    "- **ROUGE-1:**\n",
    "  - Precision: 0.833\n",
    "  - Recall: 0.625\n",
    "  - F1: 0.714\n",
    "\n",
    "- **ROUGE-2:**\n",
    "  - Precision: 0.4\n",
    "  - Recall: 0.222\n",
    "  - F1: 0.285\n",
    "\n",
    "- **ROUGE-L:**\n",
    "  - Precision: 0.667\n",
    "  - Recall: 0.5\n",
    "  - F1: 0.571\n",
    "\n",
    "### Conclusion\n",
    "This example illustrates how to calculate the ROUGE score, which provides insights into the quality of generated text by comparing it to reference summaries. In practice, multiple references and longer texts can be evaluated for a more robust score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63622899-957e-4ad6-b816-e92309df880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f05c7a8f-f3eb-48be-b23e-4a15ff03923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*Note:* Reload funtion calcuate rouge for update new tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/aya-23-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e389b226-11e3-4f60-b3fb-f9d431717b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(candidate, reference):\n",
    "    \"\"\"\n",
    "    Calculates the brevity penalty given the candidate and reference sentences.\n",
    "    \"\"\"\n",
    "    reference_length = len(reference)\n",
    "    candidate_length = len(candidate)\n",
    "\n",
    "    if reference_length < candidate_length:\n",
    "        BP = 1\n",
    "    else:\n",
    "        penalty = 1 - (reference_length / candidate_length)\n",
    "        BP = np.exp(penalty)\n",
    "\n",
    "    return BP\n",
    "\n",
    "\n",
    "def average_clipped_precision(candidate:str, reference:str,n:int):\n",
    "    \"\"\"\n",
    "    Calculates the precision given the candidate and reference sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    clipped_precision_score = []\n",
    "    \n",
    "    # Loop through values 1, 2, 3, 4. This is the length of n-grams\n",
    "    for n_gram_length in range(1, n):\n",
    "        reference_n_gram_counts = Counter(ngrams(reference, n_gram_length))        \n",
    "        candidate_n_gram_counts = Counter(ngrams(candidate, n_gram_length))\n",
    "\n",
    "        total_candidate_ngrams = sum(candidate_n_gram_counts.values())       \n",
    "        \n",
    "        for ngram in candidate_n_gram_counts: \n",
    "            # check if it is in the reference n-gram\n",
    "            if ngram in reference_n_gram_counts:\n",
    "                # if the count of the candidate n-gram is bigger than the corresponding\n",
    "                # count in the reference n-gram, then set the count of the candidate n-gram \n",
    "                # to be equal to the reference n-gram\n",
    "                \n",
    "                if candidate_n_gram_counts[ngram] > reference_n_gram_counts[ngram]: \n",
    "                    candidate_n_gram_counts[ngram] = reference_n_gram_counts[ngram] # t\n",
    "                                                   \n",
    "            else:\n",
    "                candidate_n_gram_counts[ngram] = 0 # else set the candidate n-gram equal to zero\n",
    "\n",
    "        clipped_candidate_ngrams = sum(candidate_n_gram_counts.values())\n",
    "        \n",
    "        clipped_precision_score.append(clipped_candidate_ngrams / total_candidate_ngrams)\n",
    "    \n",
    "    # Calculate the geometric average: take the mean of elemntwise log, then exponentiate\n",
    "    # This is equivalent to taking the n-th root of the product as shown in equation (1) above\n",
    "    s = np.exp(np.mean(np.log(clipped_precision_score)))\n",
    "    \n",
    "    return s\n",
    "\n",
    "def bleu_score(candidate:str, reference:str, n:int):\n",
    "    assert n >=2, \"n must >= 2\"\n",
    "    BP = brevity_penalty(candidate, reference)    \n",
    "    geometric_average_precision = average_clipped_precision(candidate, reference, n)    \n",
    "    return BP * geometric_average_precision *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f11496b-a941-4d71-924e-91ade5e4ea94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.51439283988789"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score(\"The cat is on the mat.\",\"The cat is sitting on the mat.\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0ef1a46-e780-4c0b-9c36-5b97c1033004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: {'precision': 0.8571428571428571, 'recall': 0.8571428571428571, 'f1_score': 0.8571428571428571}\n",
      "ROUGE-2: {'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1_score': 0.6666666666666666}\n",
      "ROUGE-L: {'precision': 0.8571428571428571, 'recall': 0.8571428571428571, 'f1_score': 0.8571428571428571}\n"
     ]
    }
   ],
   "source": [
    "def calculate_rouge(reference, generated, n=1):\n",
    "\n",
    "    \n",
    "    # Tokenize the input strings into words\n",
    "    reference_tokens = tokenizer.tokenize(reference) #reference.split()\n",
    "    generated_tokens = tokenizer.tokenize(generated) #generated.split()\n",
    "    \n",
    "    # Generate n-grams\n",
    "    reference_ngrams = list(ngrams(reference_tokens, n))\n",
    "    generated_ngrams = list(ngrams(generated_tokens, n))\n",
    "    \n",
    "    # Count n-grams\n",
    "    reference_count = Counter(reference_ngrams)\n",
    "    generated_count = Counter(generated_ngrams)\n",
    "\n",
    "    # Calculate matched n-grams\n",
    "    matched_ngrams = reference_count & generated_count\n",
    "    \n",
    "    # Precision\n",
    "    precision = (sum(matched_ngrams.values()) / len(generated_ngrams)) if generated_ngrams else 0.0\n",
    "    \n",
    "    # Recall\n",
    "    recall = (sum(matched_ngrams.values()) / len(reference_ngrams)) if reference_ngrams else 0.0\n",
    "    \n",
    "    # F1 Score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "reference_summary = \"The cat sat on the mat.\"\n",
    "generated_summary = \"The cat is on the mat.\"\n",
    "\n",
    "# Calculate ROUGE-1\n",
    "rouge_1 = calculate_rouge(reference_summary, generated_summary, n=1)\n",
    "print(\"ROUGE-1:\", rouge_1)\n",
    "\n",
    "# Calculate ROUGE-2\n",
    "rouge_2 = calculate_rouge(reference_summary, generated_summary, n=2)\n",
    "print(\"ROUGE-2:\", rouge_2)\n",
    "\n",
    "\n",
    "def lcs_length(x, y):\n",
    "    \"\"\"Calculate the length of the longest common subsequence (LCS)\"\"\"\n",
    "    m, n = len(x), len(y)\n",
    "    # Create a 2D array to store lengths of longest common subsequence.\n",
    "    lcs_table = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    # Fill the lcs_table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if x[i - 1] == y[j - 1]:\n",
    "                lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])\n",
    "\n",
    "    return lcs_table[m][n]\n",
    "\n",
    "def calculate_rouge_l(reference, generated):\n",
    "    # Tokenize the input strings into words\n",
    "    reference_tokens = tokenizer.tokenize(reference) #reference.split()\n",
    "    generated_tokens = tokenizer.tokenize(generated) #generated.split()\n",
    "\n",
    "    # Calculate the length of the longest common subsequence\n",
    "    lcs_len = lcs_length(reference_tokens, generated_tokens)\n",
    "\n",
    "    # Precision\n",
    "    precision = lcs_len / len(generated_tokens) if generated_tokens else 0.0\n",
    "\n",
    "    # Recall\n",
    "    recall = lcs_len / len(reference_tokens) if reference_tokens else 0.0\n",
    "\n",
    "    # F1 Score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "reference_summary = \"The cat sat on the mat.\"\n",
    "generated_summary = \"The cat is on the mat.\"\n",
    "\n",
    "# Calculate ROUGE-L\n",
    "rouge_l = calculate_rouge_l(reference_summary, generated_summary)\n",
    "print(\"ROUGE-L:\", rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f79dcd58-f536-436a-9251-57ba7335cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metrics(example):\n",
    "    return {\"bleu1\": bleu_score(example[\"completion\"],example[\"anwser_ja\"],2),\n",
    "            \"bleu2\": bleu_score(example[\"completion\"],example[\"anwser_ja\"],3),\n",
    "            \"rouge1\": calculate_rouge(example[\"anwser_ja\"], example[\"completion\"], n=1),\n",
    "            \"rouge2\": calculate_rouge(example[\"anwser_ja\"], example[\"completion\"], n=2),\n",
    "            \"rougeL\": calculate_rouge_l(example[\"anwser_ja\"], example[\"completion\"])\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65b6ce-e284-4dc5-967a-02b9849d057a",
   "metadata": {},
   "source": [
    "### CohereForAI/aya-23-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "563a701e-920f-423a-b1f8-075341a30d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*Note:* Reload funtion calcuate rouge for update new tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/aya-23-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33466cbe-c378-4ed6-ab98-fb0bc2985f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean bleu1: 25.419178026851267\n",
      "Mean bleu2: 19.59678932192104\n",
      "---\n",
      "Mean rouge1.f1_score: 0.33343956102471783\n",
      "Mean rouge1.recall: 0.45006450083634664\n",
      "Mean rouge1.precision: 0.2754184518128548\n",
      "---\n",
      "Mean rouge2.f1_score: 0.16891747968843607\n",
      "Mean rouge2.recall: 0.2281721898897782\n",
      "Mean rouge2.precision: 0.13977103108933928\n",
      "---\n",
      "Mean rougeL.f1_score: 0.25433886098061237\n",
      "Mean rougeL.recall: 0.34690443819304945\n",
      "Mean rougeL.precision: 0.20885447163295098\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"aya-23-8B.csv\"\n",
    "dataset = load_dataset('csv', data_files=dataset_name)\n",
    "\n",
    "dataset_flatten = dataset.map(add_metrics)\n",
    "dataset_flatten = dataset_flatten.flatten()\n",
    "\n",
    "print(\"Mean bleu1:\", np.mean(dataset_flatten['train']['bleu1']))\n",
    "print(\"Mean bleu2:\", np.mean(dataset_flatten['train']['bleu2']))\n",
    "print(\"---\")\n",
    "print(\"Mean rouge1.f1_score:\", np.mean(dataset_flatten['train']['rouge1.f1_score']))\n",
    "print(\"Mean rouge1.recall:\", np.mean(dataset_flatten['train']['rouge1.recall']))\n",
    "print(\"Mean rouge1.precision:\", np.mean(dataset_flatten['train']['rouge1.precision']))\n",
    "print(\"---\")\n",
    "print(\"Mean rouge2.f1_score:\", np.mean(dataset_flatten['train']['rouge2.f1_score']))\n",
    "print(\"Mean rouge2.recall:\", np.mean(dataset_flatten['train']['rouge2.recall']))\n",
    "print(\"Mean rouge2.precision:\", np.mean(dataset_flatten['train']['rouge2.precision']))\n",
    "print(\"---\")\n",
    "print(\"Mean rougeL.f1_score:\", np.mean(dataset_flatten['train']['rougeL.f1_score']))\n",
    "print(\"Mean rougeL.recall:\", np.mean(dataset_flatten['train']['rougeL.recall']))\n",
    "print(\"Mean rougeL.precision:\", np.mean(dataset_flatten['train']['rougeL.precision']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39ff97-c11a-4937-b1d2-a39a0c59395a",
   "metadata": {},
   "source": [
    "###  Qwen2.5-Coder-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3d450cb-29c9-4b56-9132-66f5c82c1e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*Note:* Reload funtion calcuate rouge for update new tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15fa4b52-f17d-47f3-a950-7bfe09fde316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9a02051c7a47b1933b741761a7e242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59f29282bc34f7fb8f45a0ca9aa5615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86456/3665515116.py:50: RuntimeWarning: divide by zero encountered in log\n",
      "  s = np.exp(np.mean(np.log(clipped_precision_score)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean bleu1: 32.74944976232238\n",
      "Mean bleu2: 24.234087187487628\n",
      "---\n",
      "Mean rouge1.f1_score: 0.3165370178074071\n",
      "Mean rouge1.recall: 0.4704536542546203\n",
      "Mean rouge1.precision: 0.2482704492802216\n",
      "---\n",
      "Mean rouge2.f1_score: 0.1506860322775021\n",
      "Mean rouge2.recall: 0.2251599131111332\n",
      "Mean rouge2.precision: 0.11797354150671178\n",
      "---\n",
      "Mean rougeL.f1_score: 0.2197487416360607\n",
      "Mean rougeL.recall: 0.3311228829553913\n",
      "Mean rougeL.precision: 0.17127286506468592\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"Qwen2.5-Coder-7B-Instruct.csv\"\n",
    "dataset = load_dataset('csv', data_files=dataset_name)\n",
    "\n",
    "dataset_flatten = dataset.map(add_metrics)\n",
    "dataset_flatten = dataset_flatten.flatten()\n",
    "\n",
    "print(\"Mean bleu1:\", np.mean(dataset_flatten['train']['bleu1']))\n",
    "print(\"Mean bleu2:\", np.mean(dataset_flatten['train']['bleu2']))\n",
    "print(\"---\")\n",
    "print(\"Mean rouge1.f1_score:\", np.mean(dataset_flatten['train']['rouge1.f1_score']))\n",
    "print(\"Mean rouge1.recall:\", np.mean(dataset_flatten['train']['rouge1.recall']))\n",
    "print(\"Mean rouge1.precision:\", np.mean(dataset_flatten['train']['rouge1.precision']))\n",
    "print(\"---\")\n",
    "print(\"Mean rouge2.f1_score:\", np.mean(dataset_flatten['train']['rouge2.f1_score']))\n",
    "print(\"Mean rouge2.recall:\", np.mean(dataset_flatten['train']['rouge2.recall']))\n",
    "print(\"Mean rouge2.precision:\", np.mean(dataset_flatten['train']['rouge2.precision']))\n",
    "print(\"---\")\n",
    "print(\"Mean rougeL.f1_score:\", np.mean(dataset_flatten['train']['rougeL.f1_score']))\n",
    "print(\"Mean rougeL.recall:\", np.mean(dataset_flatten['train']['rougeL.recall']))\n",
    "print(\"Mean rougeL.precision:\", np.mean(dataset_flatten['train']['rougeL.precision']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5235c0-5537-4828-926f-0291b601786e",
   "metadata": {},
   "source": [
    "### Meta-Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dce5037-e56b-4770-8052-967108a728b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*Note:* Reload funtion calcuate rouge for update new tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33d0ab97-7b2e-4459-a4cd-fb1f23684fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e307c856ea541728c7a654c5817662e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456e6e48d39e487a9239008fc14800af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86456/3665515116.py:50: RuntimeWarning: divide by zero encountered in log\n",
      "  s = np.exp(np.mean(np.log(clipped_precision_score)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean bleu1: 34.18927825496093\n",
      "Mean bleu2: 26.07185766155054\n",
      "---\n",
      "Mean rouge1.f1_score: 0.3462128804910441\n",
      "Mean rouge1.recall: 0.4883088364531873\n",
      "Mean rouge1.precision: 0.2805300436846406\n",
      "---\n",
      "Mean rouge2.f1_score: 0.18686798419326756\n",
      "Mean rouge2.recall: 0.26511660649131674\n",
      "Mean rouge2.precision: 0.15104844702017015\n",
      "---\n",
      "Mean rougeL.f1_score: 0.24949191722191302\n",
      "Mean rougeL.recall: 0.3577321465611052\n",
      "Mean rougeL.precision: 0.2005137731460221\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"Meta-Llama-3.1-8B-Instruct.csv\"\n",
    "dataset = load_dataset('csv', data_files=dataset_name)\n",
    "\n",
    "dataset_flatten = dataset.map(add_metrics)\n",
    "dataset_flatten = dataset_flatten.flatten()\n",
    "\n",
    "print(\"Mean bleu1:\", np.mean(dataset_flatten['train']['bleu1']))\n",
    "print(\"Mean bleu2:\", np.mean(dataset_flatten['train']['bleu2']))\n",
    "print(\"---\")\n",
    "print(\"Mean rouge1.f1_score:\", np.mean(dataset_flatten['train']['rouge1.f1_score']))\n",
    "print(\"Mean rouge1.recall:\", np.mean(dataset_flatten['train']['rouge1.recall']))\n",
    "print(\"Mean rouge1.precision:\", np.mean(dataset_flatten['train']['rouge1.precision']))\n",
    "print(\"---\")\n",
    "print(\"Mean rouge2.f1_score:\", np.mean(dataset_flatten['train']['rouge2.f1_score']))\n",
    "print(\"Mean rouge2.recall:\", np.mean(dataset_flatten['train']['rouge2.recall']))\n",
    "print(\"Mean rouge2.precision:\", np.mean(dataset_flatten['train']['rouge2.precision']))\n",
    "print(\"---\")\n",
    "print(\"Mean rougeL.f1_score:\", np.mean(dataset_flatten['train']['rougeL.f1_score']))\n",
    "print(\"Mean rougeL.recall:\", np.mean(dataset_flatten['train']['rougeL.recall']))\n",
    "print(\"Mean rougeL.precision:\", np.mean(dataset_flatten['train']['rougeL.precision']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c2cc1-343e-44a6-85d2-7157c88bd285",
   "metadata": {},
   "source": [
    "### Orion-14B-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a55a085a-f564-4d9b-a0e4-dead4da45d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*Note:* Reload funtion calcuate rouge for update new tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/aya-23-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9b21c3b-f3e3-4720-8240-10b59110512d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'prompt', 'question', 'answer', 'anwser_ja', 'question_ja', 'completion'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files='Orion-14B-Chat.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c5efaa7-20a7-48d9-973b-6f7a06936796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f783a5201d4f4bc78160cbb4b5d09d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48137/3665515116.py:50: RuntimeWarning: divide by zero encountered in log\n",
      "  s = np.exp(np.mean(np.log(clipped_precision_score)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'prompt', 'question', 'answer', 'anwser_ja', 'question_ja', 'completion', 'bleu1', 'bleu2', 'rouge1.f1_score', 'rouge1.precision', 'rouge1.recall', 'rouge2.f1_score', 'rouge2.precision', 'rouge2.recall', 'rougeL.f1_score', 'rougeL.precision', 'rougeL.recall'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_flatten = dataset.map(add_metrics)\n",
    "dataset_flatten = dataset_flatten.flatten()\n",
    "dataset_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3295bb17-5502-458f-a83f-464f7f2c1743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean bleu1: 37.08018014235039\n",
      "Mean bleu2: 28.320968909852898\n",
      "---\n",
      "Mean rouge1.f1_score: 0.34310884544511766\n",
      "Mean rouge1.recall: 0.440153094018804\n",
      "Mean rouge1.precision: 0.31273861505947614\n",
      "---\n",
      "Mean rouge2.f1_score: 0.16849878656264275\n",
      "Mean rouge2.recall: 0.21719597043868602\n",
      "Mean rouge2.precision: 0.15426227970635153\n",
      "---\n",
      "Mean rougeL.f1_score: 0.2738027757123005\n",
      "Mean rougeL.recall: 0.3490623179450801\n",
      "Mean rougeL.precision: 0.25152448830410307\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean bleu1:\", np.mean(dataset_flatten['train']['bleu1']))\n",
    "print(\"Mean bleu2:\", np.mean(dataset_flatten['train']['bleu2']))\n",
    "print(\"---\")\n",
    "print(\"Mean rouge1.f1_score:\", np.mean(dataset_flatten['train']['rouge1.f1_score']))\n",
    "print(\"Mean rouge1.recall:\", np.mean(dataset_flatten['train']['rouge1.recall']))\n",
    "print(\"Mean rouge1.precision:\", np.mean(dataset_flatten['train']['rouge1.precision']))\n",
    "print(\"---\")\n",
    "print(\"Mean rouge2.f1_score:\", np.mean(dataset_flatten['train']['rouge2.f1_score']))\n",
    "print(\"Mean rouge2.recall:\", np.mean(dataset_flatten['train']['rouge2.recall']))\n",
    "print(\"Mean rouge2.precision:\", np.mean(dataset_flatten['train']['rouge2.precision']))\n",
    "print(\"---\")\n",
    "print(\"Mean rougeL.f1_score:\", np.mean(dataset_flatten['train']['rougeL.f1_score']))\n",
    "print(\"Mean rougeL.recall:\", np.mean(dataset_flatten['train']['rougeL.recall']))\n",
    "print(\"Mean rougeL.precision:\", np.mean(dataset_flatten['train']['rougeL.precision']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af5f91c-e983-45f4-ab73-4f49f3792a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
