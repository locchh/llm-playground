{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "918c97c5-dbb0-45de-86dd-68e8908649f4",
   "metadata": {},
   "source": [
    "### Multilingual LLM Orion-14B-Chat\n",
    "\n",
    "https://huggingface.co/OrionStarAI/Orion-14B-Chat/tree/main\n",
    "\n",
    "**Requires**\n",
    "- *flash_attn-2.6.3*\n",
    "- *>=30GB RAM*\n",
    "\n",
    "If you have problem when I needed NVCC for flash attention, but it seems that torch uses a reduced version of CUDA libraries. Installing the toolkit from conda forge resolved issue for me: `conda install -c conda-forge cudatoolkit-dev -y`\n",
    "\n",
    "https://stackoverflow.com/questions/52731782/get-cuda-home-environment-path-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eff3b4f-ab70-413a-9556-c2cd01674f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version # Only inside current environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b8bc77-3cfa-4869-87a1-5d465405ea01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09f2d1a65164c7a9cece267721c859b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 31 files:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to: /home/loc/.cache/huggingface/hub/models--OrionStarAI--Orion-14B-Chat/snapshots/7aa75f1e0939fc082e67a7f58af7876907a1875e\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Specify the model repository (e.g., \"bert-base-uncased\")\n",
    "repo_id = \"OrionStarAI/Orion-14B-Chat\"\n",
    "\n",
    "# Download the model and save it to a local directory\n",
    "local_dir = snapshot_download(repo_id=repo_id)\n",
    "\n",
    "print(f\"Model downloaded to: {local_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feaf714-580e-4b11-a5c7-32f7f619740b",
   "metadata": {},
   "source": [
    "**Notes:** *to install `flash_attn`*\n",
    "\n",
    "- Install by command: `pip install flash_attn` or `conda install -c conda-forge flash_attn`\n",
    "\n",
    "- Install by repo:\n",
    "\n",
    "```\n",
    "git clone https://github.com/HazyResearch/flash-attention\n",
    "cd flash-attention\n",
    "pip install .\n",
    "\n",
    "```\n",
    "\n",
    "*If you get Error:*\n",
    "\n",
    "The error you're seeing indicates that the `flash_attn` package's build process requires `g++`, the GNU C++ compiler, and it is not available on your system. This can be resolved by installing `g++` and ensuring it is accessible.\n",
    "\n",
    "\n",
    "     \n",
    "1. **Install `g++` (GNU C++ Compiler)**:\n",
    "   Depending on your operating system, you'll need to install `g++` (>=6.0.0, <=11.5.0) to compile the required parts of the package. On **Ubuntu/Debian**:\n",
    "\n",
    "    ```bash\n",
    "     sudo apt update\n",
    "     sudo apt install g++-11\n",
    "\n",
    "2. **Verify `g++` Installation**:\n",
    "   After installation, check that `g++` is available by running:\n",
    "   ```bash\n",
    "   g++ --version\n",
    "   ```\n",
    "   You should see the version information of `g++`.\n",
    "\n",
    "3. **Retry the Installation**:\n",
    "   Once `g++` is installed and available, try installing the package again:\n",
    "   ```bash\n",
    "   pip install .\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e70ce4a6-2ba0-4857-8dc9-f48e9cf81413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee65f4f102a45659c42f0474a0567b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "repo_id = \"OrionStarAI/Orion-14B-Chat\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, use_fast=False, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id, device_map=\"auto\",\n",
    "                                             torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "\n",
    "model.generation_config = GenerationConfig.from_pretrained(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3591bcea-e30c-4fc9-b626-5202877056b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am an AI language model, so I don't have a name like a person does. How can I assist you today?\n",
      "CPU times: user 3.44 s, sys: 231 ms, total: 3.67 s\n",
      "Wall time: 3.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hello, what is your name? \"}]\n",
    "response = model.chat(tokenizer, messages, streaming=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8837123-89ca-4f12-8db1-a47af9659725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私は人工知能アシスタントで、名前はありません。あなたに会えて嬉しいです！\n",
      "CPU times: user 2.38 s, sys: 15.9 ms, total: 2.39 s\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "messages = [{\"role\": \"user\", \"content\": \"こんにちは、お名前は何ですか？\"}]\n",
    "response = model.chat(tokenizer, messages, streaming=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bbfd494-1575-4079-bacf-0b342f4214ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是一个人工智能助手，我没有具体的名字。有什么我可以帮你的吗？\n",
      "CPU times: user 2.18 s, sys: 3.52 ms, total: 2.19 s\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "messages = [{\"role\": \"user\", \"content\": \"你好! 你叫什么名字!\"}]\n",
    "response = model.chat(tokenizer, messages, streaming=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63692143-84ee-4572-b0d5-c5d75f6d42e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 저는 인공지능 보조 도구로, 이름이 있는 것이 아니라 '아이다'라고 불립니다. 무엇을 도와드릴까요?\n",
      "CPU times: user 3.5 s, sys: 3.28 ms, total: 3.51 s\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "messages = [{\"role\": \"user\", \"content\": \"안녕! 이름이 뭐예요!\"}]\n",
    "response = model.chat(tokenizer, messages, streaming=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704c3a7-cd78-4d90-86e4-639b2a604729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
