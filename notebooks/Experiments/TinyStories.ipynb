{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3154d28c-c2b4-43e9-a261-8390f64baa82",
   "metadata": {},
   "source": [
    "### TinyStories\n",
    "\n",
    "https://huggingface.co/roneneldan\n",
    "\n",
    "https://arxiv.org/abs/2305.07759\n",
    "\n",
    "https://github.com/karpathy/llama2.c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05730cce-c665-4ce4-bf75-8fee341aac8a",
   "metadata": {},
   "source": [
    "#### Load model and predict in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebbdea8-1a70-4a84-8dc4-3bd969e31716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU: Tesla P40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device index (default is 0 if no other device is specified)\n",
    "    current_device = torch.cuda.current_device()\n",
    "    \n",
    "    # Get the name of the GPU at this device index\n",
    "    gpu_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5deda9e-f2d5-4e91-81ab-a99c85624cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4465fc42912441e4947de0a28d0f4f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  47%|####6     | 136M/291M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f1fa38615f482a8f18a25b8c4baa8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c05f56cb5c048fea1966b53bdbd7fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b22f15df49412a88ddff4e6d1ffc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031c7a5a44fb44c4b6ea74fe6ace1843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9d5dba651a49a2851104e3048cc4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a little girl named Lucy. She was three years old and loved to explore. One day, Lucy was walking in the park when she saw a big, red balloon. She was so excited and ran over to it.\n",
      "\n",
      "\"Can I have it?\" she asked.\n",
      "\n",
      "\"No,\" said her mom. \"It's too big for you. You can't have it.\"\n",
      "\n",
      "Lucy was sad, but then she saw a small, red balloon. She smiled and said, \"I want that one!\"\n",
      "\n",
      "Her mom smiled and said, \"Okay, let's go get it.\"\n",
      "\n",
      "So they went to the balloon and Lucy was so happy. She held the balloon tight and ran around the park with it. She laughed and smiled and had so much fun.\n",
      "\n",
      "When it was time to go home, Lucy hugged the balloon and said, \"I love you, balloon!\"\n",
      "\n",
      "Her mom smiled and said, \"I love you too, Lucy.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "prompt = \"Once upon a time there was\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate completion\n",
    "output = model.generate(input_ids, max_length = 1000, num_beams=1)\n",
    "\n",
    "# Decode the completion\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940db2e-1dab-445c-85fa-3f9662a9e682",
   "metadata": {},
   "source": [
    "#### Export model to *.bin file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "126fa75d-aeb6-4e4d-83ba-e7a2bce22b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Save the model weights\n",
    "torch.save(model.state_dict(), \"model_weights.bin\")\n",
    "\n",
    "print(\"Export successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a132077-130e-46d6-98e1-22c30fc1222e",
   "metadata": {},
   "source": [
    "```bash\n",
    "./run model_weights.bin\n",
    "Floating point exception (core dumped)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a8e306-7b42-4d50-9015-375e9a9403c9",
   "metadata": {},
   "source": [
    "# Run.c\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/karpathy/llama2.c/blob/master/run.ipynb)\n",
    "\n",
    "More details can be found in the [README.md](README.md) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd58681-3a75-4952-970e-03f9cc9bed0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama2.c'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 1517, done.\u001b[K\n",
      "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
      "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
      "remote: Total 1517 (delta 4), reused 4 (delta 0), pack-reused 1507 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1517/1517), 1.23 MiB | 4.62 MiB/s, done.\n",
      "Resolving deltas: 100% (931/931), done.\n",
      "/home/loc/Works/llm-playground/notebooks/Experiments/llama2.c\n"
     ]
    }
   ],
   "source": [
    "#@title Clone Project\n",
    "\n",
    "!git clone https://github.com/karpathy/llama2.c.git\n",
    "%cd llama2.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df577d35-8207-419d-a66a-8a826a1d8d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/loc/Works/llm-playground/notebooks/Experiments/llama2.c\n",
      "/home/loc/Works/llm-playground/notebooks/Experiments/llama2.c\n"
     ]
    }
   ],
   "source": [
    "%cd llama2.c\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b8f67f-16da-4906-91a3-59eb80a39a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -Ofast -o run run.c -lm\n",
      "gcc -Ofast -o runq runq.c -lm\n"
     ]
    }
   ],
   "source": [
    "#@title Build\n",
    "\n",
    "!make runfast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8852cf85-de4f-4fff-9052-166f98114de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download_url: https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin\n",
      "--2024-11-10 14:40:11--  https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin\n",
      "Resolving huggingface.co (huggingface.co)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600:9000:26b8:1800:17:b174:6d00:93a1, 2600:9000:26b8:da00:17:b174:6d00:93a1, 2600:9000:26b8:ec00:17:b174:6d00:93a1, ...\n",
      "Connecting to huggingface.co (huggingface.co)|2600:9000:26b8:1800:17:b174:6d00:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.hf.co/repos/88/4b/884bade32e5ee32eea725c5087af1358179a1bea94a4f6abc3c0470c9610ac38/cd590644d963867a2b6e5a1107f51fad663c41d79c149fbecbbb1f95fa81f49a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27stories15M.bin%3B+filename%3D%22stories15M.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1731483616&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTQ4MzYxNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy84OC80Yi84ODRiYWRlMzJlNWVlMzJlZWE3MjVjNTA4N2FmMTM1ODE3OWExYmVhOTRhNGY2YWJjM2MwNDcwYzk2MTBhYzM4L2NkNTkwNjQ0ZDk2Mzg2N2EyYjZlNWExMTA3ZjUxZmFkNjYzYzQxZDc5YzE0OWZiZWNiYmIxZjk1ZmE4MWY0OWE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=PT1w1Vm4zy1LJ%7EmBCFCf5e0wxJGFyBDwtKIXe6TYMmtGeMD9%7EGqH2Uv4RHAKLl0i1yoUWOc11LV%7ELfmIM8YPZBWz4VPUeEafrrykbAy8bERe1aQz9IM-FoHDvpdC8KTazsv28KhCu0qyFLWHQ0ahSWNYgQ3IGc7a2RKyo1GQ9OrOb0QP-0IiY7CWpcNHqVyMHIcr5RCy1EVCLKG7XilCfoLJ7-0a%7EwJ6%7En6tMJJsK7F1xDP8HAT4jV7R427RHy%7EPYjMei46LCA3OxALELsXfz%7EzCPP96VfHOKaOGbs989NMSAgPExPhEs4RIj-3yr4vzu1-2aNazCJE40qroecce1A__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
      "--2024-11-10 14:40:16--  https://cdn-lfs.hf.co/repos/88/4b/884bade32e5ee32eea725c5087af1358179a1bea94a4f6abc3c0470c9610ac38/cd590644d963867a2b6e5a1107f51fad663c41d79c149fbecbbb1f95fa81f49a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27stories15M.bin%3B+filename%3D%22stories15M.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1731483616&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTQ4MzYxNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy84OC80Yi84ODRiYWRlMzJlNWVlMzJlZWE3MjVjNTA4N2FmMTM1ODE3OWExYmVhOTRhNGY2YWJjM2MwNDcwYzk2MTBhYzM4L2NkNTkwNjQ0ZDk2Mzg2N2EyYjZlNWExMTA3ZjUxZmFkNjYzYzQxZDc5YzE0OWZiZWNiYmIxZjk1ZmE4MWY0OWE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=PT1w1Vm4zy1LJ%7EmBCFCf5e0wxJGFyBDwtKIXe6TYMmtGeMD9%7EGqH2Uv4RHAKLl0i1yoUWOc11LV%7ELfmIM8YPZBWz4VPUeEafrrykbAy8bERe1aQz9IM-FoHDvpdC8KTazsv28KhCu0qyFLWHQ0ahSWNYgQ3IGc7a2RKyo1GQ9OrOb0QP-0IiY7CWpcNHqVyMHIcr5RCy1EVCLKG7XilCfoLJ7-0a%7EwJ6%7En6tMJJsK7F1xDP8HAT4jV7R427RHy%7EPYjMei46LCA3OxALELsXfz%7EzCPP96VfHOKaOGbs989NMSAgPExPhEs4RIj-3yr4vzu1-2aNazCJE40qroecce1A__&Key-Pair-Id=K3RPWS32NSSJCE\n",
      "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 108.157.14.104, 108.157.14.70, 108.157.14.14, ...\n",
      "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|108.157.14.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60816028 (58M) [application/octet-stream]\n",
      "Saving to: ‘stories15M.bin’\n",
      "\n",
      "stories15M.bin      100%[===================>]  58.00M  21.7MB/s    in 2.7s    \n",
      "\n",
      "2024-11-10 14:40:20 (21.7 MB/s) - ‘stories15M.bin’ saved [60816028/60816028]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Pick Your Model\n",
    "\n",
    "#@markdown Choose model\n",
    "model = \"stories15M\" #@param [\"stories15M\", \"stories42M\", \"stories110M\"]\n",
    "\n",
    "download_url = \"\"\n",
    "\n",
    "if(model == \"stories15M\"):\n",
    "  download_url = \"https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin\"\n",
    "if(model == \"stories42M\"):\n",
    "  download_url = \"https://huggingface.co/karpathy/tinyllamas/resolve/main/stories42M.bin\"\n",
    "if(model == \"stories110M\"):\n",
    "  download_url = \"https://huggingface.co/karpathy/tinyllamas/resolve/main/stories110M.bin\"\n",
    "\n",
    "print(f\"download_url: {download_url}\")\n",
    "\n",
    "!wget $download_url\n",
    "\n",
    "model_file = model + \".bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b05ce2-621c-418a-8038-ece1f1b305e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: stories15M.bin, max_token: 256, temperature: 0.8, top_p: 0.9, prompt: One day, Lily met a Shoggoth\n",
      "----------------------------\n",
      "\n",
      "One day, Lily met a Shoggoth. He was a bright green and shiny thing. He smiled and said, \"Hi, I'm Shog. I'm a reliable friend. I always help you. Do you"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " want to play with me?\" Lily nodded and smiled back. She said, \"Yes, I want to play with you. You are a reliable friend.\"\n",
      "They played in the park and laughed. They had fun. They were happy. Lily's mom watched them and said, \"Wow, you two are very good at playing. You are very good friends. You are both very reliable. You always help each other and share your toys. That is very kind.\"\n",
      "Lily and Shog looked at each other. They said, \"You are right. Shog is a reliable friend. And you are very reliable. Thank you for sharing.\" They hugged and said goodbye. They went back to their mom. They told her what they learned. She was proud of them. She said, \"That's wonderful. You are very smart and friendly. You are very good friends.\" She hugged them and kissed them. They hugged her back and said\n",
      "achieved tok/s: 359.154930\n"
     ]
    }
   ],
   "source": [
    "#@title Generate Stories\n",
    "\n",
    "# Generate args\n",
    "max_token = 256 #@param {type:\"slider\", min:32, max:1024, step:32}\n",
    "temperature = 0.8 #@param {type:\"slider\", min:0.0, max:1, step:0.05}\n",
    "top_p = 0.9 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "prompt = \"One day, Lily met a Shoggoth\" #@param {type:\"string\"}\n",
    "\n",
    "print(f\"model: {model_file}, max_token: {max_token}, temperature: {temperature}, top_p: {top_p}, prompt: {prompt}\")\n",
    "print(f\"----------------------------\\n\")\n",
    "\n",
    "cmd = f'./run {model_file} -t {temperature} -p {top_p} -n {max_token} -i \"{prompt}\"'\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc51383-1c6e-4add-bb04-37864f3b1976",
   "metadata": {},
   "source": [
    "```bash\n",
    "#@title Run Meta's Llama 2 models\n",
    "\n",
    "#@markdown input your huggingface [access token](https://huggingface.co/settings/tokens) to download Meta's Llama 2 models.\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "token = \"replace your huggingface access token\" #@param {type:\"string\"}\n",
    "path = snapshot_download(repo_id=\"meta-llama/Llama-2-7b\",cache_dir=\"Llama-2-7b\", use_auth_token=token)\n",
    "\n",
    "!python export.py llama2_7b.bin --meta-llama $path\n",
    "\n",
    "print(\"./run llama2_7b.bin\\n\")\n",
    "!./run llama2_7b.bin\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
