{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08004cf-9aa2-4e0b-baf5-2a26d8525285",
   "metadata": {},
   "source": [
    "### Preference Alignment with Direct Preference Optimization (DPO)\n",
    "\n",
    "This notebook will guide you through the process of fine-tuning a language model using Direct Preference Optimization (DPO). We will use the SmolLM2-135M-Instruct model which has already been through a SFT training, so it it compatible with DPO. You can also use the model you trained in [1_instruction_tuning](https://github.com/huggingface/smol-course/blob/a5cc73e2e0a9df77d2c34369314431c94674a5dd/1_instruction_tuning/notebooks/sft_finetuning_example.ipynb).\n",
    "\n",
    "\n",
    "\n",
    "The training time is influenced by the number of samples.\n",
    "\n",
    "GPU usage and power consumption are affected by factors such as the number of samples, batch size, data type (FP16, FP32, BF16, etc.), token length, and model size.\n",
    "\n",
    "To optimize efficiency, consider starting with the smallest settings or training for a few steps initially.\n",
    "The training time is influenced by the number of samples.\n",
    "\n",
    "GPU usage and power consumption are affected by factors such as the number of samples, batch size, data type (FP16, FP32, BF16, etc.), token length, and model size.\n",
    "\n",
    "To optimize efficiency, consider starting with the smallest settings or training for a few steps initially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c8dd0-d7f1-406e-afba-d18b2df19799",
   "metadata": {},
   "source": [
    "### Exercise: Aligning SmolLM2 with DPOTrainer\n",
    "\n",
    "Take a dataset from the Hugging Face hub and align a model on it.\n",
    "\n",
    "Difficulty Levels\n",
    "\n",
    "🐢 Use the `trl-lib/ultrafeedback_binarized` dataset\n",
    "\n",
    "🐕 Try out the `argilla/ultrafeedback-binarized-preferences` dataset\n",
    "\n",
    "🦁 Select a dataset that relates to a real-world use case you’re interested in, or use the model you trained in [1_instruction_tuning](https://github.com/huggingface/smol-course/blob/a5cc73e2e0a9df77d2c34369314431c94674a5dd/1_instruction_tuning/notebooks/sft_finetuning_example.ipynb)\n",
    "\n",
    "\n",
    "**Note**: *If GPU limitation*:\n",
    "\n",
    "- Reduces the dataset size to 10% using .shuffle().select()\n",
    "- Reducing batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d644ab46-9f07-4be6-a38d-93267a33131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla P40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import DPOTrainer, DPOConfig, setup_chat_format\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ba16e-b13a-4baf-acb1-b037a1e19ab8",
   "metadata": {},
   "source": [
    "### Format dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3a6816-7fda-4f3b-936d-ca1893f70122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "# TODO: 🦁🐕 change the dataset to one of your choosing\n",
    "dataset = load_dataset(path=\"trl-lib/ultrafeedback_binarized\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b00557f-e4bf-4251-861a-3e56e9fc3c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chosen\": [\n",
      "        {\n",
      "            \"content\": \"Use the pygame library to write a version of the classic game Snake, with a unique twist\",\n",
      "            \"role\": \"user\"\n",
      "        },\n",
      "        {\n",
      "            \"content\": \"Sure, I'd be happy to help you write a version of the classic game Snake using the pygame library! Here's a basic outline of how we can approach this:\\n\\n1. First, we'll need to set up the game display and create a game object that we can use to handle the game's state.\\n2. Next, we'll create the game's grid, which will be used to represent the game board. We'll need to define the size of the grid and the spaces within it.\\n3. After that, we'll create the snake object, which will be used to represent the player's movement. We'll need to define the size of the snake and the speed at which it moves.\\n4. We'll also need to create a food object, which will be used to represent the food that the player must collect to score points. We'll need to define the location of the food and the speed at which it moves.\\n5. Once we have these objects set up, we can start handling the game's logic. We'll need to define the rules for the player's movements, such as how the player can move the snake and how the snake will grow as the player collects more food.\\n6. We'll also need to add collisions detection to the game, so that the snake and the food can collide with each other.\\n7. Finally, we'll need to add a user interface to the game, such as a menu and a scoreboard.\\n\\nNow, as for the unique twist, we could add a few features to make the game more interesting. For example, we could add power-ups that give the player special abilities, such as the ability to grow the snake faster or to change its direction. We could also add obstacles, such as walls or pits, that the player must avoid.\\n\\nHere's some sample code to get us started:\\n```\\nimport pygame\\n\\n# Initialize pygame\\npygame.init()\\n\\n# Set up the game display\\nwidth = 800\\nheight = 600\\nscreen = pygame.display.set_mode((width, height))\\n\\n# Define the colors\\nWHITE = (255, 255, 255)\\nBLACK = (0, 0, 0)\\nGREEN = (0, 255, 0)\\n\\n# Define the game objects\\nsnake = pygame.Rect(50, 50, 10, 10)\\nfood = pygame.Rect(100, 100, 10, 10)\\n\\n# Define the game logic\\ndef update_snake():\\n    # Get the mouse position\\n    mouse_pos = pygame.mouse.get_pos()\\n\\n    # Move the snake based on the mouse position\\n    if mouse_pos[0] > snake.x:\\n        snake.x += 10\\n    elif mouse_pos[0] < snake.x:\\n        snake.x -= 10\\n    elif mouse_pos[1] > snake.y:\\n        snake.y += 10\\n    elif mouse_pos[1] < snake.y:\\n        snake.y -= 10\\n\\n    # Update the snake's size\\n    if snake.x == food.x and snake.y == food.y:\\n        snake.width += 10\\n        snake.height += 10\\n\\n# Define the game loop\\ndef game_loop():\\n    # Clear the screen\\n    screen.fill(BLACK)\\n\\n    # Update the game objects\\n    update_snake()\\n\\n    # Draw the game objects\\n    screen.fill(WHITE)\\n    screen.draw.rect(snake, GREEN)\\n    screen.draw.rect(food, BLACK)\\n\\n    # Update the display\\n    pygame.display.flip()\\n\\n# Start the game loop\\ngame_loop()\\n\\n# Run the game\\nwhile True:\\n    for event in pygame.event.get():\\n        if event.type == pygame.QUIT:\\n            pygame.quit()\\n            break\\n\\n    pygame.time.Clock().tick(60)\\n```\\nThis code sets up a basic game display, defines the snake and food objects, and updates the game state based on the player's movements. We've also added a simple user interface and some basic collision detection.\\n\\nAs for the unique twist, we could add a few features to make the game\",\n",
      "            \"role\": \"assistant\"\n",
      "        }\n",
      "    ],\n",
      "    \"rejected\": [\n",
      "        {\n",
      "            \"content\": \"Use the pygame library to write a version of the classic game Snake, with a unique twist\",\n",
      "            \"role\": \"user\"\n",
      "        },\n",
      "        {\n",
      "            \"content\": \"Sure, here's an example of how to write a version of Snake game with a unique twist using the Pygame library:\\n```python\\nimport pygame\\n\\nclass SnakeGame:\\n    def __init__(self, game_width, game_height):\\n        pygame.init()\\n        screen = pygame.display.set_mode((game_width, game_height))\\n        pygame.display.set_caption(\\\"Snake Game\\\")\\n        self.speed = 5  # Speed of the snake\\n        self.food_speed = 1  # Speed of the food\\n        self.direction = 0  # Initial direction of the snake\\n        self.snakelen = 0  # Length of the snake\\n        self.food = pygame.image.load(\\\"snake_food.png\\\")\\n        self.head = pygame.image.load(\\\"snake_head.png\\\")\\n        self.tail = pygame.image.load(\\\"snake_tail.png\\\")\\n        self.game Quint()\\n    def Quint(self):\\n        for i in range(50):\\n            pygame.draw.line(screen, (180, 100, 220), (0, 0, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 0, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 50, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 50, 300), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 80, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 80, 300), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 50, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 50, 300), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 80, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 80, 300), 2)\\n            pygame.display.flip()\\n        self.game.run()\\n    def run(self):\\n        while True:\\n            for event in pygame.event. pygame.KEYDOWN:\\n                if event.key == pygame.K_LEFT:\\n                    self.direction = -1\\n                if event.key == pygame.K_RIGHT:\\n                    self.direction = 1\\n            self.snakelen += 1\\n            if self.snakelen == 0:\\n                self.snakelen = 10\\n            if self.snakelen > 20:\\n                self.snakelen = 20\\n            self.gameQuint()\\n            self.foodCrossing()\\n            self.headRun()\\n            pygame.display.update()\\ngame = SnakeGame(800, 600)\\ngame.run()\\n```\\nIn this game, the snake moves with a constant speed, but the direction of the snake can be controlled by the user using the left and right arrow keys. The snake grows in length every 10 segments, and when it reaches a certain length, it resets to 10 segments. The food moves fast and randomly crosses the screen, and the snake can eat it by colliding with it. The snake's head and tail move independently of each other. The game ends when the snake dies or reaches the end of the screen.\",\n",
      "            \"role\": \"assistant\"\n",
      "        }\n",
      "    ],\n",
      "    \"score_chosen\": 6.0,\n",
      "    \"score_rejected\": 4.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(dataset[0],indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af66d9f-14f2-4361-bff5-11bc6b8f922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 🐕 If your dataset is not represented as conversation lists,\n",
    "#you can use the `process_dataset` function to convert it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de2e2a-7875-4d73-ae08-817d1770c002",
   "metadata": {},
   "source": [
    "### Select the model\n",
    "\n",
    "We will use the SmolLM2-135M-Instruct model which has already been through a SFT training, so it it compatible with DPO. You can also use the model you trained in [1_instruction_tuning](https://github.com/huggingface/smol-course/blob/a5cc73e2e0a9df77d2c34369314431c94674a5dd/1_instruction_tuning/notebooks/sft_finetuning_example.ipynb).\n",
    "\n",
    "🦁 change the model to the path or repo id of the model you trained in 1_instruction_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62656d44-d53c-4d0d-aeef-987dd6d33b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 🦁 change the model to the path or repo id of the model \n",
    "#you trained in [1_instruction_tuning](../../1_instruction_tuning/notebooks/sft_finetuning_example.ipynb)\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\" #\"SmolLM2-FT-MyDataset\"\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load model with reduced precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    ").to(device)\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-DPO\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e26894-8601-4e31-ab05-c706dbdd3717",
   "metadata": {},
   "source": [
    "### Train model with DPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680a0ea7-af5d-4bb3-9aa1-3f3e37c77c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = DPOConfig(\n",
    "    # Training batch size per GPU\n",
    "    per_device_train_batch_size=4,\n",
    "    # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    # Effective batch size = per_device_train_batch_size * gradient_accumulation_steps\n",
    "    gradient_accumulation_steps=4,\n",
    "    # Saves memory by not storing activations during forward pass\n",
    "    # Instead recomputes them during backward pass\n",
    "    gradient_checkpointing=True,\n",
    "    # Base learning rate for training\n",
    "    learning_rate=5e-5,\n",
    "    # Learning rate schedule - 'cosine' gradually decreases LR following cosine curve\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # Total number of training steps\n",
    "    max_steps=2,\n",
    "    # Disables model checkpointing during training\n",
    "    save_strategy=\"no\",\n",
    "    # How often to log training metrics\n",
    "    logging_steps=1,\n",
    "    # Directory to save model outputs\n",
    "    output_dir=\"smol_dpo_output\",\n",
    "    # Number of steps for learning rate warmup\n",
    "    warmup_steps=100,\n",
    "    # Use bfloat16 precision for faster training\n",
    "    bf16=True,\n",
    "    # Disable wandb/tensorboard logging\n",
    "    report_to=\"none\",\n",
    "    # Keep all columns in dataset even if not used\n",
    "    remove_unused_columns=False,\n",
    "    # Enable MPS (Metal Performance Shaders) for Mac devices\n",
    "    use_mps_device=device == \"mps\",\n",
    "    # Model ID for HuggingFace Hub uploads\n",
    "    hub_model_id=finetune_name,\n",
    "    # DPO-specific temperature parameter that controls the strength of the preference model\n",
    "    # Lower values (like 0.1) make the model more conservative in following preferences\n",
    "    beta=0.1,\n",
    "    # Maximum length of the input prompt in tokens\n",
    "    max_prompt_length=1024,\n",
    "    # Maximum combined length of prompt + response in tokens\n",
    "    max_length=1536,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c3dd461-af45-4ad1-9f94-ba8857827d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbd1eb7c8e8488497723dd683333dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/62135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea0e048743842458d8c67e2ec8ee35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/62135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    # The model to be trained\n",
    "    model=model,\n",
    "    # Training configuration from above\n",
    "    args=training_args,\n",
    "    # Dataset containing preferred/rejected response pairs\n",
    "    train_dataset=dataset,\n",
    "    # Tokenizer for processing inputs\n",
    "    processing_class=tokenizer,\n",
    "    # DPO-specific temperature parameter that controls the strength of the preference model\n",
    "    # Lower values (like 0.1) make the model more conservative in following preferences\n",
    "    # beta=0.1,\n",
    "    # Maximum length of the input prompt in tokens\n",
    "    # max_prompt_length=1024,\n",
    "    # Maximum combined length of prompt + response in tokens\n",
    "    # max_length=1536,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b29ec7-6ccf-4e1a-8fba-6fcf767993e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")\n",
    "\n",
    "# Save to the huggingface hub if login (HF_TOKEN is set)\n",
    "# if os.getenv(\"HF_TOKEN\"):\n",
    "#     trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec1a38-c71e-4f38-b1fb-44ecb06c4d25",
   "metadata": {},
   "source": [
    "\n",
    "💐 You're done!\n",
    "\n",
    "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `DPOTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n",
    "\n",
    "- Try this notebook on a harder difficulty\n",
    "- Review a colleagues PR\n",
    "- Improve the course material via an Issue or PR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1852f8-5730-4be5-add9-702204e0e9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
