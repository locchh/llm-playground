{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cdaa4e6-b290-4a54-b1e9-8b49b3ec10d8",
   "metadata": {},
   "source": [
    "## Introduction to post-training\n",
    "\n",
    "- **Pre-training**: Learning knowledge from everywhere, the output is **Base model** (Predicts next word/token).\n",
    "- \n",
    "- **Post-training**: Learning responses from curated data, the output is **Instruct/Chat model** (Respond to instructions).\n",
    "- \n",
    "- **(Continual) Post-training**: Changing behaviors or enhancing capabilities, the output is **Customized model** (Specialized in certain domain or have specific behaviors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d08013-48b7-4d12-b3ad-8bb44fcbec50",
   "metadata": {},
   "source": [
    "## Methods Used During LLM Training\n",
    "\n",
    "- **Pre-Training (Unsupervised Learning)**: Unlabeled Text Corpus (2T tokens)\n",
    "\n",
    "formula:\n",
    "$\n",
    "\\min_{\\pi} -\\log \\pi(\\text{I}) - \\log \\pi(\\text{like} \\mid \\text{I}) - \\log \\pi(\\text{cats} \\mid \\text{I like})\n",
    "$\n",
    "\n",
    "- **Post-training Method 1: Supervised Fine-tuning (Supervised/ Imitation Learning**: Labeled Prompt-Response Pairs (~1K-1B tokens)\n",
    "\n",
    "formula:\n",
    "$\n",
    "\\min_{\\pi} -\\log \\pi(\\text{Response} \\mid \\text{Prompt})\n",
    "$\n",
    "\n",
    "- **Post-training Method 2: Direct Preference Optimization (DPO)**: Prompt + Good and Bad Responses (~1K-1B tokens)\n",
    "\n",
    "formula:\n",
    "$\n",
    "\\min_{\\pi} -\\log \\sigma \\left( \\beta \\left( \n",
    "\\log \\frac{\\pi(\\text{Good R} \\mid \\text{Prompt})}{\\pi_{\\text{ref}}(\\text{Good R} \\mid \\text{Prompt})}\n",
    "- \\log \\frac{\\pi(\\text{Bad R} \\mid \\text{Prompt})}{\\pi_{\\text{ref}}(\\text{Bad R} \\mid \\text{Prompt})}\n",
    "\\right) \\right)\n",
    "$\n",
    "\n",
    "- **Post-training Method 3: Online Reinforcement Learning**: Prompt-Response + Reward Function (~1K-1M prompts)\n",
    "\n",
    "formula:\n",
    "$\n",
    "\\max_{\\pi} \\ \\text{Reward}(\\text{Prompt}, \\text{Response}(\\pi))\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f72b5eb-21f3-45d2-ae26-8ea85d5662f7",
   "metadata": {},
   "source": [
    "## Post-training Requires Getting 3 Elements Right\n",
    "\n",
    "### Data & algorithm co-desing\n",
    "- SFT\n",
    "- DPO\n",
    "- Reinforce/RLOO\n",
    "- GPRO\n",
    "- PPO\n",
    "  \n",
    "### Reliable and efficient library\n",
    "- HuggingFace TRL\n",
    "- OpenRLHF\n",
    "- veRL\n",
    "- Nemo RL\n",
    "\n",
    "### Appropriate evaluation suite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540fe21b-3f96-434f-b21e-170caba20641",
   "metadata": {},
   "source": [
    "## (An incomplete List of) Popular LLM Evals\n",
    "- Human Preferences for chat: **Chatbot Arena**\n",
    "- LLM as a judge for chat: Alpaca Eval, MT Bench, **Arena Hard V1/V2**\n",
    "- Static Benchmarks for Instruct LLM: **LivecodeBench**, **AIME 2024/2025**, GPQA, MMLU Pro, IFEval\n",
    "- Function Calling & Agent: BFCL V2/V3, NexusBench V1/V2, **TauBench**, **ToolSandbox**\n",
    "\n",
    "- *It's easy to improve any one of the benchmarks*\n",
    "- *It's much harder to improve* **without degrading other domains**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74202783-9fdf-43e5-b250-e7e8eede5ebd",
   "metadata": {},
   "source": [
    "## Do you really need post-training?\n",
    "\n",
    "| Use Cases                                                                 | Methods                                      | Characteristics                                                                                  |\n",
    "|---------------------------------------------------------------------------|----------------------------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| **Follow a few instructions** (do not discuss XXX)                        | Prompting                                    | Simple yet brittle: models may not always follow all instructions                               |\n",
    "| Query real-time database or knowledgebase                                 | Retrieval-Augmented Generation (RAG) or Search | Adapt to rapidly-changing knowledgebase                                                          |\n",
    "| Create a medical LLM / Cybersecurity LLM                                  | Continual Pre-training + Post-training       | Inject large-scale domain knowledge (>1B tokens) not seen during pre-training                    |\n",
    "| **Follow 20+ instructions tightly**; Improve targeted capabilities (“Create a strong SQL / function calling / reasoning model”) | Post-training                               | Reliably change model behavior & improve targeted capabilities; May degrade other capabilities if not done right |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2e596-ceb2-4fae-956f-4a9a5c4dbcd4",
   "metadata": {},
   "source": [
    "## SFT: Imitating Example Responses\n",
    "\n",
    "### Lost function\n",
    "\n",
    "SFT minimizes negative log likelihood for the responses (maximizes likelihood) with cross entropy loss:\n",
    "\n",
    "$\n",
    "\\mathcal{L}_{\\text{SFT}} = - \\sum_{i=1}^{N} \\log \\left( p_{\\theta}(\\text{Response}(i) \\mid \\text{Prompt}(i)) \\right)\n",
    "$\n",
    "\n",
    "### Best Use Cases for SFT\n",
    "\n",
    "**Jumpstarting new model bahavior**\n",
    "\n",
    "- Pre-trained models -> Instruct models\n",
    "- Non-reasoning models -> reasoning models\n",
    "- Let the model uses certain tools without providing tool descriptions in the prompt\n",
    "\n",
    "**Improving model capabilities**\n",
    "- Distilling capabilities for small models by training on high-quality synthetic data generated from larger models\n",
    "\n",
    "### Principles of SFT Data Curation\n",
    "\n",
    "**Common methods for high-quality SFT data curation**:\n",
    "\n",
    "- **Distillation**: Generate repsonses from a stronger and larger instruct model\n",
    "- **Best of K/rejection sampling** Generate multiple responses from the original model, select the best among them\n",
    "- **Filtering**: Start from larger scale SFT dataset, filter according to the quality of repsonses and diversity of the prompts\n",
    "\n",
    "**Quality > quantity for improving capabilities** 1000 high-quality, diverse data > 1000000 mixed-quality data\n",
    "\n",
    "\n",
    "### Full Fine-tuning vs Parameter Efficient Fine-tuning (PEFT)\n",
    "\n",
    "**Full Fine-tuning**\n",
    "\n",
    "$\n",
    "h = (W + \\Delta W)x\n",
    "$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $W, \\Delta W \\in \\mathbb{R}^{d \\times d}$\n",
    "* $h, x \\in \\mathbb{R}^{d \\times 1}$\n",
    "\n",
    "\n",
    "**PEFT (e.g., LoRA):**\n",
    "\n",
    "$\n",
    "h = (W + BA)x\n",
    "$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $B \\in \\mathbb{R}^{d \\times r}$\n",
    "* $A \\in \\mathbb{R}^{r \\times d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bba8de-3bed-4aac-8c67-00be6df19f74",
   "metadata": {},
   "source": [
    "## DPO: Contrastive Learning from Positive and Negative Samples\n",
    "\n",
    "### Lost Function\n",
    "\n",
    "DPO **minimizes** the contrastive loss which penalizes negative response and encourages positive response\n",
    "\n",
    "DPO loss is cross entropy loss on the reward difference of a \"re-parameterized\" reward model\n",
    "\n",
    "$\n",
    "\\mathcal{L}_{\\text{DPO}} = -\\log \\sigma \\left( \\beta \\left( \n",
    "\\log \\frac{\\pi_{\\theta}(y_{\\text{pos}} \\mid x)}{\\pi_{\\text{ref}}(y_{\\text{pos}} \\mid x)} \n",
    "- \\log \\frac{\\pi_{\\theta}(y_{\\text{neg}} \\mid x)}{\\pi_{\\text{ref}}(y_{\\text{neg}} \\mid x)} \n",
    "\\right) \\right)\n",
    "$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\mathcal{L}_{\\text{DPO}}$: Direct Preference Optimization loss\n",
    "\n",
    "$\\sigma$: Sigmoid function\n",
    "\n",
    "$\\beta$: Temperature/hyperparameter\n",
    "\n",
    "$\\pi_{\\theta}$: Fine-tuned model's probability\n",
    "\n",
    "$\\pi_{\\text{ref}}$: Reference (original) model's probability\n",
    "\n",
    "$y_{\\text{pos}}, y_{\\text{neg}}$: Positive/Negative responses\n",
    "\n",
    "$\n",
    "\\log \\frac{\\pi_{\\theta}(y_{\\text{neg}} \\mid x)}{\\pi_{\\text{ref}}(y_{\\text{neg}} \\mid x)} \n",
    "$: Reparameterized reward model for preference\n",
    "\n",
    "### Best Use Cases for DPO\n",
    "\n",
    "**Changing model behavior** Making small modifications of model responses: Identity, Multilingual, Instruction following, Safety,etc.\n",
    "\n",
    "**Improving model capabilities**\n",
    "- Better than SFT in improving model capabilities due to contrastive nature\n",
    "- Online DPO is better for improving capabilities than offline DPO\n",
    "\n",
    "### Principles of DPO Data Curation\n",
    "\n",
    "#### Common methods for high-quality DPO data curation:\n",
    "\n",
    "**Correction**: Generate responses from original model as negative, make enhancements as positive response. **Example** I'm Llama (negative) -> I'm Athene (Positve)\n",
    "\n",
    "**Online/On-policy**: Your positive & negative exmple can both come from your model's distribution. One may generate multiple repsonses from the current model for the same prompt, and collect the best repsonse as positive sample and the worst repsonse as negative. One can choose best/ worst response based on reward functions / human judgement\n",
    "\n",
    "**Avoid overfitting**: DPO is doing reward learning with can easily overfit to some shortcut when the preferred answers have shortcuts to learn compared with  non-preferred answers. **Example**: when positive sample always contains a few special words while negative samples do not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb8237-2855-4a17-9379-0e3c68636263",
   "metadata": {},
   "source": [
    "## Reinforcement Learning for LLMs: Online vs Offline\n",
    "\n",
    "**Online Learning (Let model explore better responses by itself)**\n",
    "- The model learns by generating new responses in real time - it iteratively collects new responses and their reward, updates its weights, and explores new responses as it learns.\n",
    "\n",
    "**Offline Learning**\n",
    "- The model learns purely from a pre-collected prompt-repsonse (-reward) tuple. No fresh responses generated during learning process.\n",
    "\n",
    "### Reward Function in Online RL\n",
    "\n",
    "**Option 1: Trained Reward Model**\n",
    "\n",
    "- Usually initialized from an exsiting instruct model, then trained on large-scale human, machine generated preferences data\n",
    "- Works for any open-ended generations.\n",
    "- Good for improving chat & safety.\n",
    "- Less accurate for correctness-based domains like coding, math, function calling, etc.\n",
    "\n",
    "**Option 2: Verifiable Reward**\n",
    "- Requires preparation of ground truth for math, unit tests for coding, or sandbox execution environment for multi-turn agentic behavior.\n",
    "- More reliable than reward model in those domains.\n",
    "- Used more often for training reasoning models.\n",
    "\n",
    "### Policy Training in Online RL\n",
    "\n",
    "<img src=\"./policy.png\" alt=\"img\" style=\"display:block; margin-left:auto; margin-right:auto;\">\n",
    "\n",
    "This image illustrates two reinforcement learning (RL) algorithms: PPO (Proximal Policy Optimization) and GRPO (Generalized Relative Policy Optimization). Both are techniques used to train policies in online RL settings, but they have different structures and processes.\n",
    "PPO (Proximal Policy Optimization)\n",
    "\n",
    "PPO is a popular RL algorithm that works by optimizing a surrogate objective to ensure that the policy doesn’t change too much during each update. Here's how it works based on the diagram:\n",
    "\n",
    "    Policy Model (q): The agent uses a policy model that decides on actions based on states.\n",
    "\n",
    "    State (o): The environment provides an observation or state, which is input to the policy model.\n",
    "\n",
    "    Reference Model & Reward Model: These models help evaluate the performance of the agent, where the reference model provides a baseline, and the reward model estimates the expected reward.\n",
    "\n",
    "    Reward (r): The environment gives feedback in the form of a reward, which the agent uses to update its policy.\n",
    "\n",
    "    Value Model (v): This model estimates the value of being in a certain state, helping the agent assess how good the state is in terms of expected future rewards.\n",
    "\n",
    "    Generalized Advantage Estimation (GAE): GAE helps improve the estimation of advantages by combining rewards and value estimates. It’s used to calculate how much better an action was compared to the expected value.\n",
    "\n",
    "    Advantage (A): The advantage function helps determine whether an action was better or worse than expected, and PPO tries to optimize this.\n",
    "\n",
    "GRPO (Group Relative Policy Optimization)\n",
    "\n",
    "GRPO is an extension of PPO, aiming to address limitations in policy updates by considering multiple actions in a group instead of updating based on single actions. Here’s how GRPO is different:\n",
    "\n",
    "    Policy Model (q): Similar to PPO, GRPO uses a policy model for decision-making.\n",
    "\n",
    "    State (o1, o2, ..., og): Instead of a single state, the agent processes multiple states or observations in a group (o1, o2,...og).\n",
    "\n",
    "    Reference Model & Reward Model: These models are similar to PPO, helping assess performance with the reward feedback from the environment.\n",
    "\n",
    "    Rewards (r1, r2,..., rg): GRPO tracks rewards for multiple actions within a group (r1, r2,..., rg).\n",
    "\n",
    "    Group Computation: This refers to processing rewards and actions in groups, which helps improve stability and efficiency when optimizing policies.\n",
    "\n",
    "    Advantage (A1, A2,..., Ag): The advantage is computed for each action in the group, and this information is used to optimize the policy.\n",
    "\n",
    "Key Differences Between PPO and GRPO\n",
    "\n",
    "    Action Processing: PPO updates the policy based on a single action and state, whereas GRPO processes multiple actions at once, using a group of observations and rewards to make the update more stable and efficient.\n",
    "\n",
    "    Model Grouping: GRPO’s use of a \"group computation\" for multiple actions contrasts with PPO's more individualistic approach for each update.\n",
    "\n",
    "    Advantage Calculation: Both methods use advantages for optimization, but GRPO does this for multiple actions in a group, making it potentially more robust for certain environments.\n",
    "\n",
    "Both approaches rely on models for reference, reward, and value estimation to improve the agent's policy, with GRPO offering a more generalized version that can potentially enhance performance in complex environments.\n",
    "\n",
    "\n",
    "$\n",
    "\\mathcal{J}_{PPO}(\\theta) = \\mathbb{E}_{q \\sim \\mathcal{P}(Q), o \\sim \\pi_{\\theta_{\\text{old}}} (O|q)} \\left[ \\frac{1}{|O|} \\sum_{t=1}^{|O|} \\min \\left[ \\frac{\\pi_{\\theta}(o_t | q, o_{<t})}{\\pi_{\\theta_{\\text{old}}}(o_t | q, o_{<t})} A_t, \\, \\text{clip} \\left( \\frac{\\pi_{\\theta}(o_t | q, o_{<t})}{\\pi_{\\theta_{\\text{old}}}(o_t | q, o_{<t})}, 1 - \\epsilon, 1 + \\epsilon \\right) A_t \\right] \\right]\n",
    "$\n",
    "\n",
    "### GRPO vs PPO\n",
    "\n",
    "Both GRPO and PPO are very effective online algorithm\n",
    "\n",
    "**GRPO**\n",
    "- Well-suited for binary (often correctness-based) reward\n",
    "- Requires learger amount of samples\n",
    "- Requires less GPU memory (no value model needed)\n",
    "\n",
    "**PPO**:\n",
    "- Works well with reward model or binary reward\n",
    "- More sample efficient with a well-trained value model\n",
    "- Requires more GPU memory (value model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
