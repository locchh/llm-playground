{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884c2bb8-d101-4345-a1a4-876b2b7a3bc7",
   "metadata": {},
   "source": [
    "### smolagents\n",
    "\n",
    "####  Key Advantages of smolagents\n",
    "\n",
    "- **Simplicity**: Minimal code complexity and abstractions, to make the framework easy to understand, adopt and extend\n",
    "- **Flexible LLM Support**: Works with any LLM through integration with Hugging Face tools and external APIs\n",
    "- **Code-First Approach**: First-class support for Code Agents that write their actions directly in code, removing the need for parsing and simplifying tool calling\n",
    "- **HF Hub Integration**: Seamless integration with the Hugging Face Hub, allowing the use of Gradio Spaces as tools\n",
    "\n",
    "####  When to use smolagents?\n",
    "\n",
    "With these advantages in mind, when should we use smolagents over other frameworks? smolagents is ideal when:\n",
    "\n",
    "- You need a lightweight and minimal solution.\n",
    "- You want to experiment quickly without complex configurations.\n",
    "- Your application logic is straightforward.\n",
    "\n",
    "####  Agent Types in smolagents\n",
    "\n",
    "Agents in smolagents operate as **multi-step agents**.\n",
    "\n",
    "Each [MultiStepAgent](https://huggingface.co/docs/smolagents/main/en/reference/agents#smolagents.MultiStepAgent) performs:\n",
    "\n",
    "- One thought\n",
    "- One tool call and execution\n",
    "\n",
    "In addition to using [CodeAgent](https://huggingface.co/docs/smolagents/main/en/reference/agents#smolagents.CodeAgent) as the primary type of agent, smolagents also supports [ToolCallingAgent](https://huggingface.co/docs/smolagents/main/en/reference/agents#smolagents.ToolCallingAgent), which writes tool calls in JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d511a40-9797-42e0-9d16-3559ca544075",
   "metadata": {},
   "source": [
    "###  Model Integration in smolagents\n",
    "\n",
    "`smolagents` supports flexible LLM integration, allowing you to use any callable model that meets [certain criteria](https://huggingface.co/docs/smolagents/main/en/reference/models). The framework provides several predefined classes to simplify model connections:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60872aaf-e250-4cab-aa88-37f52450f639",
   "metadata": {},
   "source": [
    "####  TransformersModel\n",
    "\n",
    "For convenience, we have added a `TransformersModel` that implements the points above by building a local transformers pipeline for the model_id given at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045e830a-76a1-45ec-a813-74d7872a2dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_new_tokens` not provided, using this default value for `max_new_tokens`: 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatMessage(role='assistant', content='assistant\\nWhat a ', tool_calls=None, raw={'out': tensor([[    1,  4093,   198, 24487,    17,     2,   198,     1,   520,  9531,\n",
      "           198,  1780,   253,  1109]], device='cuda:0'), 'completion_kwargs': {'max_new_tokens': 5000}})\n"
     ]
    }
   ],
   "source": [
    "from smolagents import TransformersModel\n",
    "\n",
    "model = TransformersModel(model_id=\"HuggingFaceTB/SmolLM-135M-Instruct\")\n",
    "\n",
    "print(model([{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Ok!\"}]}], stop_sequences=[\"great\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b3a1e-868c-476a-880e-132af0585aa8",
   "metadata": {},
   "source": [
    "#### HfApiModel\n",
    "\n",
    "The `HfApiModel` wraps huggingface_hub’s [InferenceClient](https://huggingface.co/docs/huggingface_hub/main/en/guides/inference) for the execution of the LLM. It supports both HF’s own [Inference API](https://huggingface.co/docs/api-inference/index) as well as all [Inference Providers](https://huggingface.co/blog/inference-providers) available on the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e4d9ba-5031-44d9-bc01-da5b66058ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatMessage(role='assistant', content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", tool_calls=None, raw=ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", tool_calls=None), logprobs=None)], created=1742699152, id='', model='Qwen/Qwen2.5-Coder-32B-Instruct', system_fingerprint='3.0.1-sha-bb9095a', usage=ChatCompletionOutputUsage(completion_tokens=34, prompt_tokens=35, total_tokens=69), object='chat.completion'))\n"
     ]
    }
   ],
   "source": [
    "from smolagents import HfApiModel\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, how are you?\"}]}\n",
    "]\n",
    "\n",
    "model = HfApiModel()\n",
    "print(model(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2c54c-6cf1-4cd9-896b-52f69cd47e2f",
   "metadata": {},
   "source": [
    "####  LiteLLMModel\n",
    "\n",
    "The `LiteLLMModel` leverages [LiteLLM](https://www.litellm.ai/) to support 100+ LLMs from various providers. You can pass kwargs upon model initialization that will then be used whenever using the model, for instance below we pass `temperature`.\n",
    "\n",
    "```python\n",
    "from smolagents import LiteLLMModel\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, how are you?\"}]}\n",
    "]\n",
    "\n",
    "model = LiteLLMModel(model_id=\"anthropic/claude-3-5-sonnet-latest\", temperature=0.2, max_tokens=10)\n",
    "print(model(messages))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a708a-7299-4305-8f78-ecdabb5d86a8",
   "metadata": {},
   "source": [
    "####  OpenAIServerModel\n",
    "\n",
    "This class lets you call any OpenAIServer compatible model. Here’s how you can set it (you can customise the api_base url to point to another server):\n",
    "\n",
    "```python\n",
    "import os\n",
    "from smolagents import OpenAIServerModel\n",
    "\n",
    "model = OpenAIServerModel(\n",
    "    model_id=\"gpt-4o\",\n",
    "    api_base=\"https://api.openai.com/v1\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ed64a-d58d-489f-9fd2-8d78fc66f99e",
   "metadata": {},
   "source": [
    "#### AzureOpenAIServerModel\n",
    "\n",
    "`AzureOpenAIServerModel` allows you to connect to any Azure OpenAI deployment.\n",
    "\n",
    "Below you can find an example of how to set it up, note that you can omit the `azure_endpoint`, `api_key`, and `api_version arguments`, provided you’ve set the corresponding environment variables — `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, and `OPENAI_API_VERSION`.\n",
    "\n",
    "Pay attention to the lack of an `AZURE_` prefix for `OPENAI_API_VERSION`, this is due to the way the underlying [openai](https://github.com/openai/openai-python) package is designed.\n",
    "\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "from smolagents import AzureOpenAIServerModel\n",
    "\n",
    "model = AzureOpenAIServerModel(\n",
    "    model_id = os.environ.get(\"AZURE_OPENAI_MODEL\"),\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.environ.get(\"OPENAI_API_VERSION\")    \n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
