{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a956e8b5-4899-4166-8b1c-aa5633ebfa6a",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "First, we need to install the AgentChat and Extension packages `pip install -U \"autogen-agentchat\" \"autogen-ext[openai,azure]\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cfc3e0c-fd8a-4254-9735-1d8c88fad756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3ffe0da-86de-4a52-9132-65feb3cd6835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Deinfe a model client, you can use other client that implements\n",
    "# the 'ChatCompletionClient' interface\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4.1-nano\", # $0.20 / 1M tokens\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a911f50-7855-4275-bff6-eef340f25653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple function tool that the agent can use.\n",
    "# For this examplem we use a fake weather tool for demonstration purpose.\n",
    "\n",
    "async def get_weather(city:str)->str:\n",
    "    \"\"\"\n",
    "    Get the weather for a given city.\n",
    "    \"\"\"\n",
    "    return f\"The weather in {city} is 73 degrees and Sunny!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8673d2-5f5e-47a1-a04d-613304ba8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "# Define a AssistantAgent with the model, tool, system message and reflection enabled\n",
    "# The system message instructs the agent via natural language.\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\"\"\"\n",
    "\n",
    "agent = AssistantAgent(\n",
    "    name=\"weather_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_weather],\n",
    "    system_message=system_message,\n",
    "    reflect_on_tool_use=True,\n",
    "    model_client_stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d2f5e-ad5f-422a-9765-2fe586609a34",
   "metadata": {},
   "source": [
    "- `reflect_on_tool_use`: When enabled, the agent will analyze and learn from its tool usage patterns. The agent will generate additional reflections after using tools Helps the agent improve its tool usage over time. Can provide better context for future interactions. Must be explicitly set (no default value)\n",
    "\n",
    "- `model_client_stream (bool)`: Controls whether the model client streams tokens as they're generated. When False: Waits for the complete response before returning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d723b664-4260-4585-bcc2-dc2b74a39677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "# Run the agent and stream the messages to the console\n",
    "async def main() -> None:\n",
    "    await Console(agent.run_stream(task=\"What is the weather in New York?\"))\n",
    "    # Close the connection to the model client\n",
    "    await model_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a61c7b4-d8a3-4d8b-9770-d8918e6bd2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "What is the weather in New York?\n",
      "---------- ToolCallRequestEvent (weather_agent) ----------\n",
      "[FunctionCall(id='call_CEoA37mOFgshOmVpGsbpDuNg', arguments='{\"city\": \"New York\"}', name='get_weather')]\n",
      "---------- ToolCallExecutionEvent (weather_agent) ----------\n",
      "[FunctionExecutionResult(content='The weather in New York is 73 degrees and Sunny!', name='get_weather', call_id='call_CEoA37mOFgshOmVpGsbpDuNg', is_error=False)]\n",
      "---------- ModelClientStreamingChunkEvent (weather_agent) ----------\n",
      "The weather in New York is 73 degrees and sunny.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c3322-73ee-4bf5-8d60-827726b9f71f",
   "metadata": {},
   "source": [
    "# Explaination\n",
    "\n",
    "Let me break down the output you're seeing, which is a sequence of events in an agent's interaction. I'll explain each component and its role in the `autogen_agentchat` framework.\n",
    "\n",
    "## **Event Flow Explanation**\n",
    "\n",
    "### **1. TextMessage (user)**\n",
    "\n",
    "**Plaintext:**\n",
    "\n",
    "```\n",
    "What is the weather in New York?\n",
    "```\n",
    "\n",
    "**What it is:** A message from the user to the agent\n",
    "\n",
    "**Role:** Represents the initial user input\n",
    "\n",
    "**Implementation:** Defined in `autogen_agentchat/messages.py` as a basic message type\n",
    "\n",
    "---\n",
    "\n",
    "### **2. ToolCallRequestEvent (weather\\_agent)**\n",
    "\n",
    "**Plaintext:**\n",
    "\n",
    "```\n",
    "[FunctionCall(id='call_CEoA37mOFgshOmVpGsbpDuNg', arguments='{\"city\": \"New York\"}', name='get_weather')]\n",
    "```\n",
    "\n",
    "**What it is:** The agent's request to execute a tool\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "* **id:** Unique identifier for this tool call\n",
    "* **name:** The tool being called (`get_weather`)\n",
    "* **arguments:** The parameters being passed (`{\"city\": \"New York\"}`)\n",
    "\n",
    "**Implementation:** Defined in `autogen_agentchat/messages.py` as an event signaling tool usage\n",
    "\n",
    "---\n",
    "\n",
    "### **3. ToolCallExecutionEvent (weather\\_agent)**\n",
    "\n",
    "**Plaintext:**\n",
    "\n",
    "```\n",
    "[FunctionExecutionResult(content='The weather in New York is 73 degrees and Sunny!', name='get_weather', call_id='call_CEoA37mOFgshOmVpGsbpDuNg', is_error=False)]\n",
    "```\n",
    "\n",
    "**What it is:** The result of the tool execution\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "* **content:** The actual result from the tool\n",
    "* **name:** The tool that was executed\n",
    "* **call\\_id:** Matches the ID from the `ToolCallRequestEvent`\n",
    "* **is\\_error:** Indicates if there was an error\n",
    "\n",
    "**Implementation:** Defined in `autogen_agentchat/messages.py` as an event signaling tool execution results\n",
    "\n",
    "---\n",
    "\n",
    "### **4. ModelClientStreamingChunkEvent (weather\\_agent)**\n",
    "\n",
    "**Plaintext:**\n",
    "\n",
    "```\n",
    "The weather in New York is 73 degrees and sunny.\n",
    "```\n",
    "\n",
    "**What it is:** The final, formatted response being streamed to the user\n",
    "\n",
    "**Role:** Provides real-time streaming of the model's response\n",
    "\n",
    "**Implementation:** Defined in `autogen_agentchat/messages.py` for streaming text chunks\n",
    "\n",
    "---\n",
    "\n",
    "## **How It Works Together**\n",
    "\n",
    "1. The user sends a question about the weather.\n",
    "2. The agent (`weather_agent`) processes this and determines it needs to call the `get_weather` tool.\n",
    "3. A **`ToolCallRequestEvent`** is generated with the tool call details.\n",
    "4. The tool executes and produces a **`ToolCallExecutionEvent`** with the result.\n",
    "5. The agent formats the result and streams it back as a **`ModelClientStreamingChunkEvent`**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Components in the Codebase**\n",
    "\n",
    "### **Message Types (in `messages.py`)**\n",
    "\n",
    "* **TextMessage:** For simple text messages\n",
    "\n",
    "* **ToolCallRequestEvent:** For tool call requests\n",
    "\n",
    "* **ToolCallExecutionEvent:** For tool execution results\n",
    "\n",
    "* **ModelClientStreamingChunkEvent:** For streaming text chunks\n",
    "\n",
    "### **Agent Implementation**\n",
    "\n",
    "* **AssistantAgent** (in `_assistant_agent.py`): Handles the logic for processing messages and generating tool calls\n",
    "\n",
    "* **CodeExecutorAgent** (in `_code_executor_agent.py`): Handles code execution if needed\n",
    "\n",
    "### **Tool Integration**\n",
    "\n",
    "* Tools are defined as callable objects that the agent can invoke\n",
    "* The agent uses the tool's schema to generate proper function calls\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This **event-based architecture** allows for **flexible and observable agent interactions**, making it easier to **debug** and **extend** the system's capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
