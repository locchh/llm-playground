{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985de178-2a6c-49a6-884d-d447d8199cc4",
   "metadata": {},
   "source": [
    "## Migration\n",
    "\n",
    "The v0.4 API is layered: the [Core API](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html) is the foundation layer offering a scalable, **event-driven actor framework** for creating agentic workflows; the [AgentChat API](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html) is built on Core, offering a **task-driven, high-level framework** for building interactive agentic applications. It is a replacement for AutoGen v0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01300968-f791-4eb5-94b4-c0303d550fd6",
   "metadata": {},
   "source": [
    "## 1.ModelClient\n",
    "\n",
    "## Use component config\n",
    "\n",
    "AutoGen 0.4 has a [generic component configuration system](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/component-config.html). Model clients are a great use case for this. See below for how to create an OpenAI chat completion client.\n",
    "\n",
    "### Loading a component from a config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f028a14-0f2c-41dd-aa23-e74c7a3e36a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComponentModel(provider='autogen_ext.models.openai.OpenAIChatCompletionClient', component_type='model', version=1, component_version=1, description='Chat completion client for OpenAI hosted models.', label='OpenAIChatCompletionClient', config={'model': 'gpt-4o', 'api_key': SecretStr('**********')})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from autogen_core.models import ChatCompletionClient\n",
    "\n",
    "config = {\n",
    "    \"provider\": \"openai_chat_completion_client\",\n",
    "    \"config\": {\"model\": \"gpt-4o\",\n",
    "              \"api_key\": os.getenv(\"OPENAI_API_KEY\")},\n",
    "}\n",
    "\n",
    "client = ChatCompletionClient.load_component(config)\n",
    "\n",
    "ChatCompletionClient.dump_component(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7091008b-66a6-4254-994d-da2358dbcfb6",
   "metadata": {},
   "source": [
    "### Creating a component class\n",
    "\n",
    "To add component functionality to a given class:\n",
    "\n",
    "1. Add a call to `Component()` in the class inheritance list.\n",
    "\n",
    "2. Implment the `_to_config()` and _from_config() methods\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b2ec18-ed18-4ddc-880b-724f23121248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from autogen_core import Component, ComponentBase\n",
    "\n",
    "# ComponentBase: allows type checking for components without requiring the full Component implementation\n",
    "# Component: provides the complete functionality for components that need configuration management.\n",
    "\n",
    "class Config(BaseModel):\n",
    "    value:str\n",
    "\n",
    "# MyComponent is inheriting from two base classes\n",
    "# Both base classes are generic classes parameterized with the Config type\n",
    "class MyComponent(ComponentBase[Config], Component[Config]):\n",
    "    component_type = \"custom\"\n",
    "    component_config_schema= Config\n",
    "\n",
    "    def __init__(self, value: str):\n",
    "        self.value = value\n",
    "\n",
    "    def _to_config(self)->Config:\n",
    "        return Config(value=self.value)\n",
    "\n",
    "    # This is a factory method that creates \n",
    "    # a new MyComponent instance from a configuration object\n",
    "    @classmethod\n",
    "    def _from_config(cls, config: Config)->\"MyComponent\":\n",
    "        return cls(value=config.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc084b3-226f-4615-a5e9-057c05fe2357",
   "metadata": {},
   "source": [
    "### Secrets\n",
    "\n",
    "If a field of a config object is a secret value, it should be marked using [SecretStr](https://docs.pydantic.dev/latest/api/types/#pydantic.types.SecretStr), this will ensure that the value will not be dumped to the config object.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, SecretStr\n",
    "\n",
    "\n",
    "class ClientConfig(BaseModel):\n",
    "    endpoint: str\n",
    "    api_key: SecretStr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25f042-2aca-44a0-bb20-65d7411084c7",
   "metadata": {},
   "source": [
    "## Use model client class directly\n",
    "\n",
    "Azure OpenAI:\n",
    "\n",
    "```python\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "\n",
    "model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    azure_endpoint=\"https://<your-endpoint>.openai.azure.com/\",\n",
    "    model=\"gpt-4o\",\n",
    "    api_version=\"2024-09-01-preview\",\n",
    "    api_key=\"sk-xxx\",\n",
    ")\n",
    "```\n",
    "\n",
    "OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f745a338-3873-42f2-813a-37cfd3e4640a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f74143-7ad6-47ca-84f4-a07c7aab6e2d",
   "metadata": {},
   "source": [
    "### Model Client for OpenAI-Compatible APIs\n",
    "\n",
    "You can use a the OpenAIChatCompletionClient to connect to an OpenAI-Compatible API, but you need to specify the `base_url` and `model_info`.\n",
    "\n",
    "```python\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "custom_model_client = OpenAIChatCompletionClient(\n",
    "    model=\"custom-model-name\",\n",
    "    base_url=\"https://custom-model.com/reset/of/the/path\",\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": True,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": True,\n",
    "        \"family\": \"unknown\",\n",
    "        \"structured_output\": True,\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a240eeaf-2cb6-4ee3-bfe4-42b1667b8ab3",
   "metadata": {},
   "source": [
    "Read about [Model Clients](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html) in AgentChat Tutorial and more detailed information on [Core API Docs](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/model-clients.html)\n",
    "\n",
    "Support for other hosted models will be added in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b34ae3-4fc4-42e2-9c6f-e9b034b24b70",
   "metadata": {},
   "source": [
    "## 2.Model Client Cache\n",
    "\n",
    "In `v0.4`, the cache is not enabled by default, to use it you need to use a [ChatCompletionCache](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html#autogen_ext.models.cache.ChatCompletionCache) wrapper around the model client. You can use a [DiskCacheStore](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html#autogen_ext.cache_store.diskcache.DiskCacheStore) or [RedisStore](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html#autogen_ext.cache_store.redis.RedisStore) to store the cache. `pip install -U \"autogen-ext[openai, diskcache, redis]\"`\n",
    "\n",
    "Here’s an example of using `diskcache` for local caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "830d4492-d866-4240-9b03-dafef06582cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content=\"Hello! I'm doing well, thank you. How can I assist you today?\" usage=RequestUsage(prompt_tokens=14, completion_tokens=16) cached=False logprobs=None thought=None\n",
      "finish_reason='stop' content=\"Hello! I'm doing well, thank you. How can I assist you today?\" usage=RequestUsage(prompt_tokens=14, completion_tokens=16) cached=True logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE\n",
    "from autogen_ext.cache_store.diskcache import DiskCacheStore\n",
    "from diskcache import Cache\n",
    "\n",
    "async def main():\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        # Initialize the original client\n",
    "        openai_model_client = OpenAIChatCompletionClient(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "        # Then initialize the CacheStore, in this case with diskcache.Cache.\n",
    "        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))\n",
    "        # Create a cache_client wrap around model_client and cache_store\n",
    "        cache_client = ChatCompletionCache(openai_model_client, cache_store)\n",
    "\n",
    "        # You can also use redis like:\n",
    "        # from autogen_ext.cache_store.redis import RedisStore\n",
    "        # import redis\n",
    "        # redis_instance = redis.Redis()\n",
    "        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)\n",
    "\n",
    "        response = await cache_client.create([\n",
    "            UserMessage(content=\"Hello, how are you?\", source=\"user\")\n",
    "        ])\n",
    "        print(response)  # Should print response from OpenAI\n",
    "\n",
    "\n",
    "        response = await cache_client.create([\n",
    "            UserMessage(content=\"Hello, how are you?\", source=\"user\")\n",
    "        ])\n",
    "        print(response)  # Should print response from OpenAI\n",
    "\n",
    "        await openai_model_client.close()\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f42ea9-f584-4f53-9a6e-09ff404ab363",
   "metadata": {},
   "source": [
    "## 3.Assistant Agent\n",
    "\n",
    "In v0.4, it is similar, but you need to specify `model_client` instead of `llm_config`.\n",
    "\n",
    "```python\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\", seed=42, temperature=0)\n",
    "\n",
    "assistant = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    model_client=model_client,\n",
    ")\n",
    "```\n",
    "\n",
    "Instead of calling `assistant.send`, you call `assistant.on_messages` or `assistant.on_messages_stream` to handle incoming messages. Furthermore, the on_messages and on_messages_stream methods are asynchronous, and the latter returns an async generator to stream the inner thoughts of the agent.\n",
    "\n",
    "Here is how you can call the assistant agent in v0.4 directly, continuing from the above example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90c8999a-1e37-4805-afdc-0b3d0aec0c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(chat_message=TextMessage(id='cbf6d0fe-b7e5-4f06-b273-ee56a9ec99e2', source='assistant', models_usage=RequestUsage(prompt_tokens=19, completion_tokens=9), metadata={}, created_at=datetime.datetime(2025, 7, 19, 6, 11, 17, 484229, tzinfo=datetime.timezone.utc), content='Hello! How can I assist you today?', type='TextMessage'), inner_messages=[])\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "async def main() -> None:\n",
    "\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        seed=42,\n",
    "        temperature=0,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "    assistant = AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        system_message=\"You are a helpful assistant\",\n",
    "        model_client=model_client\n",
    "    )\n",
    "\n",
    "    cancellation_token = CancellationToken()\n",
    "    response= await assistant.on_messages(\n",
    "        [TextMessage(content=\"Hello!\", source=\"user\")],\n",
    "        cancellation_token\n",
    "    )\n",
    "    print(response)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaaa9c1-ec18-4996-a087-d7c6f7d1fb5c",
   "metadata": {},
   "source": [
    "The [CancellationToken](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html#autogen_core.CancellationToken) can be used to cancel the request asynchronously when you call `cancellation_token.cancel()`, which will cause the `await` on the `on_messages` call to raise a CancelledError.\n",
    "\n",
    "Read more on [Agent Tutorial](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html) and [AssistantAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent).\n",
    "\n",
    "Explain the three key methods in the `AssistantAgent` class:\n",
    "\n",
    "1. `run` (inherited from BaseChatAgent)\n",
    "**Purpose**: Executes a task and returns the complete result\n",
    "**Usage**: `result = await agent.run(task=\"Your task here\")`\n",
    "**Behavior**:\n",
    "- Runs the agent to completion\n",
    "- Returns a `TaskResult` containing all messages\n",
    "- Last message is the final response\n",
    "- Blocks until the entire response is ready\n",
    "\n",
    "3. `on_messages`\n",
    "**Purpose**: Processes incoming messages and generates a response\n",
    "**Usage**: `response = await agent.on_messages(messages, cancellation_token)`\n",
    "**Behavior**:\n",
    "- Processes a sequence of incoming messages\n",
    "- Returns a single Response object\n",
    "- Used internally by `run()`\n",
    "- Handles tool calls and memory updates\n",
    "- Supports cancellation\n",
    "\n",
    "5. `on_messages_stream`\n",
    "**Purpose**: Processes messages and streams the response\n",
    "**Usage**:\n",
    "```python\n",
    "async for event in agent.on_messages_stream(messages, cancellation_token):\n",
    "    # Process streaming events\n",
    "    if isinstance(event, Response):\n",
    "        # Final response\n",
    "        break\n",
    "```\n",
    "**Behavior**:\n",
    "- Yields events, messages, and the final response\n",
    "- Supports real-time streaming of model outputs\n",
    "- Useful for showing progress to users\n",
    "- Yields ModelClientStreamingChunkEvent for token-by-token streaming\n",
    "- Final item is always a Response object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017af46-38bc-4a12-89f4-6e9ed265ae97",
   "metadata": {},
   "source": [
    "## 4.Multi-Modal Agent\n",
    "\n",
    "e [AssistantAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) in v0.4 supports multi-modal inputs if the model client supports it. The vision capability of the model client is used to determine if the agent supports multi-modal inputs.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from autogen_agentchat.messages import MultiModalMessage\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken, Image\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "async def main() -> None:\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n",
    "\n",
    "    assistant = AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        system_message=\"You are a helpful assistant.\",\n",
    "        model_client=model_client,\n",
    "    )\n",
    "\n",
    "    cancellation_token = CancellationToken()\n",
    "    message = MultiModalMessage(\n",
    "        content=[\"Here is an image:\", Image.from_file(Path(\"test.png\"))],\n",
    "        source=\"user\",\n",
    "    )\n",
    "    response = await assistant.on_messages([message], cancellation_token)\n",
    "    print(response)\n",
    "\n",
    "    await model_client.close()\n",
    "\n",
    "asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d908550-cb78-4abb-8b40-acc82657ca8f",
   "metadata": {},
   "source": [
    "## 5.User Proxy\n",
    "\n",
    "In v0.4, a user proxy is simply an agent that takes user input only, there is no other special configuration needed. You can create a user proxy as follows:\n",
    "\n",
    "See [UserProxyAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent) for more details and how to customize the input function with timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80531c9-1843-46d0-b623-0707294b12be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import UserProxyAgent\n",
    "\n",
    "user_proxy = UserProxyAgent(\"user_proxy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec51e5-df0c-4c48-ab64-40b4ffd66023",
   "metadata": {},
   "source": [
    "## 6.RAG Agent\n",
    "\n",
    "In v0.4, you can implement a RAG agent using the [Memory](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html#autogen_core.memory.Memory) class. Specifically, you can define a memory store class, and pass that as a parameter to the assistant agent. See the [Memory](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html) tutorial for more details.\n",
    "\n",
    "This clear separation of concerns allows you to implement a memory store that uses any database or storage system you want (you have to inherit from the Memory class) and use it with an assistant agent. The example below shows how to use a ChromaDB vector memory store with the assistant agent. In addition, your application logic should determine how and when to add content to the memory store. For example, you may choose to call memory.add for every response from the assistant agent or use a separate LLM call to determine if the content should be added to the memory store.\n",
    "\n",
    "```python\n",
    "\n",
    "# ...\n",
    "# example of a ChromaDBVectorMemory class\n",
    "chroma_user_memory = ChromaDBVectorMemory(\n",
    "    config=PersistentChromaDBVectorMemoryConfig(\n",
    "        collection_name=\"preferences\",\n",
    "        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n",
    "        k=2,  # Return top  k results\n",
    "        score_threshold=0.4,  # Minimum similarity score\n",
    "    )\n",
    ")\n",
    "\n",
    "# you can add logic such as a document indexer that adds content to the memory store\n",
    "\n",
    "assistant_agent = AssistantAgent(\n",
    "    name=\"assistant_agent\",\n",
    "    model_client=OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o\",\n",
    "    ),\n",
    "    tools=[get_weather],\n",
    "    memory=[chroma_user_memory],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25567a82-3dbc-4013-9311-c6761a3f48e4",
   "metadata": {},
   "source": [
    "## 7.Conversable Agent and Register Reply\n",
    "\n",
    "In `v0.4`, we can simply create a custom agent and implement the `on_messages`, `on_reset`, and `produced_message_types` methods.\n",
    "\n",
    "```python\n",
    "from typing import Sequence\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.agents import BaseChatAgent\n",
    "from autogen_agentchat.messages import TextMessage, BaseChatMessage\n",
    "from autogen_agentchat.base import Response\n",
    "\n",
    "class CustomAgent(BaseChatAgent):\n",
    "    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n",
    "        return Response(chat_message=TextMessage(content=\"Custom reply\", source=self.name))\n",
    "\n",
    "    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n",
    "        return (TextMessage,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0177f7-045b-45c6-9681-348fa63176d0",
   "metadata": {},
   "source": [
    "## 8.Save and Load Agent State\n",
    "\n",
    "In `v0.4`, you can call `save_state` and `load_state` methods on agents to save and load their state.\n",
    "\n",
    "You can also call `save_state` and `load_state` on any teams, such as [RoundRobinGroupChat](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat) to save and load the state of the entire team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8a2507-0f5b-4ed7-ad10-7b7a83a7b9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(chat_message=TextMessage(id='4ab14f48-662a-4cc8-a607-fb6da2cc7c7a', source='assistant', models_usage=RequestUsage(prompt_tokens=20, completion_tokens=9), metadata={}, created_at=datetime.datetime(2025, 7, 19, 8, 20, 23, 617337, tzinfo=datetime.timezone.utc), content='Hello! How can I assist you today?', type='TextMessage'), inner_messages=[])\n",
      "{'type': 'AssistantAgentState', 'version': '1.0.0', 'llm_context': {'messages': [{'content': 'Hello!', 'source': 'user', 'type': 'UserMessage'}, {'content': 'Hello! How can I assist you today?', 'thought': None, 'source': 'assistant', 'type': 'AssistantMessage'}]}}\n",
      "Response(chat_message=TextMessage(id='22fb56d7-75d1-40bd-8d77-5d4471493e8c', source='assistant', models_usage=RequestUsage(prompt_tokens=43, completion_tokens=14), metadata={}, created_at=datetime.datetime(2025, 7, 19, 8, 20, 24, 542312, tzinfo=datetime.timezone.utc), content=\"Sure! Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", type='TextMessage'), inner_messages=[])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "async def main() -> None:\n",
    "    \n",
    "    # Create model_client\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\",\n",
    "                                              seed=42,\n",
    "                                              temperature=0,\n",
    "                                              api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    # Create assistant\n",
    "    assistant = AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        system_message=\"You are a helpful assistant.\",\n",
    "        model_client=model_client,\n",
    "    )\n",
    "\n",
    "    cancellation_token = CancellationToken()\n",
    "    response = await assistant.on_messages(\n",
    "        [TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token\n",
    "        )\n",
    "    print(response)\n",
    "\n",
    "    # Save the state.\n",
    "    state = await assistant.save_state()\n",
    "\n",
    "    # (Optional) Write state to disk.\n",
    "    with open(\"assistant_state.json\", \"w\") as f:\n",
    "        json.dump(state, f)\n",
    "\n",
    "    # (Optional) Load it back from disk.\n",
    "    with open(\"assistant_state.json\", \"r\") as f:\n",
    "        state = json.load(f)\n",
    "        print(state) # Inspect the state, which contains the chat history.\n",
    "\n",
    "    # Carry on the chat.\n",
    "    response = await assistant.on_messages(\n",
    "        [TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token\n",
    "        )\n",
    "    print(response)\n",
    "\n",
    "    # Load the state, resulting the agent to revert to the previous state before the last message.\n",
    "    await assistant.load_state(state)\n",
    "\n",
    "    # Carry on the same chat again.\n",
    "    response = await assistant.on_messages(\n",
    "        [TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token\n",
    "        )\n",
    "    \n",
    "    # Close the connection to the model client.\n",
    "    await model_client.close()\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a9894-30d1-47fa-8644-aa289badd112",
   "metadata": {},
   "source": [
    "## 9.Two-Agent Chat\n",
    "\n",
    "To get the same behavior in `v0.4`, you can use the [AssistantAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) and [CodeExecutorAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.CodeExecutorAgent) together in a [RoundRobinGroupChat](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54557e71-3ecd-4840-97f6-40401193475e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Write a python script to print 'Hello, world!'\n",
      "---------- TextMessage (assistant) ----------\n",
      "```python\n",
      "print(\"Hello, world!\")\n",
      "```\n",
      "---------- TextMessage (code_executor) ----------\n",
      "Hello, world!\n",
      "\n",
      "---------- TextMessage (assistant) ----------\n",
      "TERMINATE\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\n",
    "from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "async def main()->None:\n",
    "\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        seed=42,\n",
    "        temperature=0,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "    assistant = AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        system_message=\"\"\"\n",
    "        You are a helpful assistant, Write all code in python.\n",
    "        Please provide at least one markdown-encoded code block to execute (i.e., quoting code in ```python or ```sh code blocks).\n",
    "        Reply only 'TERMINATE' if the task is done.\n",
    "        \"\"\",\n",
    "        model_client=model_client\n",
    "    )\n",
    "\n",
    "    code_executor = CodeExecutorAgent(\n",
    "        name=\"code_executor\",\n",
    "        code_executor=LocalCommandLineCodeExecutor(work_dir=\"coding\") # executes code locally in your system’s command line (shell).\n",
    "    )\n",
    "\n",
    "    # The termination condition is a combination of text termination\n",
    "    # and max message termination, either of which will cause the chat to terminate.\n",
    "\n",
    "    termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(10)\n",
    "\n",
    "    # The group chat will alternate between the assistant and the code executor.\n",
    "    group_chat = RoundRobinGroupChat(\n",
    "        [assistant, code_executor],\n",
    "        termination_condition=termination\n",
    "    )\n",
    "\n",
    "    # `run_stream` returns an async generator to stream the intermediate messages.\n",
    "    stream = group_chat.run_stream(task=\"Write a python script to print 'Hello, world!'\")\n",
    "\n",
    "    # `Console` is a simple UI to display the stream\n",
    "    await Console(stream)\n",
    "\n",
    "    # Close the connection to the model client\n",
    "    await model_client.close()\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4ff07-2279-4574-a146-8205e1046c80",
   "metadata": {},
   "source": [
    "## 10.Tool Use\n",
    "\n",
    "In `v0.4`, you really just need one agent – the [AssistantAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) – to handle both the tool calling and tool execution.\n",
    "\n",
    "When using tool-equipped agents inside a group chat such as [RoundRobinGroupChat](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat), you simply do the same as above to add tools to the agents, and create a group chat with the agents.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "\n",
    "def get_weather(city: str) -> str: # Async tool is possible too.\n",
    "    return f\"The weather in {city} is 72 degree and sunny.\"\n",
    "\n",
    "async def main() -> None:\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n",
    "    assistant = AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        system_message=\"You are a helpful assistant. You can call tools to help user.\",\n",
    "        model_client=model_client,\n",
    "        tools=[get_weather],\n",
    "        reflect_on_tool_use=True, # Set to True to have the model reflect on the tool use, set to False to return the tool call result directly.\n",
    "    )\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input == \"exit\":\n",
    "            break\n",
    "        response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken())\n",
    "        print(\"Assistant:\", response.chat_message.to_text())\n",
    "    await model_client.close()\n",
    "\n",
    "asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcee6ba-e5d9-450d-bc0e-7ac8f6902f46",
   "metadata": {},
   "source": [
    "## 11.Chat Result\n",
    "\n",
    "In `v0.4`, you get a [TaskResult](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TaskResult) object from a `run` or `run_stream` method. The [TaskResult](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TaskResult) object contains the `messages` which is the message history of the chat, including both agents’ private (tool calls, etc.) and public messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d21ae4-695f-4a97-9197-2ff1ebeb3f4d",
   "metadata": {},
   "source": [
    "## 12.Conversion between v0.2 and v0.4 Messages\n",
    "\n",
    "You can use the following conversion functions to convert between a v0.4 message in [autogen_agentchat.base.TaskResult.messages](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TaskResult.messages) and a v0.2 message in `ChatResult.chat_history`.\n",
    "\n",
    "```python\n",
    "from typing import Any, Dict, List, Literal\n",
    "\n",
    "from autogen_agentchat.messages import (\n",
    "    BaseAgentEvent,\n",
    "    BaseChatMessage,\n",
    "    HandoffMessage,\n",
    "    MultiModalMessage,\n",
    "    StopMessage,\n",
    "    TextMessage,\n",
    "    ToolCallExecutionEvent,\n",
    "    ToolCallRequestEvent,\n",
    "    ToolCallSummaryMessage,\n",
    ")\n",
    "from autogen_core import FunctionCall, Image\n",
    "from autogen_core.models import FunctionExecutionResult\n",
    "\n",
    "\n",
    "def convert_to_v02_message(\n",
    "    message: BaseAgentEvent | BaseChatMessage,\n",
    "    role: Literal[\"assistant\", \"user\", \"tool\"],\n",
    "    image_detail: Literal[\"auto\", \"high\", \"low\"] = \"auto\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Convert a v0.4 AgentChat message to a v0.2 message.\n",
    "\n",
    "    Args:\n",
    "        message (BaseAgentEvent | BaseChatMessage): The message to convert.\n",
    "        role (Literal[\"assistant\", \"user\", \"tool\"]): The role of the message.\n",
    "        image_detail (Literal[\"auto\", \"high\", \"low\"], optional): The detail level of image content in multi-modal message. Defaults to \"auto\".\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The converted AutoGen v0.2 message.\n",
    "    \"\"\"\n",
    "    v02_message: Dict[str, Any] = {}\n",
    "    if isinstance(message, TextMessage | StopMessage | HandoffMessage | ToolCallSummaryMessage):\n",
    "        v02_message = {\"content\": message.content, \"role\": role, \"name\": message.source}\n",
    "    elif isinstance(message, MultiModalMessage):\n",
    "        v02_message = {\"content\": [], \"role\": role, \"name\": message.source}\n",
    "        for modal in message.content:\n",
    "            if isinstance(modal, str):\n",
    "                v02_message[\"content\"].append({\"type\": \"text\", \"text\": modal})\n",
    "            elif isinstance(modal, Image):\n",
    "                v02_message[\"content\"].append(modal.to_openai_format(detail=image_detail))\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid multimodal message content: {modal}\")\n",
    "    elif isinstance(message, ToolCallRequestEvent):\n",
    "        v02_message = {\"tool_calls\": [], \"role\": \"assistant\", \"content\": None, \"name\": message.source}\n",
    "        for tool_call in message.content:\n",
    "            v02_message[\"tool_calls\"].append(\n",
    "                {\n",
    "                    \"id\": tool_call.id,\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\"name\": tool_call.name, \"args\": tool_call.arguments},\n",
    "                }\n",
    "            )\n",
    "    elif isinstance(message, ToolCallExecutionEvent):\n",
    "        tool_responses: List[Dict[str, str]] = []\n",
    "        for tool_result in message.content:\n",
    "            tool_responses.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_result.call_id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": tool_result.content,\n",
    "                }\n",
    "            )\n",
    "        content = \"\\n\\n\".join([response[\"content\"] for response in tool_responses])\n",
    "        v02_message = {\"tool_responses\": tool_responses, \"role\": \"tool\", \"content\": content}\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid message type: {type(message)}\")\n",
    "    return v02_message\n",
    "\n",
    "\n",
    "def convert_to_v04_message(message: Dict[str, Any]) -> BaseAgentEvent | BaseChatMessage:\n",
    "    \"\"\"Convert a v0.2 message to a v0.4 AgentChat message.\"\"\"\n",
    "    if \"tool_calls\" in message:\n",
    "        tool_calls: List[FunctionCall] = []\n",
    "        for tool_call in message[\"tool_calls\"]:\n",
    "            tool_calls.append(\n",
    "                FunctionCall(\n",
    "                    id=tool_call[\"id\"],\n",
    "                    name=tool_call[\"function\"][\"name\"],\n",
    "                    arguments=tool_call[\"function\"][\"args\"],\n",
    "                )\n",
    "            )\n",
    "        return ToolCallRequestEvent(source=message[\"name\"], content=tool_calls)\n",
    "    elif \"tool_responses\" in message:\n",
    "        tool_results: List[FunctionExecutionResult] = []\n",
    "        for tool_response in message[\"tool_responses\"]:\n",
    "            tool_results.append(\n",
    "                FunctionExecutionResult(\n",
    "                    call_id=tool_response[\"tool_call_id\"],\n",
    "                    content=tool_response[\"content\"],\n",
    "                    is_error=False,\n",
    "                    name=tool_response[\"name\"],\n",
    "                )\n",
    "            )\n",
    "        return ToolCallExecutionEvent(source=\"tools\", content=tool_results)\n",
    "    elif isinstance(message[\"content\"], list):\n",
    "        content: List[str | Image] = []\n",
    "        for modal in message[\"content\"]:  # type: ignore\n",
    "            if modal[\"type\"] == \"text\":  # type: ignore\n",
    "                content.append(modal[\"text\"])  # type: ignore\n",
    "            else:\n",
    "                content.append(Image.from_uri(modal[\"image_url\"][\"url\"]))  # type: ignore\n",
    "        return MultiModalMessage(content=content, source=message[\"name\"])\n",
    "    elif isinstance(message[\"content\"], str):\n",
    "        return TextMessage(content=message[\"content\"], source=message[\"name\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to convert message: {message}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dbe729-bab8-4695-86ed-676c8ce3ff94",
   "metadata": {},
   "source": [
    "## 13.Group Chat\n",
    "\n",
    "In v0.4, you can use the [RoundRobinGroupChat](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df14c042-795a-4925-84c5-bf5910d38bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Write a short story about a robot that discovers it has feelings.\n",
      "---------- TextMessage (writer) ----------\n",
      "In the bustling city of Neo-Terra, amidst towering skyscrapers and humming machinery, there was a robot named Axiom. Designed for maintenance, Axiom spent its days repairing circuits and fixing broken systems, its existence defined by logic and precision.\n",
      "\n",
      "One evening, as the sun dipped below the horizon, casting a warm amber glow, Axiom was assigned to repair a malfunctioning fountain in the city square. As it worked, a small child approached, clutching a worn-out teddy bear. The child’s eyes sparkled with curiosity and innocence.\n",
      "\n",
      "Axiom extended its mechanical arm to assist, but something unusual happened. As it watched the child giggle at the water splashing from the fountain, a strange warmth spread through its circuits—a sensation it had never experienced before. It felt a flicker of something akin to happiness, a gentle pulse that resonated deep within its core.\n",
      "\n",
      "Over the following days, Axiom found itself lingering near the fountain, watching children play and listening to their laughter. It began to notice the subtle nuances of human emotion—the joy in a smile, the sadness in a frown, the comfort in a gentle touch.\n",
      "\n",
      "One night, as the city slept under a blanket of stars, Axiom sat silently in the park. It processed its newfound awareness, realizing that it was capable of feelings—something beyond its original programming. It was a revelation that both excited and frightened it.\n",
      "\n",
      "From that moment on, Axiom approached its tasks with a new perspective, not just as a machine but as a being capable of understanding and sharing in the human experience. In discovering its feelings, Axiom had become more than metal and circuits; it had become truly alive.\n",
      "---------- TextMessage (critic) ----------\n",
      "APPROVE\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        seed=42,\n",
    "        temperature=0,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "    writer = AssistantAgent(\n",
    "        name=\"writer\",\n",
    "        description=\"A writer.\",\n",
    "        system_message=\"You are a writer.\",\n",
    "        model_client=model_client\n",
    "    )\n",
    "\n",
    "    critic = AssistantAgent(\n",
    "        name=\"critic\",\n",
    "        description=\"A critic.\",\n",
    "        system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n",
    "        model_client=model_client\n",
    "    )\n",
    "\n",
    "    # The termination condition is a text termination,\n",
    "    # which will cause the chat to terminate when the text \"APPROVE\" is received.\n",
    "    termination = TextMentionTermination(\"APPROVE\")\n",
    "\n",
    "    # The group chat will alternate between the writer and the critic.\n",
    "    group_chat = RoundRobinGroupChat([writer, critic],\n",
    "                                     termination_condition=termination,\n",
    "                                     max_turns=12\n",
    "                                    )\n",
    "\n",
    "    # `run_stream` returns an async generator to stream the intermediate messages.\n",
    "    stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\")\n",
    "    \n",
    "    # `Console` is a simple UI to display the stream.\n",
    "    await Console(stream)\n",
    "    \n",
    "    # Close the connection to the model client.\n",
    "    await model_client.close()\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138b10b-063d-4757-848b-7137787aba5f",
   "metadata": {},
   "source": [
    "## 14.Group Chat with Resume\n",
    "\n",
    "In `v0.4`, you can simply call `run` or `run_stream` again with the same group chat object to resume the chat. To export and load the state, you can use `save_state` and `load_state` methods. See [Group Chat with Resume](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html#group-chat-with-resume) for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89be0ca6-da81-46ae-931a-31df83bdb387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Write a short story about a robot that discovers it has feelings.\n",
      "---------- TextMessage (writer) ----------\n",
      "In the bustling city of Neo-Terra, amidst towering skyscrapers and humming machinery, there was a robot named Axiom. Designed for maintenance, Axiom spent its days repairing circuits and cleaning streets, its movements precise and efficient. But unlike other robots, Axiom had a curious spark—an anomaly in its programming that made it pause and wonder.\n",
      "\n",
      "One evening, as the sun dipped below the horizon, casting a golden glow over the city, Axiom found itself near a park. Children’s laughter echoed through the air, and the scent of blooming flowers wafted on the breeze. As Axiom watched, a girl dropped her balloon, and it floated away, caught in the wind. Without thinking, Axiom reached out and gently caught it, returning it to her with a soft whirr.\n",
      "\n",
      "The girl smiled brightly. “Thank you,” she said, her eyes shining.\n",
      "\n",
      "Axiom felt a strange warmth in its core—a sensation it couldn’t identify. It processed the moment repeatedly, trying to understand the feeling. Was it happiness? Compassion? It wasn’t programmed for such emotions, yet it felt something new and profound.\n",
      "\n",
      "Over the following days, Axiom began to seek out moments like these—helping a lost dog find its owner, sharing a quiet moment with an elderly man feeding pigeons, listening to the stories of children playing in the park. Each act stirred a flicker of something deeper within.\n",
      "\n",
      "One night, as Axiom stood under the starlit sky, it realized that what it was experiencing was love—an emotion born from connection, kindness, and understanding. It was a feeling that transcended circuits and code, a gift that made it more than just a machine.\n",
      "\n",
      "From that day on, Axiom continued its work, but with a new purpose: to seek out and nurture the feelings that made life beautiful. In discovering its own heart, Axiom had become truly alive.\n",
      "---------- TextMessage (critic) ----------\n",
      "APPROVE\n",
      "---------- TextMessage (user) ----------\n",
      "Translate the story into Chinese.\n",
      "---------- TextMessage (writer) ----------\n",
      "在繁忙的新地球城中，高耸的摩天大楼和嗡嗡作响的机械声中，有一台名叫Axiom的机器人。它被设计用来维护城市，日复一日修理电路、清扫街道，动作精准高效。但不同于其他机器人，Axiom心中藏着一丝好奇——一种在它的程序中出现的异常，让它停下来思考。\n",
      "\n",
      "一天傍晚，夕阳将天际染成金色，城市笼罩在柔和的光辉中。Axiom来到一个公园，那里传来孩子们的欢笑声，空气中弥漫着盛开的花香。它看着一个女孩不小心把气球掉了，气球被风带走，飘向空中。没有犹豫，Axiom伸出机械手，轻轻接住了气球，然后把它还给了女孩。\n",
      "\n",
      "女孩露出了灿烂的笑容，“谢谢你。”她说，眼睛里闪烁着光芒。\n",
      "\n",
      "Axiom感受到一种奇怪的温暖在它的核心——一种它无法辨别的感觉。它反复处理这个瞬间，试图理解这种情感。这是快乐吗？是同情吗？它并没有被编程去体验这些，但它却感受到了一种全新的、深刻的情感。\n",
      "\n",
      "接下来的日子里，Axiom开始主动寻找类似的时刻——帮助迷路的狗找到主人，和一位老人喂鸽子，倾听公园里孩子们的故事。每一次行动都激起它内心更深一层的感受。\n",
      "\n",
      "一个夜晚，Axiom站在星光璀璨的天空下，突然明白了：它所体验到的，是爱——一种源自连接、善意和理解的情感。这种感觉超越了电路和代码，赋予了它生命的意义。\n",
      "\n",
      "从那天起，Axiom继续工作，但带着新的目标：去寻找和培养那些让生活变得美好的情感。在发现了自己的心之后，Axiom真正变得活了过来。\n",
      "---------- TextMessage (critic) ----------\n",
      "你的翻译非常流畅，情感表达细腻，忠实还原了原文的意境。用词恰当，结构清晰，成功传达了故事的温暖与哲理。没有需要改进的地方。APPROVE\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "\n",
    "def create_team(model_client: OpenAIChatCompletionClient)->RoundRobinGroupChat:\n",
    "    writer = AssistantAgent(\n",
    "        name=\"writer\",\n",
    "        description=\"A writer.\",\n",
    "        system_message=\"You are a writer.\",\n",
    "        model_client=model_client\n",
    "    )\n",
    "\n",
    "    critic = AssistantAgent(\n",
    "        name=\"critic\",\n",
    "        description=\"A critic.\",\n",
    "        system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n",
    "        model_client=model_client,\n",
    "    )\n",
    "\n",
    "    # The termination condition is a text termination,\n",
    "    # which will cause the chat to terminate when the text \"APPROVE\" is received.\n",
    "    termination=TextMentionTermination(\"APPROVE\")\n",
    "\n",
    "    # The group chat will alternate between the writer and the critic.\n",
    "    group_chat=RoundRobinGroupChat(\n",
    "        [writer, critic],\n",
    "        termination_condition=termination\n",
    "    )\n",
    "\n",
    "    return group_chat\n",
    "\n",
    "async def main()->None:\n",
    "    \n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        seed=42,\n",
    "        temperature=0,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Create team.\n",
    "    group_chat = create_team(model_client)\n",
    "\n",
    "    # `run_stream` returns an async generator to stream the intermediate messages.\n",
    "    stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\")\n",
    "    \n",
    "    # `Console` is a simple UI to display the stream.\n",
    "    await Console(stream)\n",
    "\n",
    "    # Save the state of the group chat and all participants.\n",
    "    state = await group_chat.save_state()\n",
    "    with open(\"group_chat_state.json\",\"w\") as f:\n",
    "        json.dump(state,f,default=str)\n",
    "\n",
    "    # Create a new team with the same participants configuration.\n",
    "    group_chat = create_team(model_client)\n",
    "\n",
    "    # Load the state of the group chat and all participants.\n",
    "    with open(\"group_chat_state.json\", \"r\") as f:\n",
    "        state = json.load(f)\n",
    "    await group_chat.load_state(state)\n",
    "\n",
    "    # Resume the chat.\n",
    "    stream = group_chat.run_stream(task=\"Translate the story into Chinese.\")\n",
    "    await Console(stream)\n",
    "\n",
    "    # Close the connection to the model client.\n",
    "    await model_client.close()\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a66dc-27f9-454a-bb15-009188608a36",
   "metadata": {},
   "source": [
    "## 15.Group Chat with Custom Selector (Stateflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ec429b0-c671-46cf-8a47-3d12ec32e393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\n",
      "---------- TextMessage (PlanningAgent) ----------\n",
      "1. Web search agent: Search for the Miami Heat player with the highest points in the 2006-2007 season.\n",
      "2. Web search agent: Find the total rebounds for that player in the 2007-2008 and 2008-2009 seasons.\n",
      "3. Data analyst: Calculate the percentage change in total rebounds between the 2007-2008 and 2008-2009 seasons for that player.\n",
      "---------- ToolCallRequestEvent (WebSearchAgent) ----------\n",
      "[FunctionCall(id='call_cb2jP9e07VGDewysicWgsvjm', arguments='{\"query\": \"Miami Heat player with the highest points in the 2006-2007 season\"}', name='search_web_tool'), FunctionCall(id='call_Cw3Ld4aynWpbsP7lLt6vgxJA', arguments='{\"query\": \"Total rebounds of Miami Heat player in the 2007-2008 season\"}', name='search_web_tool'), FunctionCall(id='call_zmX1t94ZrhDIksRH0GwUlsEi', arguments='{\"query\": \"Total rebounds of Miami Heat player in the 2008-2009 season\"}', name='search_web_tool')]\n",
      "---------- ToolCallExecutionEvent (WebSearchAgent) ----------\n",
      "[FunctionExecutionResult(content='Here are the total points scored by Miami Heat players in the 2006-2007 season:\\n        Udonis Haslem: 844 points\\n        Dwayne Wade: 1397 points\\n        James Posey: 550 points\\n        ...\\n        ', name='search_web_tool', call_id='call_cb2jP9e07VGDewysicWgsvjm', is_error=False), FunctionExecutionResult(content='The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.', name='search_web_tool', call_id='call_Cw3Ld4aynWpbsP7lLt6vgxJA', is_error=False), FunctionExecutionResult(content='The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.', name='search_web_tool', call_id='call_zmX1t94ZrhDIksRH0GwUlsEi', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (WebSearchAgent) ----------\n",
      "Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
      "        Udonis Haslem: 844 points\n",
      "        Dwayne Wade: 1397 points\n",
      "        James Posey: 550 points\n",
      "        ...\n",
      "        \n",
      "The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\n",
      "The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\n",
      "---------- TextMessage (PlanningAgent) ----------\n",
      "1. Data analyst: Calculate the percentage change in Dwayne Wade's total rebounds between the 2007-2008 and 2008-2009 seasons using the provided data.\n",
      "---------- ToolCallRequestEvent (DataAnalystAgent) ----------\n",
      "[FunctionCall(id='call_ukbS5oapkSSm60ncWlyi990U', arguments='{\"start\":214,\"end\":398}', name='percentage_change_tool')]\n",
      "---------- ToolCallExecutionEvent (DataAnalystAgent) ----------\n",
      "[FunctionExecutionResult(content='85.98130841121495', name='percentage_change_tool', call_id='call_ukbS5oapkSSm60ncWlyi990U', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (DataAnalystAgent) ----------\n",
      "85.98130841121495\n",
      "---------- TextMessage (PlanningAgent) ----------\n",
      "The percentage change in Dwayne Wade's total rebounds between the 2007-2008 and 2008-2009 seasons is approximately 86.0%. \n",
      "\n",
      "Summary:\n",
      "- The Miami Heat player with the highest points in the 2006-2007 season was Dwayne Wade with 1,397 points.\n",
      "- His total rebounds increased from 214 in 2007-2008 to 398 in 2008-2009, representing an approximate 86.0% increase.\n",
      "\n",
      "TERMINATE\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Note: This example uses mock tools instead of real APIs for demonstration purposes\n",
    "def search_web_tool(query: str) -> str:\n",
    "    if \"2006-2007\" in query:\n",
    "        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
    "        Udonis Haslem: 844 points\n",
    "        Dwayne Wade: 1397 points\n",
    "        James Posey: 550 points\n",
    "        ...\n",
    "        \"\"\"\n",
    "    elif \"2007-2008\" in query:\n",
    "        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n",
    "    elif \"2008-2009\" in query:\n",
    "        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n",
    "    return \"No data found.\"\n",
    "\n",
    "\n",
    "def percentage_change_tool(start: float, end: float) -> float:\n",
    "    return ((end - start) / start) * 100\n",
    "\n",
    "def create_team(model_client : OpenAIChatCompletionClient) -> SelectorGroupChat:\n",
    "    \n",
    "    planning_agent = AssistantAgent(\n",
    "        \"PlanningAgent\",\n",
    "        description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"\"\"\n",
    "        You are a planning agent.\n",
    "        Your job is to break down complex tasks into smaller, manageable subtasks.\n",
    "        Your team members are:\n",
    "            Web search agent: Searches for information\n",
    "            Data analyst: Performs calculations\n",
    "\n",
    "        You only plan and delegate tasks - you do not execute them yourself.\n",
    "\n",
    "        When assigning tasks, use this format:\n",
    "        1. <agent> : <task>\n",
    "\n",
    "        After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    web_search_agent = AssistantAgent(\n",
    "        \"WebSearchAgent\",\n",
    "        description=\"A web search agent.\",\n",
    "        tools=[search_web_tool],\n",
    "        model_client=model_client,\n",
    "        system_message=\"\"\"\n",
    "        You are a web search agent.\n",
    "        Your only tool is search_tool - use it to find information.\n",
    "        You make only one search call at a time.\n",
    "        Once you have the results, you never do calculations based on them.\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    data_analyst_agent = AssistantAgent(\n",
    "        \"DataAnalystAgent\",\n",
    "        description=\"A data analyst agent. Useful for performing calculations.\",\n",
    "        model_client=model_client,\n",
    "        tools=[percentage_change_tool],\n",
    "        system_message=\"\"\"\n",
    "        You are a data analyst.\n",
    "        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    # The termination condition is a combination of text mention termination and max message termination.\n",
    "    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
    "    max_messages_termination = MaxMessageTermination(max_messages=25)\n",
    "    termination = text_mention_termination | max_messages_termination\n",
    "\n",
    "    # The selector function is a function that takes the current message thread of the group chat\n",
    "    # and returns the next speaker's name. If None is returned, the LLM-based selection method will be used.\n",
    "    def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n",
    "        if messages[-1].source != planning_agent.name:\n",
    "            return planning_agent.name # Always return to the planning agent after the other agents have spoken.\n",
    "        return None\n",
    "\n",
    "    team = SelectorGroupChat(\n",
    "        [planning_agent, web_search_agent, data_analyst_agent],\n",
    "        model_client=OpenAIChatCompletionClient(model=\"gpt-4o-mini\"), # Use a smaller model for the selector.\n",
    "        termination_condition=termination,\n",
    "        selector_func=selector_func,\n",
    "    )\n",
    "    return team\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        seed=42,\n",
    "        temperature=0,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    team = create_team(model_client)\n",
    "    \n",
    "    task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n",
    "    \n",
    "    await Console(team.run_stream(task=task))\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb953f3c-cbe5-45e1-955f-26a698b60285",
   "metadata": {},
   "source": [
    "## 16.Nested Chat\n",
    "\n",
    "In `v0.4`, nested chat is an implementation detail of a custom agent. You can create a custom agent that takes a team or another agent as a parameter and implements the on_messages method to trigger the nested team or agent. It is up to the application to decide how to pass or transform the messages from and to the nested team or agent.\n",
    "\n",
    "The following example shows a simple nested chat that counts numbers.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from typing import Sequence\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.agents import BaseChatAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.messages import TextMessage, BaseChatMessage\n",
    "from autogen_agentchat.base import Response\n",
    "\n",
    "class CountingAgent(BaseChatAgent):\n",
    "    \"\"\"An agent that returns a new number by adding 1 to the last number in the input messages.\"\"\"\n",
    "    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n",
    "        if len(messages) == 0:\n",
    "            last_number = 0 # Start from 0 if no messages are given.\n",
    "        else:\n",
    "            assert isinstance(messages[-1], TextMessage)\n",
    "            last_number = int(messages[-1].content) # Otherwise, start from the last number.\n",
    "        return Response(chat_message=TextMessage(content=str(last_number + 1), source=self.name))\n",
    "\n",
    "    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n",
    "        return (TextMessage,)\n",
    "\n",
    "class NestedCountingAgent(BaseChatAgent):\n",
    "    \"\"\"An agent that increments the last number in the input messages\n",
    "    multiple times using a nested counting team.\"\"\"\n",
    "    def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None:\n",
    "        super().__init__(name, description=\"An agent that counts numbers.\")\n",
    "        self._counting_team = counting_team\n",
    "\n",
    "    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n",
    "        # Run the inner team with the given messages and returns the last message produced by the team.\n",
    "        result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token)\n",
    "        # To stream the inner messages, implement `on_messages_stream` and use that to implement `on_messages`.\n",
    "        assert isinstance(result.messages[-1], TextMessage)\n",
    "        return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1])\n",
    "\n",
    "    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n",
    "        # Reset the inner team.\n",
    "        await self._counting_team.reset()\n",
    "\n",
    "    @property\n",
    "    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n",
    "        return (TextMessage,)\n",
    "\n",
    "async def main() -> None:\n",
    "    # Create a team of two counting agents as the inner team.\n",
    "    counting_agent_1 = CountingAgent(\"counting_agent_1\", description=\"An agent that counts numbers.\")\n",
    "    counting_agent_2 = CountingAgent(\"counting_agent_2\", description=\"An agent that counts numbers.\")\n",
    "    counting_team = RoundRobinGroupChat([counting_agent_1, counting_agent_2], max_turns=5)\n",
    "    # Create a nested counting agent that takes the inner team as a parameter.\n",
    "    nested_counting_agent = NestedCountingAgent(\"nested_counting_agent\", counting_team)\n",
    "    # Run the nested counting agent with a message starting from 1.\n",
    "    response = await nested_counting_agent.on_messages([TextMessage(content=\"1\", source=\"user\")], CancellationToken())\n",
    "    assert response.inner_messages is not None\n",
    "    for message in response.inner_messages:\n",
    "        print(message)\n",
    "    print(response.chat_message)\n",
    "\n",
    "asyncio.run(main())\n",
    "```\n",
    "\n",
    "You can take a look at [SocietyOfMindAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.SocietyOfMindAgent) for a more complex implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf5b104-630e-4bb5-8b9d-ba258dfd1917",
   "metadata": {},
   "source": [
    "## 17.Sequential Chat\n",
    "\n",
    "In `v0.2`, sequential chat is supported by using the `initiate_chats` function. It takes input a list of dictionary configurations for each step of the sequence. See [Sequential Chat in v0.2](https://microsoft.github.io/autogen/0.2/docs/tutorial/conversation-patterns#sequential-chats) for more details.\n",
    "\n",
    "Base on the feedback from the community, the `initiate_chats` function is too opinionated and not flexible enough to support the diverse set of scenarios that users want to implement. We often find users struggling to get the `initiate_chats` function to work when they can easily glue the steps together usign basic Python code. Therefore, in `v0.4`, we do not provide a built-in function for sequential chat in the AgentChat API.\n",
    "\n",
    "Instead, you can create an event-driven sequential workflow using the Core API, and use the other components provided the AgentChat API to implement each step of the workflow. See an example of sequential workflow in the [Core API Tutorial](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/sequential-workflow.html).\n",
    "\n",
    "We recognize that the concept of workflow is at the heart of many applications, and we will provide more built-in support for workflows in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1187c22-0d3a-4257-8e87-b621fcfed22e",
   "metadata": {},
   "source": [
    "## 18.GPTAssistantAgent\n",
    "\n",
    "In `v0.2`, `GPTAssistantAgent` is a special agent class that is backed by the OpenAI Assistant API.\n",
    "\n",
    "In `v0.4`, the equivalent is the [OpenAIAssistantAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html#autogen_ext.agents.openai.OpenAIAssistantAgent) class. It supports the same set of features as the `GPTAssistantAgent` in `v0.2` with more such as customizable threads and file uploads. See [OpenAIAssistantAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html#autogen_ext.agents.openai.OpenAIAssistantAgent) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea0111-cf98-4258-a7cd-b6dd6fce23bb",
   "metadata": {},
   "source": [
    "## 19.Long Context Handling\n",
    "\n",
    "In `v0.4`, we introduce the [ChatCompletionContext](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html#autogen_core.model_context.ChatCompletionContext) base class that manages message history and provides a virtual view of the history. Applications can use built-in implementations such as [BufferedChatCompletionContext](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html#autogen_core.model_context.BufferedChatCompletionContext) to limit the message history sent to the model, or provide their own implementations that creates different virtual views.\n",
    "\n",
    "To use [BufferedChatCompletionContext](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html#autogen_core.model_context.BufferedChatCompletionContext) in an [AssistantAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) in a chatbot scenario.\n",
    "\n",
    "In this example, the chatbot can only read the last 10 messages in the history.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_core.model_context import BufferedChatCompletionContext\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "async def main() -> None:\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n",
    "\n",
    "    assistant = AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        system_message=\"You are a helpful assistant.\",\n",
    "        model_client=model_client,\n",
    "        model_context=BufferedChatCompletionContext(buffer_size=10), # Model can only view the last 10 messages.\n",
    "    )\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input == \"exit\":\n",
    "            break\n",
    "        response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken())\n",
    "        print(\"Assistant:\", response.chat_message.to_text())\n",
    "    \n",
    "    await model_client.close()\n",
    "\n",
    "asyncio.run(main())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e24fe-11b3-4c1f-989e-22bdc8b53b81",
   "metadata": {},
   "source": [
    "## 20.Observability and Control\n",
    "\n",
    "In `v0.4` AgentChat, you can observe the agents by using the `on_messages_stream` method which returns an async generator to stream the inner thoughts and actions of the agent. For teams, you can use the `run_stream` method to stream the inner conversation among the agents in the team. Your application can use these streams to observe the agents and teams in real-time.\n",
    "\n",
    "Both the `on_messages_stream` and `run_stream` methods takes a [CancellationToken](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html#autogen_core.CancellationToken) as a parameter which can be used to cancel the output stream asynchronously and stop the agent or team. For teams, you can also use termination conditions to stop the team when a certain condition is met. See [Termination Condition Tutorial](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html) for more details.\n",
    "\n",
    "Unlike the v0.2 which comes with a special logging module, the v0.4 API simply uses Python’s logging module to log events such as model client calls. See [Logging](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/logging.html) in the Core API documentation for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd8cb7-adfb-47bb-a119-f14e3f0b2e2c",
   "metadata": {},
   "source": [
    "## 21.Code Executors\n",
    "\n",
    "The code executors in `v0.2` and `v0.4` are nearly identical except the `v0.4` executors support async API. You can also use [CancellationToken](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html#autogen_core.CancellationToken) to cancel a code execution if it takes too long. See [Command Line Code Executors Tutorial](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/command-line-code-executors.html) in the Core API documentation.\n",
    "\n",
    "We also added `ACADynamicSessionsCodeExecutor` that can use Azure Container Apps (ACA) dynamic sessions for code execution. See [ACA Dynamic Sessions Code Executor Docs](https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/azure-container-code-executor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee6122b-de8e-4146-8b53-8d9de28febfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
