{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ed82ad-916b-4fa8-8b68-2e7d25724e4b",
   "metadata": {},
   "source": [
    "## Asking clarifying questions\n",
    "\n",
    "**Prompt**: *Say i had milions of URLs that needed to count the words in. Does counter scale better than diction for that*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae9f12-b434-49a4-bb7d-3ef61e500b85",
   "metadata": {},
   "source": [
    "Yes, Python's `collections.Counter` is essentially a specialized dictionary designed for counting hashable objects. It has some advantages in terms of convenience and clarity but doesn't inherently scale better than a regular dictionary (`dict`) for massive workloads like counting words from millions of URLs. The scalability is more dependent on your algorithm and the hardware resources available (e.g., memory and CPU) than on whether you use `Counter` or `dict`.\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "#### **1. Counter vs. Dict for Word Counting**\n",
    "- **Performance:** `Counter` is built on `dict` and uses the same underlying hashing mechanism. Its performance is practically identical to using a `dict` where you manually update counts.  \n",
    "- **Convenience:** `Counter` provides handy methods like `.most_common()` and automatically initializes counts to zero, which simplifies the code. \n",
    "\n",
    "For scaling to millions of URLs, the choice between `Counter` and `dict` is negligible in terms of raw performance.\n",
    "\n",
    "#### **2. Scalability Bottlenecks**\n",
    "Counting words from millions of URLs has bottlenecks beyond the choice of data structure:\n",
    "- **I/O Overhead**: Downloading text from millions of URLs can be slow. Optimize using concurrency or parallelism (e.g., `asyncio` or threading).\n",
    "- **Memory Usage**: Keeping word counts in memory can become a problem. Consider streaming processing or external storage.\n",
    "- **Processing Speed**: Parsing and counting words might require optimization (e.g., batching or distributed computing).\n",
    "\n",
    "### Optimized Solution for Millions of URLs\n",
    "\n",
    "1. **Streamed Processing with Batching**\n",
    "   Instead of keeping everything in memory, process the URLs in batches and update a global count.\n",
    "\n",
    "2. **Distributed Counting with MapReduce**\n",
    "   Use frameworks like Apache Hadoop, Spark, or even Pythonâ€™s multiprocessing to distribute the counting process.\n",
    "\n",
    "3. **Using a Database**\n",
    "   For extreme cases, store intermediate counts in an external system like Redis, PostgreSQL, or SQLite.\n",
    "\n",
    "### Optimized Python Example: Batched Processing\n",
    "\n",
    "```python\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "import re\n",
    "import concurrent.futures\n",
    "\n",
    "def download_and_count(url):\n",
    "    try:\n",
    "        # Download text from URL\n",
    "        response = urllib.request.urlopen(url)\n",
    "        text = response.read().decode('utf-8')\n",
    "        \n",
    "        # Normalize and count words\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return Counter(words)\n",
    "    except Exception:\n",
    "        return Counter()\n",
    "\n",
    "def merge_counters(counters):\n",
    "    # Merge a list of Counters into a single Counter\n",
    "    total_counter = Counter()\n",
    "    for counter in counters:\n",
    "        total_counter.update(counter)\n",
    "    return total_counter\n",
    "\n",
    "def main():\n",
    "    urls = [\n",
    "        # Add your list of millions of URLs here\n",
    "        \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
    "        \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "    ]\n",
    "    \n",
    "    batch_size = 10\n",
    "    global_counter = Counter()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for i in range(0, len(urls), batch_size):\n",
    "            # Process URLs in batches\n",
    "            batch = urls[i:i + batch_size]\n",
    "            results = list(executor.map(download_and_count, batch))\n",
    "            global_counter.update(merge_counters(results))\n",
    "    \n",
    "    # Print the top 10 most common words\n",
    "    print(\"Top 10 words:\")\n",
    "    for word, count in global_counter.most_common(10):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "### Key Features of the Solution\n",
    "- **Concurrency:** `ThreadPoolExecutor` processes multiple URLs simultaneously.\n",
    "- **Batching:** Reduces memory usage by merging counts periodically.\n",
    "- **Counter for Merging:** Simplifies summing counts from batches.\n",
    "\n",
    "This approach is more scalable and practical for large-scale word counting. The choice of `Counter` vs. `dict` remains secondary to how the entire process is structured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e64ad2-cc9b-4663-a162-c1feaef8c0cc",
   "metadata": {},
   "source": [
    "## Counting words in text\n",
    "\n",
    "**Prompt**: *Please write a plain Python code using the base language to download text from a given URL, and then count every instance of every word in that text*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0933c28-ddcd-4034-a45d-875febd630dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def download_text(url):\n",
    "    \"\"\"\n",
    "    Downloads text from the given URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url)\n",
    "        text = response.read().decode('utf-8')\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"\n",
    "    Counts the occurrences of each word in the given text.\n",
    "    \"\"\"\n",
    "    # Normalize the text: convert to lowercase and remove non-alphanumeric characters\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518e4c2f-b5ba-48dd-af60-e0661bfe0452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading text from: https://www.gutenberg.org/files/1342/1342-0.txt\n",
      "Top 10 most common words:\n",
      "the: 4658\n",
      "to: 4323\n",
      "of: 3842\n",
      "and: 3763\n",
      "her: 2260\n",
      "i: 2098\n",
      "a: 2036\n",
      "in: 1991\n",
      "was: 1871\n",
      "she: 1732\n"
     ]
    }
   ],
   "source": [
    "# Example URL\n",
    "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"  # Text of Pride and Prejudice\n",
    "print(f\"Downloading text from: {url}\")\n",
    "\n",
    "# Download the text\n",
    "text = download_text(url)\n",
    "\n",
    "if text:\n",
    "\n",
    "    # Count words\n",
    "    word_counts = count_words(text)\n",
    "    \n",
    "    # Display the 10 most common words\n",
    "    print(\"Top 10 most common words:\")\n",
    "    for word, count in word_counts.most_common(10):\n",
    "        print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c8854-4762-434f-9270-4ae53b82f862",
   "metadata": {},
   "source": [
    "## HashTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f865a32-6dea-43b1-a462-5620af17fa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Alice\n",
      "Age: 30\n",
      "City: New York\n",
      "Key 'name' exists in the hash table.\n",
      "Updated Age: 31\n",
      "After deletion: {'name': 'Alice', 'age': 31}\n"
     ]
    }
   ],
   "source": [
    "# Create a hash table (dictionary)\n",
    "hash_table = {}\n",
    "\n",
    "# Adding key-value pairs\n",
    "hash_table['name'] = 'Alice'\n",
    "hash_table['age'] = 30\n",
    "hash_table['city'] = 'New York'\n",
    "\n",
    "# Accessing values by keys\n",
    "print(\"Name:\", hash_table['name'])  # Output: Name: Alice\n",
    "print(\"Age:\", hash_table['age'])    # Output: Age: 30\n",
    "print(\"City:\", hash_table['city'])  # Output: City: New York\n",
    "\n",
    "# Checking if a key exists\n",
    "if 'name' in hash_table:\n",
    "    print(\"Key 'name' exists in the hash table.\")\n",
    "\n",
    "# Updating a value\n",
    "hash_table['age'] = 31\n",
    "print(\"Updated Age:\", hash_table['age'])  # Output: Updated Age: 31\n",
    "\n",
    "# Removing a key-value pair\n",
    "del hash_table['city']\n",
    "print(\"After deletion:\", hash_table)  # Output: After deletion: {'name': 'Alice', 'age': 31}\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc430d9-3fef-433f-9b41-6c9697072410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
