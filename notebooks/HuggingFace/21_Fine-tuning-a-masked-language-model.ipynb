{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6d30f1-9f39-431f-b80d-8318f0eb072e",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<ol>\n",
    "    <li>\n",
    "        <a href=\"#Introduction\">Introduction</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Picking-a-pretrained-model-for-masked-language-modeling\">Picking a pretrained model for masked language modeling</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#The-dataset\">The dataset</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Preprocessing-the-data\">Preprocessing the data</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Fine-tuning DistilBERT-with-the-Trainer-API\">Fine-tuning DistilBERT with the Trainer API</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Using-our-fine-tuned-model\">Fine-tuning the model</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Fine-tuning-DistilBERT-with-ðŸ¤—-Accelerate\">Fine-tuning DistilBERT with ðŸ¤— Accelerate</a>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6dd418-ab87-4377-9471-a4fe361aca36",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91361f-dac4-4487-8d1d-3ec3744dd19f",
   "metadata": {},
   "source": [
    "For many NLP applications involving Transformer models, you can simply take a pretrained model from the Hugging Face Hub and fine-tune it directly on your data for the task at hand. Provided that the corpus used for pretraining is not too different from the corpus used for fine-tuning, transfer learning will usually produce good results.\n",
    "\n",
    "However, there are a few cases where youâ€™ll want to first fine-tune the language models on your data, before training a task-specific head. For example, if your dataset contains legal contracts or scientific articles, a vanilla Transformer model like BERT will typically treat the domain-specific words in your corpus as rare tokens, and the resulting performance may be less than satisfactory. By fine-tuning the language model on in-domain data you can boost the performance of many downstream tasks, which means you usually only have to do this step once!\n",
    "\n",
    "This process of fine-tuning a pretrained language model on in-domain data is usually called *domain adaptation*. It was popularized in 2018 by [ULMFiT](https://arxiv.org/abs/1801.06146), which was one of the first neural architectures (based on LSTMs) to make transfer learning really work for NLP. An example of domain adaptation with ULMFiT is shown in the image below; in this section weâ€™ll do something similar, but with a Transformer instead of an LSTM!\n",
    "\n",
    "\n",
    "![img](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit-dark.svg)\n",
    "\n",
    "By the end of this section youâ€™ll have a [masked language model](https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D.) on the Hub that can autocomplete sentences as shown below:\n",
    "\n",
    "Letâ€™s dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03965f8-fe8e-46ea-b180-d813cea447f7",
   "metadata": {},
   "source": [
    "## Picking a pretrained model for masked language modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b2edd0-a180-4977-ad95-b8e335cbf579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
