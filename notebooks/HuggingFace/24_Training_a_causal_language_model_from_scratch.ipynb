{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0992bdac-bab0-44eb-af74-1b9103f31254",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<ol>\n",
    "    <li>\n",
    "        <a href=\"#Introduction\">Introduction</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Gathering-the-data\">Gathering-the-data</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Preparing-the-dataset\">Preparing the dataset</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Code-generation-with-a-pipeline\">Code generation with a pipeline</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Training-with-ðŸ¤—-Accelerate\">Training with ðŸ¤— Accelerate</a>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4507fb-c1ac-445d-9b3f-6ced201d8267",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Up until now, weâ€™ve mostly been using pretrained models and fine-tuning them for new use cases by reusing the weights from pretraining. As we saw in [Chapter 1](https://huggingface.co/course/chapter1), this is commonly referred to as transfer learning, and itâ€™s a very successful strategy for applying Transformer models to most real-world use cases where labeled data is sparse. In this chapter, weâ€™ll take a different approach and train a completely new model from scratch. This is a good approach to take if you have a lot of data and it is very different from the pretraining data used for the available models. However, it also requires considerably more compute resources to pretrain a language model than just to fine-tune an existing one. Examples where it can make sense to train a new model include for datasets consisting of musical notes, molecular sequences such as DNA, or programming languages. The latter have recently gained traction thanks to tools such as TabNine and GitHubâ€™s Copilot, powered by OpenAIâ€™s Codex model, that can generate long sequences of code. This task of text generation is best addressed with auto-regressive or causal language models such as GPT-2.\n",
    "\n",
    "In this section we will build a scaled-down version of a code generation model: weâ€™ll focus on one-line completions instead of full functions or classes, using a subset of Python code. When working with data in Python you are in frequent contact with the Python data science stack, consisting of the matplotlib, seaborn, pandas, and scikit-learn libraries. When using those frameworks itâ€™s common to need to look up specific commands, so it would be nice if we could use a model to complete these calls for us.\n",
    "\n",
    "In [Chapter 6](https://huggingface.co/course/chapter6) we created an efficient tokenizer to process Python source code, but what we still need is a large-scale dataset to pretrain a model on. Here, weâ€™ll apply our tokenizer to a corpus of Python code derived from GitHub repositories. We will then use the `Trainer API` and ðŸ¤— Accelerate to train the model. Letâ€™s get to it!\n",
    "\n",
    "This is actually showcasing the model that was trained and uploaded to the Hub using the code shown in this section. You can find it here. Note that since there is some randomization happening in the text generation, you will probably get a slightly different result.\n",
    "\n",
    "This is actually showcasing the model that was trained and uploaded to the Hub using the code shown in this section. You can find it [here](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28). Note that since there is some randomization happening in the text generation, you will probably get a slightly different result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583c6f1-741e-4f3c-be1a-d882b241476b",
   "metadata": {},
   "source": [
    "##  Gathering the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc43803-6dfb-4704-893b-2ef3ffb6bd1c",
   "metadata": {},
   "source": [
    "Python code is abundantly available from code repositories such as GitHub, which we can use to create a dataset by scraping for every Python repository. This was the approach taken in the [Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) to pretrain a large GPT-2 model. Using a GitHub dump of about 180 GB containing roughly 20 million Python files called `codeparrot`, the authors built a dataset that they then shared on the [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot).\n",
    "\n",
    "However, training on the full corpus is time- and compute-consuming, and we only need the subset of the dataset concerned with the Python data science stack. So, letâ€™s start by filtering the `codeparrot` dataset for all files that include any of the libraries in this stack. Because of the datasetâ€™s size, we want to avoid downloading it; instead, weâ€™ll use the streaming feature to filter it on the fly. To help us filter the code samples using the libraries we mentioned earlier, weâ€™ll use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fda17da-c063-4461-9c79-e8b9783046a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4915ac3-8610-4738-96be-d232048c5c2f",
   "metadata": {},
   "source": [
    "Letâ€™s test it on two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dc3ad0-b3b7-4439-962d-5b58530c0c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\"\n",
    "\n",
    "print(\n",
    "    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870666df-5e7a-4896-8887-1024d015cc5a",
   "metadata": {},
   "source": [
    "We can use this to create a function that will stream the dataset and filter the elements we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815f356c-17d5-45f5-ac27-a09f23e4e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filtered_dict = defaultdict(list)\n",
    "    total = 0\n",
    "    for sample in tqdm(iter(dataset)):\n",
    "        total += 1\n",
    "        if any_keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v)\n",
    "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
    "    return Dataset.from_dict(filtered_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53152640-5e09-41e4-a56e-0d3344a556b3",
   "metadata": {},
   "source": [
    "Then we can simply apply this function to the streaming dataset:\n",
    "\n",
    "```python\n",
    "# This cell will take a very long time to execute, so you should skip it and go to\n",
    "# the next one!\n",
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"  # \"valid\"\n",
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "\n",
    "data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
    "filtered_data = filter_streaming_dataset(data, filters)\n",
    "```\n",
    "\n",
    "This leaves us with about 3% of the original dataset, which is still quite sizable â€” the resulting dataset is 6 GB and consists of 600,000 Python scripts!\n",
    "\n",
    "Filtering the full dataset can take 2-3h depending on your machine and bandwidth. If you donâ€™t want to go through this lengthy process yourself, we provide the filtered dataset on the Hub for you to download:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c4f960-b7c2-4732-968c-632cce2cb446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train.shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid.shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd7bb8-bb75-4fcc-92fa-11c0b38d9d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
