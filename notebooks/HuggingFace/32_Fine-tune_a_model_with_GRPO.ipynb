{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1497928-2630-4024-abc2-759e6bcc9dc0",
   "metadata": {},
   "source": [
    "Now that youâ€™ve seen the theory, letâ€™s put it into practice! In this exercise, youâ€™ll fine-tune a model with GRPO.\n",
    "\n",
    "###  Install dependencies\n",
    "\n",
    "First, letâ€™s install the dependencies for this exercise.\n",
    "\n",
    "```\n",
    "!pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0 accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off\n",
    "\n",
    "\n",
    "!nvcc --version\n",
    "!sudo apt install nvidia-cuda-toolkit\n",
    "! git clone https://github.com/Dao-AILab/flash-attention.git cd flash-attention pip install .\n",
    "!pip install -qqq flash-attn --no-build-isolation --progress-bar off\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bbf714-f060-4728-b17d-f73b5a8d2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59ee87-c88b-433a-98c1-66a510b9c0c0",
   "metadata": {},
   "source": [
    "###  Import and log in to Weights & Biases\n",
    "\n",
    "Weights & Biases is a tool for logging and monitoring your experiments. Weâ€™ll use it to log our fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea6abe-adc2-4008-9c5c-dd10d6147f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a7b00-0ef3-4ab5-a744-869e0678199b",
   "metadata": {},
   "source": [
    "###  Load the dataset\n",
    "\n",
    "Now, letâ€™s load the dataset. In this case, weâ€™ll use the [mlabonne/smoltldr](https://huggingface.co/datasets/mlabonne/smoltldr) dataset, which contains a list of short stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04622cc3-839a-4148-80c6-51fd29640d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mlabonne/smoltldr\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135effc-1adb-4d8c-9285-f97aeed68dbc",
   "metadata": {},
   "source": [
    "###  Load model\n",
    "\n",
    "Now, letâ€™s load the model.\n",
    "\n",
    "For this exercise, weâ€™ll use the [SmolLM2-135M](hhttps://huggingface.co/HuggingFaceTB/SmolLM2-135M) model.\n",
    "\n",
    "This is a small 135M parameter model that runs on limited hardware. This makes the model ideal for learning, but itâ€™s not the most powerful model out there. If you have access to more powerful hardware, you can try to fine-tune a larger model like [SmolLM2-1.7B](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78c41a-bd77-46a3-8e8b-921ee55a084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e24b2-abc8-4711-a9cf-82e1f38c217f",
   "metadata": {},
   "source": [
    "###  Load LoRA\n",
    "\n",
    "Now, letâ€™s load the LoRA configuration. Weâ€™ll take advantage of LoRA to reduce the number of trainable parameters, and in turn the memory footprint we need to fine-tune the model.\n",
    "\n",
    "If youâ€™re not familiar with LoRA, you can read more about it in [Chapter 11](https://huggingface.co/learn/course/en/chapter11/3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418c449-8cca-4742-b0fb-03faeb2dac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae649ae4-2c63-42f3-8e14-1e805854361a",
   "metadata": {},
   "source": [
    "###  Define the reward function\n",
    "\n",
    "As mentioned in the previous section, GRPO can use any reward function to improve the model. In this case, weâ€™ll use a simple reward function that encourages the model to generate text that is 50 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c641eed5-f79e-42ac-b99f-8745f0dae9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function\n",
    "ideal_length = 50\n",
    "\n",
    "\n",
    "def reward_len(completions, **kwargs):\n",
    "    return [-abs(ideal_length - len(completion)) for completion in completions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147fd7c-0a39-433a-84c5-88840a2ca74f",
   "metadata": {},
   "source": [
    "###  Define the training arguments\n",
    "\n",
    "Now, letâ€™s define the training arguments. Weâ€™ll use the `GRPOConfig` class to define the training arguments in a typical `transformers` style.\n",
    "\n",
    "If this is the first time youâ€™re defining training arguments, you can check the [TrainingArguments](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainingarguments) class for more information, or [Chapter 2](https://huggingface.co/learn/course/en/chapter2/1) for a detailed introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466bbdcb-1665-4c55-be5d-db2a3759f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"GRPO\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_prompt_length=512,\n",
    "    max_completion_length=96,\n",
    "    num_generations=8,\n",
    "    optim=\"adamw_8bit\",\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    report_to=[\"wandb\"],\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2064a340-1a78-455c-b8fd-b1bd40ec14a6",
   "metadata": {},
   "source": [
    "Now, we can initialize the trainer with model, dataset, and training arguments and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5452a-7bf7-4fc0-a232-bda224ef3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[reward_len],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "wandb.init(project=\"GRPO\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924bac4d-2ed0-4fa1-83e0-be8ec917ef67",
   "metadata": {},
   "source": [
    "Training takes around 1 hour on a single A10G GPU which is available on Google Colab or via Hugging Face Spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc125f36-d8f1-4b59-bf83-c46dc73ed78d",
   "metadata": {},
   "source": [
    "###  Push the model to the Hub during training\n",
    "\n",
    "If we set the `push_to_hub` argument to `True` and the `model_id argument` to a valid model name, the model will be pushed to the Hugging Face Hub whilst weâ€™re training. This is useful if you want to start vibe testing the model straight away!\n",
    "\n",
    "\n",
    "####  Interpret training results\n",
    "\n",
    "`GRPOTrainer` logs the reward from your reward function, the loss, and a range of other metrics.\n",
    "\n",
    "We will focus on the reward from the reward function and the loss.\n",
    "\n",
    "As you can see, the reward from the reward function moves closer to 0 as the model learns. This is a good sign that the model is learning to generate text of the correct length.\n",
    "\n",
    "![plt1](https://huggingface.co/reasoning-course/images/resolve/main/grpo/13.png)\n",
    "\n",
    "You might notice that the loss starts at zero and then increases during training, which may seem counterintuitive. This behavior is expected in GRPO and is directly related to the mathematical formulation of the algorithm. The loss in GRPO is proportional to the KL divergence (the cap relative to original policy) . As training progresses, the model learns to generate text that better matches the reward function, causing it to diverge more from its initial policy. This increasing divergence is reflected in the rising loss value, which actually indicates that the model is successfully adapting to optimize for the reward function.\n",
    "\n",
    "![plt2](https://huggingface.co/reasoning-course/images/resolve/main/grpo/14.png)\n",
    "\n",
    "\n",
    "####  Save and publish the model\n",
    "\n",
    "Letâ€™s share the model with the community!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad21b73-0763-4ddc-8e6d-64f86cc3be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = trainer.model.merge_and_unload()\n",
    "merged_model.push_to_hub(\n",
    "    \"SmolGRPO-135M\", private=False, tags=[\"GRPO\", \"Reasoning-Course\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed23e8-e920-4fb9-8787-b963a5b868aa",
   "metadata": {},
   "source": [
    "#### Generate text\n",
    "\n",
    "ðŸŽ‰ Youâ€™ve successfully fine-tuned a model with GRPO! Now, letâ€™s generate some text with the model.\n",
    "\n",
    "First, weâ€™ll define a really long document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe6115-8835-43a5-9172-5267f2de0718",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "# A long document about the Cat\n",
    "\n",
    "The cat (Felis catus), also referred to as the domestic cat or house cat, is a small \n",
    "domesticated carnivorous mammal. It is the only domesticated species of the family Felidae.\n",
    "Advances in archaeology and genetics have shown that the domestication of the cat occurred\n",
    "in the Near East around 7500 BC. It is commonly kept as a pet and farm cat, but also ranges\n",
    "freely as a feral cat avoiding human contact. It is valued by humans for companionship and\n",
    "its ability to kill vermin. Its retractable claws are adapted to killing small prey species\n",
    "such as mice and rats. It has a strong, flexible body, quick reflexes, and sharp teeth,\n",
    "and its night vision and sense of smell are well developed. It is a social species,\n",
    "but a solitary hunter and a crepuscular predator. Cat communication includes\n",
    "vocalizationsâ€”including meowing, purring, trilling, hissing, growling, and gruntingâ€”as\n",
    "well as body language. It can hear sounds too faint or too high in frequency for human ears,\n",
    "such as those made by small mammals. It secretes and perceives pheromones.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a9a4a0-de3b-4554-8769-8a3d2a8fbb9b",
   "metadata": {},
   "source": [
    "Now, we can generate text with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe1f5d-7eaf-4d9e-8ac8-750a4cfb8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"SmolGRPO-135M\")\n",
    "\n",
    "## Or use the model and tokenizer we defined earlier\n",
    "# generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.5,\n",
    "    \"min_p\": 0.1,\n",
    "}\n",
    "\n",
    "generated_text = generator(messages, generate_kwargs=generate_kwargs)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed3ba0-f1a6-4c01-af9f-4c96cdb8d4c1",
   "metadata": {},
   "source": [
    "###  Conclusion\n",
    "\n",
    "In this chapter, weâ€™ve seen how to fine-tune a model with GRPO. Weâ€™ve also seen how to interpret the training results and generate text with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5efeb-0577-4e8c-8899-93017cee2e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
