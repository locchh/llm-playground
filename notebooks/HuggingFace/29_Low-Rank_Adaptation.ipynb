{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5440f9-76a6-4624-8c4d-b51d9466d99b",
   "metadata": {},
   "source": [
    "## LoRA (Low-Rank Adaptation)\n",
    "\n",
    "Fine-tuning large language models is a resource intensive process. LoRA is a technique that allows us to fine-tune large language models with a small number of parameters. It works by adding and optimizing smaller matrices to the attention weights, typically reducing trainable parameters by about 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8ffd4-2074-438a-bdc3-3d75fa66b591",
   "metadata": {},
   "source": [
    "### Understanding LoRA\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that freezes the pre-trained model weights and injects trainable rank decomposition matrices into the model’s layers. Instead of training all model parameters during fine-tuning, LoRA decomposes the weight updates into smaller matrices through low-rank decomposition, significantly reducing the number of trainable parameters while maintaining model performance. For example, when applied to GPT-3 175B, LoRA reduced trainable parameters by 10,000x and GPU memory requirements by 3x compared to full fine-tuning. You can read more about LoRA in the [LoRA paper](https://arxiv.org/pdf/2106.09685).\n",
    "\n",
    "LoRA works by adding pairs of rank decomposition matrices to transformer layers, typically focusing on attention weights. During inference, these adapter weights can be merged with the base model, resulting in no additional latency overhead. LoRA is particularly useful for adapting large language models to specific tasks or domains while keeping resource requirements manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4fa61d-5aae-45ea-976e-aef2a89133c7",
   "metadata": {},
   "source": [
    "### Key advantages of LoRA\n",
    "\n",
    "#### 1.Memory Efficiency:\n",
    "- Only adapter parameters are stored in GPU memory\n",
    "- Base model weights remain frozen and can be loaded in lower precision\n",
    "- Enables fine-tuning of large models on consumer GPUs\n",
    "\n",
    "#### 2.Training Features:\n",
    "- Native PEFT/LoRA integration with minimal setup\n",
    "- Support for QLoRA (Quantized LoRA) for even better memory efficiency\n",
    "\n",
    "#### 3.Adapter Management:\n",
    "- Adapter weight saving during checkpoints\n",
    "- Features to merge adapters back into base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae2066-68ec-4a9b-a29e-812eb7b2eb93",
   "metadata": {},
   "source": [
    "### Loading LoRA Adapters with PEFT\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft) is a library that provides a unified interface for loading and managing PEFT methods, including LoRA. It allows you to easily load and switch between different PEFT methods, making it easier to experiment with different fine-tuning techniques.\n",
    "\n",
    "![fig](https://github.com/huggingface/smol-course/raw/main/3_parameter_efficient_finetuning/images/lora_adapter.png)\n",
    "Adapters can be loaded onto a pretrained model with `load_adapter()`, which is useful for trying out different adapters whose weights aren’t merged. Set the active adapter weights with the `set_adapter()` function. To return the base model, you could use `unload()` to unload all of the LoRA modules. This makes it easy to switch between different task-specific weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01ef283-a9da-4975-9de0-91597b0f9aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla P40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "# Free GPU Memory Before Training\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "488eaa68-ff46-4d63-9a46-b306562416c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d14d58b89474c50aef0ae9c7a763293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a039d9a719a43c8824b3b9988bb606b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51e5966223442dd88b7b803efe5b5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a079ea0c124c14bbc55f2b21e76832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"ybelkada/opt-350m-lora\")\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "lora_model = PeftModel.from_pretrained(model, \"ybelkada/opt-350m-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457cc46-7d93-44e4-ad4d-20d9f596beff",
   "metadata": {},
   "source": [
    "### Fine-tune LLM using trl and the SFTTrainer with LoRA\n",
    "\n",
    "The [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` provides integration with LoRA adapters through the [PEFT](https://huggingface.co/docs/peft/en/index) library. This means that we can fine-tune a model in the same way as we did with SFT, but use LoRA to reduce the number of parameters we need to train.\n",
    "\n",
    "We’ll use the `LoRAConfig` class from PEFT in our example. The setup requires just a few configuration steps:\n",
    "\n",
    "1. Define the LoRA configuration (rank, alpha, dropout)\n",
    "2. Create the SFTTrainer with PEFT config\n",
    "3. Train and save the adapter weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f2507-d2b9-4b33-bc18-145b9e70f438",
   "metadata": {},
   "source": [
    "### LoRA Configuration\n",
    "\n",
    "Let’s walk through the LoRA configuration and key parameters.\n",
    "\n",
    "\n",
    "| Parameter        | Description |\n",
    "|-----------------|-------------|\n",
    "| **r (rank)**    | Dimension of the low-rank matrices used for weight updates. Typically between 4-32. Lower values provide more compression but potentially less expressiveness. |\n",
    "| **lora_alpha**  | Scaling factor for LoRA layers, usually set to 2x the rank value. Higher values result in stronger adaptation effects. |\n",
    "| **lora_dropout** | Dropout probability for LoRA layers, typically 0.05-0.1. Higher values help prevent overfitting during training. |\n",
    "| **bias**        | Controls training of bias terms. Options are “none”, “all”, or “lora_only”. “none” is most common for memory efficiency. |\n",
    "| **target_modules** | Specifies which model modules to apply LoRA to. Can be “all-linear” or specific modules like “q_proj,v_proj”. More modules enable greater adaptability but increase memory usage. |\n",
    "\n",
    "*When implementing PEFT methods, start with small rank values (4-8) for LoRA and monitor training loss. Use validation sets to prevent overfitting and compare results with full fine-tuning baselines when possible. The effectiveness of different methods can vary by task, so experimentation is key.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802e247-6e09-4de0-b25f-994f9db5461b",
   "metadata": {},
   "source": [
    "###  Using TRL with PEFT\n",
    "\n",
    "PEFT methods can be combined with TRL for fine-tuning to reduce memory requirements. We can pass the LoraConfig to the model when loading it.\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: Configure LoRA parameters\n",
    "# r: rank dimension for LoRA update matrices (smaller = more compression)\n",
    "rank_dimension = 6\n",
    "# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_alpha = 8\n",
    "# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # Rank dimension - typically between 4-32\n",
    "    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    target_modules=\"all-linear\",  # Which modules to apply LoRA to\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257ab40-8edc-40bf-bed7-bd3854e29892",
   "metadata": {},
   "source": [
    "Above, we used `device_map=\"auto\"` to automatically assign the model to the correct device. You can also manually assign the model to a specific device using `device_map={\"\": device_index}`.\n",
    "\n",
    "We will also need to define the `SFTTrainer` with the LoRA configuration.\n",
    "\n",
    "```python\n",
    "# Create SFTTrainer with LoRA configuration\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,  # LoRA configuration\n",
    "    max_seq_length=max_seq_length,  # Maximum sequence length\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "```\n",
    "\n",
    "✏️ Try it out! Build on your fine-tuned model from the previous section, but fine-tune it with LoRA. Use the `HuggingFaceTB/smoltalk` dataset to fine-tune a `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` model, using the LoRA configuration we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646636b-b3b1-4140-a76f-acc628fecd3a",
   "metadata": {},
   "source": [
    "###  Merging LoRA Adapters\n",
    "\n",
    "After training with LoRA, you might want to merge the adapter weights back into the base model for easier deployment. This creates a single model with the combined weights, eliminating the need to load adapters separately during inference.\n",
    "\n",
    "The merging process requires attention to memory management and precision. Since you’ll need to load both the base model and adapter weights simultaneously, ensure sufficient GPU/CPU memory is available. Using `device_map=\"auto\"` in `transformers` will find the correct device for the model based on your hardware.\n",
    "\n",
    "Maintain consistent precision (e.g., float16) throughout the process, matching the precision used during training and saving the merged model in the same format for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668c8f2-09f2-46fc-b8b7-47489ac23f11",
   "metadata": {},
   "source": [
    "###  Merging Implementation\n",
    "\n",
    "After training a LoRA adapter, you can merge the adapter weights back into the base model. Here’s how to do it:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"base_model_name\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. Load the PEFT model with adapter\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model, \"path/to/adapter\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 3. Merge adapter weights with base model\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "```\n",
    "\n",
    "If you encounter size discrepancies in the saved model, ensure you’re also saving the tokenizer:\n",
    "\n",
    "```python\n",
    "# Save both model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"base_model_name\")\n",
    "merged_model.save_pretrained(\"path/to/save/merged_model\")\n",
    "tokenizer.save_pretrained(\"path/to/save/merged_model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625c8cf-6aaf-4eaa-8ede-6b9d3c679469",
   "metadata": {},
   "source": [
    "✏️ Try it out! Merge the adapter weights back into the base model. Use the `HuggingFaceTB/smoltalk` dataset to fine-tune a `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B model`, using the LoRA configuration we defined above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
