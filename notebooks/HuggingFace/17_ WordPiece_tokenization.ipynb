{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb2a15d-4686-4963-9102-3ffdc5652656",
   "metadata": {},
   "source": [
    "###  WordPiece tokenization\n",
    "\n",
    "WordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It’s very similar to BPE in terms of the training, but the actual tokenization is done differently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79075c92-124f-41d7-b08f-d5eb9a9f8c7a",
   "metadata": {},
   "source": [
    "###  Training algorithm\n",
    "\n",
    "⚠️ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate.\n",
    "\n",
    "Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like ## for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, \"word\" gets split like this:\n",
    "\n",
    "`w ##o ##r ##d`\n",
    "\n",
    "Thus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.\n",
    "\n",
    "Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula: $score=(freq_of_pair)/(freq_of_first_element×freq_of_second_element)$\n",
    "\n",
    "By dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary. For instance, it won’t necessarily merge `(\"un\", \"##able\")` even if that pair occurs very frequently in the vocabulary, because the two pairs `\"un\"` and `\"##able\"` will likely each appear in a lot of other words and have a high frequency. In contrast, a pair like (`\"hu\"`, `\"##gging\"`) will probably be merged faster (assuming the word `“hugging”` appears often in the vocabulary) since `\"hu\"` and `\"##gging\"` are likely to be less frequent individually.\n",
    "\n",
    "Let’s look at the same vocabulary we used in the BPE training example:\n",
    "\n",
    "`(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)`\n",
    "\n",
    "The splits here will be:\n",
    "\n",
    "`(\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##g\" \"##s\", 5)`\n",
    "\n",
    "so the initial vocabulary will be `[\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\"]` (if we forget about special tokens for now). The most frequent pair is `(\"##u\", \"##g\")` (present 20 times), but the individual frequency of `\"##u\"` is very high, so its score is not the highest (it’s 1 / 36). All pairs with a `\"##u\"` actually have that same score (1 / 36), so the best score goes to the pair `(\"##g\", \"##s\")` — the only one without a `\"##u\"` — at 1 / 20, and the first merge learned is `(\"##g\", \"##s\") -> (\"##gs\")`.\n",
    "\n",
    "Note that when we merge, we remove the `##` between the two tokens, so we add `\"##gs\"` to the vocabulary and apply the merge in the words of the corpus:\n",
    "\n",
    "```\n",
    "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\"]\n",
    "Corpus: (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##gs\", 5)\n",
    "```\n",
    "\n",
    "At this point, `\"##u\"` is in all the possible pairs, so they all end up with the same score. Let’s say that in this case, the first pair is merged, so `(\"h\", \"##u\") -> \"hu\"`. This takes us to:\n",
    "\n",
    "```\n",
    "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\"]\n",
    "Corpus: (\"hu\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5)\n",
    "```\n",
    "\n",
    "Then the next best score is shared by `(\"hu\", \"##g\")` and `(\"hu\", \"##gs\")` (with 1/15, compared to 1/21 for all the other pairs), so the first pair with the biggest score is merged:\n",
    "```\n",
    "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\", \"hug\"]\n",
    "Corpus: (\"hug\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5)\n",
    "```\n",
    "\n",
    "and we continue like this until we reach the desired vocabulary size.\n",
    "\n",
    "✏️ Now your turn! What will the next merge rule be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ae67c-6502-4dce-9796-271cf3275770",
   "metadata": {},
   "source": [
    "###  Tokenization algorithm\n",
    "\n",
    "Tokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned. Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it. For instance, if we use the vocabulary learned in the example above, for the word `\"hugs\"` the longest subword starting from the beginning that is inside the vocabulary is `\"hug\"`, so we split there and get `[\"hug\", \"##s\"]`. We then continue with `\"##s\"`, which is in the vocabulary, so the tokenization of `\"hugs\"` is `[\"hug\", \"##s\"]`.\n",
    "\n",
    "With BPE, we would have applied the merges learned in order and tokenized this as `[\"hu\", \"##gs\"]`, so the encoding is different.\n",
    "\n",
    "As another example, let’s see how the word `\"bugs\"` would be tokenized. `\"b\"` is the longest subword starting at the beginning of the word that is in the vocabulary, so we split there and get `[\"b\", \"##ugs\"]`. Then `\"##u\"` is the longest subword starting at the beginning of `\"##ugs\"` that is in the vocabulary, so we split there and get `[\"b\", \"##u, \"##gs\"]`. Finally, `\"##gs\"` is in the vocabulary, so this last list is the tokenization of `\"bugs\"`.\n",
    "\n",
    "When the tokenization gets to a stage where it’s not possible to find a subword in the vocabulary, the whole word is tokenized as unknown — so, for instance, `\"mug\"` would be tokenized as `[\"[UNK]\"]`, as would `\"bum\"` (even if we can begin with `\"b\"` and `\"##u\"`, `\"##m\"` is not the vocabulary, and the resulting tokenization will just be `[\"[UNK]\"]`, not `[\"b\", \"##u\", \"[UNK]\"]`). This is another difference from BPE, which would only classify the individual characters not in the vocabulary as unknown.\n",
    "\n",
    "✏️ Now your turn! How will the word `\"pugs\"` be tokenized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58019a44-9842-4f4e-9b7b-53e9b4e53b8a",
   "metadata": {},
   "source": [
    "###  Implementing WordPiece\n",
    "\n",
    "Now let’s take a look at an implementation of the WordPiece algorithm. Like with BPE, this is just pedagogical, and you won’t able to use this on a big corpus.\n",
    "\n",
    "We will use the same corpus as in the BPE example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2ec97e-347d-4366-b533-b24d532a3d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b76eae-9377-4f14-ade7-f806e446d2f4",
   "metadata": {},
   "source": [
    "First, we need to pre-tokenize the corpus into words. Since we are replicating a WordPiece tokenizer (like BERT), we will use the `bert-base-cased tokenizer` for the pre-tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7fc8195-e5f3-45bc-9e5b-1ce28f3befb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c19110-8440-419f-b22d-ec1b9d5104ff",
   "metadata": {},
   "source": [
    "Then we compute the frequencies of each word in the corpus as we do the pre-tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "273641aa-ef53-45af-aeb3-5a76ba2de2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'This': 3,\n",
       "             'is': 2,\n",
       "             'the': 1,\n",
       "             'Hugging': 1,\n",
       "             'Face': 1,\n",
       "             'Course': 1,\n",
       "             '.': 4,\n",
       "             'chapter': 1,\n",
       "             'about': 1,\n",
       "             'tokenization': 1,\n",
       "             'section': 1,\n",
       "             'shows': 1,\n",
       "             'several': 1,\n",
       "             'tokenizer': 1,\n",
       "             'algorithms': 1,\n",
       "             'Hopefully': 1,\n",
       "             ',': 1,\n",
       "             'you': 1,\n",
       "             'will': 1,\n",
       "             'be': 1,\n",
       "             'able': 1,\n",
       "             'to': 1,\n",
       "             'understand': 1,\n",
       "             'how': 1,\n",
       "             'they': 1,\n",
       "             'are': 1,\n",
       "             'trained': 1,\n",
       "             'and': 1,\n",
       "             'generate': 1,\n",
       "             'tokens': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23f5c1-e812-4f31-9342-eea6780b610a",
   "metadata": {},
   "source": [
    "As we saw before, the alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by `##`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9381f63e-86aa-42f5-a9f1-b7cdb2066923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16f1d75-261c-4eca-af35-6d79cc59775f",
   "metadata": {},
   "source": [
    "We also add the special tokens used by the model at the beginning of that vocabulary. In the case of BERT, it’s the list `[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad65bf1-1d7a-47f0-a2c6-bc9da9c4b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab7412f-2ca4-450c-a679-00cf7894e7ab",
   "metadata": {},
   "source": [
    "Next we need to split each word, with all the letters that are not the first prefixed by `##`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a7675e-29bb-4bb9-a277-ca730fc9ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf199e2-9c3c-4633-b1ce-0b4ee47ca03d",
   "metadata": {},
   "source": [
    "Now that we are ready for training, let’s write a function that computes the score of each pair. We’ll need to use this at each step of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0387e156-158d-406b-a4f8-b4ec22548b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    # Initialize dictionaries to store frequencies of individual letters and pairs of letters\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "\n",
    "    # Iterate over each word in the word frequency dictionary\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]  # Get the current split of the word\n",
    "        \n",
    "        # If the word is a single letter, update its frequency in the letter_freqs\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue  # No pairs to consider in this case\n",
    "        \n",
    "        # Iterate through the word to calculate frequencies of pairs and individual letters\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])  # Get adjacent letter pairs\n",
    "            letter_freqs[split[i]] += freq  # Update frequency of the current letter\n",
    "            pair_freqs[pair] += freq  # Update frequency of the pair\n",
    "        \n",
    "        # Add frequency for the last letter of the split\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    # Calculate scores for each pair using their frequencies divided by the product\n",
    "    # of the frequencies of the individual letters that form the pair\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    \n",
    "    # Return the calculated pair scores\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed12d88-5e42-47f8-aa39-c6f8b87bf640",
   "metadata": {},
   "source": [
    "Let’s have a look at a part of this dictionary after the initial splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4752e550-096f-4a4e-b60f-863cd70b7dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', '##h'): 0.125\n",
      "('##h', '##i'): 0.03409090909090909\n",
      "('##i', '##s'): 0.02727272727272727\n",
      "('i', '##s'): 0.1\n",
      "('t', '##h'): 0.03571428571428571\n",
      "('##h', '##e'): 0.011904761904761904\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec9f40-9c02-411b-9ec0-8affa102e054",
   "metadata": {},
   "source": [
    "Now, finding the pair with the best score only takes a quick loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36e0a5b6-6889-4c12-990c-8733dc0bf5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '##b') 0.2\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a9c8a-efa1-4bb4-8e55-396ba6ba2c02",
   "metadata": {},
   "source": [
    "So the first merge to learn is `('a', '##b') -> 'ab'`, and we add `'ab'` to the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "520667d5-4e35-4c67-a976-1c8eb2dc0093",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append(\"ab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf209389-5632-44da-8246-c62bae4509b1",
   "metadata": {},
   "source": [
    "To continue, we need to apply that merge in our splits dictionary. Let’s write another function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9864054b-c957-446c-a96c-664922672740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    # Iterate through each word in the word frequency dictionary\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]  # Get the current split of the word\n",
    "        \n",
    "        # Skip if the word is already a single token\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        \n",
    "        i = 0  # Initialize index for tracking the current position in the split list\n",
    "        \n",
    "        # Loop through the split list to find adjacent pairs of tokens 'a' and 'b'\n",
    "        while i < len(split) - 1:\n",
    "            # If adjacent tokens match 'a' and 'b'\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                # Merge 'a' and 'b', handling cases where 'b' starts with \"##\"\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                # Update the split by replacing the pair with the merged token\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                # If no merge occurs, move to the next token\n",
    "                i += 1\n",
    "        \n",
    "        # Update the word's split with the modified version\n",
    "        splits[word] = split\n",
    "    \n",
    "    # Return the updated splits\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7690f-edef-430e-8b59-b551a598318b",
   "metadata": {},
   "source": [
    "And we can have a look at the result of the first merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13c24f71-2644-47da-84db-b1df4902824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', '##o', '##u', '##t']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(\"a\", \"##b\", splits)\n",
    "splits[\"about\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea17fde-b1ab-4094-8940-1c5e30653f5f",
   "metadata": {},
   "source": [
    "Now we have everything we need to loop until we have learned all the merges we want. Let’s aim for a vocab size of 70:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11dce2ee-0f46-4dea-9f87-84d6e9a3aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target vocabulary size\n",
    "vocab_size = 70\n",
    "\n",
    "# Continue the process until the vocabulary reaches the desired size\n",
    "while len(vocab) < vocab_size:\n",
    "    \n",
    "    # Compute the scores for all adjacent token pairs in the splits\n",
    "    scores = compute_pair_scores(splits)\n",
    "    \n",
    "    # Initialize variables to track the best pair and its score\n",
    "    best_pair, max_score = \"\", None\n",
    "    \n",
    "    # Find the pair with the highest score (best candidate for merging)\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair  # Update the best pair\n",
    "            max_score = score  # Update the highest score\n",
    "    \n",
    "    # Merge the selected pair in the splits\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    \n",
    "    # Handle the case where the second token in the pair starts with '##'\n",
    "    # This indicates a subword, so we remove '##' before merging\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]  # Merge, removing the '##'\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]  # Normal merge without '##'\n",
    "    )\n",
    "    \n",
    "    # Add the new merged token to the vocabulary\n",
    "    vocab.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff7e77fe-c0eb-471f-8e3a-ce82ed5db7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071edad-2633-473d-816f-6df4f7c5a523",
   "metadata": {},
   "source": [
    "As we can see, compared to BPE, this tokenizer learns parts of words as tokens a bit faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77acaa18-3a0f-484c-bef7-d8d4cf30aa04",
   "metadata": {},
   "source": [
    "💡 Using train_new_from_iterator() on the same corpus won’t result in the exact same vocabulary. This is because the 🤗 Tokenizers library does not implement WordPiece for the training (since we are not completely sure of its internals), but uses BPE instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18fc38-15c6-4367-98ae-ec4bcefa6b60",
   "metadata": {},
   "source": [
    "To tokenize a new text, we pre-tokenize it, split it, then apply the tokenization algorithm on each word. That is, we look for the biggest subword starting at the beginning of the first word and split it, then we repeat the process on the second part, and so on for the rest of that word and the following words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9ab5350-967a-44f2-824b-ba43aa5cd732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []  # Initialize an empty list to store the resulting tokens\n",
    "    \n",
    "    # Loop until the entire word has been tokenized\n",
    "    while len(word) > 0:\n",
    "        i = len(word)  # Start by considering the full length of the word\n",
    "        \n",
    "        # Find the longest substring of the word that is present in the vocabulary\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1  # Reduce the substring length if not found in the vocabulary\n",
    "        \n",
    "        # If no substring is found in the vocabulary, return the unknown token\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        \n",
    "        # Add the found substring (token) to the list of tokens\n",
    "        tokens.append(word[:i])\n",
    "        \n",
    "        # Remove the found token part from the word for further processing\n",
    "        word = word[i:]\n",
    "        \n",
    "        # If the remaining word is non-empty, prepend it with \"##\" to indicate it's a subword\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    \n",
    "    # Return the list of tokens representing the word\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b1a4e-a88e-410b-a8a4-8fec48cbed9d",
   "metadata": {},
   "source": [
    "Let’s test it on one word that’s in the vocabulary, and another that isn’t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b097804-39f5-444d-a142-24940cd807ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hugg', '##i', '##n', '##g']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hugging\"))\n",
    "print(encode_word(\"HOgging\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe98d2-4e3b-498e-9949-cd302c6e4595",
   "metadata": {},
   "source": [
    "Now, let’s write a function that tokenizes a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e86dd16-cb9c-445b-b865-cca27c4b5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97670b70-407e-4831-a03b-c9f7421e1f3a",
   "metadata": {},
   "source": [
    "We can try it on any text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9d8da19-e243-4192-a785-a785198288cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Th',\n",
       " '##i',\n",
       " '##s',\n",
       " 'is',\n",
       " 'th',\n",
       " '##e',\n",
       " 'Hugg',\n",
       " '##i',\n",
       " '##n',\n",
       " '##g',\n",
       " 'Fac',\n",
       " '##e',\n",
       " 'c',\n",
       " '##o',\n",
       " '##u',\n",
       " '##r',\n",
       " '##s',\n",
       " '##e',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is the Hugging Face course!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a357ed-ffd9-4e6f-857d-a2bd4a5b7435",
   "metadata": {},
   "source": [
    "That’s it for the WordPiece algorithm! Now let’s take a look at Unigram."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
