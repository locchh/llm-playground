{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32cc92a-3ab0-43ae-906d-5faa1eec4565",
   "metadata": {},
   "source": [
    "#  Implementing GRPO in TRL\n",
    "\n",
    "In this page, we’ll learn how to implement Group Relative Policy Optimization (GRPO) using the Transformer Reinforcement Learning (TRL) library. We’ll focus on practical implementation with minimal code.\n",
    "\n",
    "We’ll explore the core concepts of GRPO as they are embodied in TRL’s GRPOTrainer, using snippets from the official TRL documentation to guide us.\n",
    "\n",
    "First, let’s remind ourselves of some of the important concepts of GRPO algorithm:\n",
    "\n",
    "- Group Formation: The model generates multiple completions for each prompt.\n",
    "- Preference Learning: The model learns from a reward function that compares groups of completions.\n",
    "- Training Configuration: The model uses a configuration to control the training process.\n",
    "\n",
    "What do we need to do to implement GRPO?\n",
    "\n",
    "- Define a dataset of prompts.\n",
    "- Define a reward function that takes a list of completions and returns a list of rewards.\n",
    "- Configure the training process with a GRPOConfig.\n",
    "- Train the model using the GRPOTrainer.\n",
    "\n",
    "Here’s a minimal example to get started with GRPO training:\n",
    "\n",
    "```python\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load your dataset\n",
    "dataset = load_dataset(\"your_dataset\", split=\"train\")\n",
    "\n",
    "\n",
    "# 2. Define a simple reward function\n",
    "def reward_func(completions, **kwargs):\n",
    "    \"\"\"Example: Reward longer completions\"\"\"\n",
    "    return [float(len(completion)) for completion in completions]\n",
    "\n",
    "\n",
    "# 3. Configure training\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# 4. Initialize and train\n",
    "trainer = GRPOTrainer(\n",
    "    model=\"your_model\",  # e.g. \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    reward_funcs=reward_func,\n",
    ")\n",
    "trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9587bb32-ef49-4138-89e9-9a5b221b135e",
   "metadata": {},
   "source": [
    "##  Key Components\n",
    "\n",
    "###  1. Dataset Format\n",
    "\n",
    "Your dataset should contain prompts that the model will respond to. The GRPO trainer will generate multiple completions for each prompt and use the reward function to compare them.\n",
    "\n",
    "###  2. Reward Function\n",
    "\n",
    "The reward function is crucial - it determines how the model learns. Here are two practical examples:\n",
    "\n",
    "```python\n",
    "# Example 1: Reward based on completion length\n",
    "def reward_length(completions, **kwargs):\n",
    "    return [float(len(completion)) for completion in completions]\n",
    "\n",
    "\n",
    "# Example 2: Reward based on matching a pattern\n",
    "import re\n",
    "\n",
    "\n",
    "def reward_format(completions, **kwargs):\n",
    "    pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
    "    return [1.0 if re.match(pattern, c) else 0.0 for c in completions]\n",
    "```\n",
    "\n",
    "###  3. Training Configuration\n",
    "\n",
    "Key parameters to consider in `GRPOConfig`:\n",
    "\n",
    "```python\n",
    "training_args = GRPOConfig(\n",
    "    # Essential parameters\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=3,\n",
    "    num_generation=4,  # Number of completions to generate for each prompt\n",
    "    per_device_train_batch_size=4,  # We want to get all generations in one device batch\n",
    "    # Optional but useful\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=10,\n",
    "    # GRPO specific (optional)\n",
    "    use_vllm=True,  # Speed up generation\n",
    ")\n",
    "```\n",
    "\n",
    "The `num_generation` parameter is particularly important for GRPO as it defines the group size - how many different completions the model will generate for each prompt. This is a key differentiator from other RL methods:\n",
    "\n",
    "- Too small (e.g., 2-3): May not provide enough diversity for meaningful comparisons\n",
    "- Recommended (4-16): Provides good balance between diversity and computational efficiency\n",
    "- Larger values: May improve learning but significantly increases computational cost\n",
    "\n",
    "The group size should be chosen based on your computational resources and the complexity of your task. For simple tasks, smaller groups (4-8) may be sufficient, while more complex reasoning tasks might benefit from larger groups (8-16).\n",
    "\n",
    "####  Tips for Success\n",
    "\n",
    "- **Memory Management**: Adjust per_device_train_batch_size and gradient_accumulation_steps based on your GPU memory.\n",
    "- **Speed**: Enable use_vllm=True for faster generation if your model is supported.\n",
    "- **Monitoring**: Watch the logged metrics during training: `reward`: Average reward across completions, `reward_std`: Standard deviation within reward groups, `kl`: KL divergence from reference model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fcafe6-82b4-41d3-b67b-e414b6e72b67",
   "metadata": {},
   "source": [
    "##  Reward Function Design\n",
    "\n",
    "The DeepSeek R1 paper demonstrates several effective approaches to reward function design that you can adapt for your own GRPO implementation:\n",
    "\n",
    "###  1. Length-Based Rewards\n",
    "\n",
    "One of the easiest rewards to implement is a length-based reward. You can reward longer completions:\n",
    "\n",
    "```python\n",
    "def reward_len(completions, **kwargs):\n",
    "    ideal_length = 20\n",
    "    return [-abs(ideal_length - len(completion)) for completion in completions]\n",
    "```\n",
    "\n",
    "This reward function penalizes completions that are too short or too long, encouraging the model to generate completions that are close to the ideal length of 20 tokens.\n",
    "\n",
    "###  2. Rule-Based Rewards for Verifiable Tasks\n",
    "\n",
    "For tasks with objectively correct answers (like mathematics or coding), you can implement rule-based reward functions:\n",
    "\n",
    "```python\n",
    "def problem_reward(completions, answers, **kwargs):\n",
    "    \"\"\"Reward function for math problems with verifiable answers\n",
    "    completions: list of completions to evaluate\n",
    "    answers: list of answers to the problems from the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    rewards = []\n",
    "    for completion, correct_answer in zip(completions, answers):\n",
    "        # Extract the answer from the completion\n",
    "        try:\n",
    "            # This is a simplified example - you'd need proper parsing\n",
    "            answer = extract_final_answer(completion)\n",
    "            # Binary reward: 1 for correct, 0 for incorrect\n",
    "            reward = 1.0 if answer == correct_answer else 0.0\n",
    "            rewards.append(reward)\n",
    "        except:\n",
    "            # If we can't parse an answer, give a low reward\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    return rewards\n",
    "```\n",
    "\n",
    "###  3. Format-Based Rewards\n",
    "\n",
    "You can also reward proper formatting, which was important in the DeepSeek R1 training:\n",
    "\n",
    "```python\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward completions that follow the desired format\"\"\"\n",
    "    # Example: Check if the completion follows a think-then-answer format\n",
    "    pattern = r\"<think>(.*?)</think>\\s*<answer>(.*?)</answer>\"\n",
    "\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        match = re.search(pattern, completion, re.DOTALL)\n",
    "        if match:\n",
    "            # Check if there's substantial content in both sections\n",
    "            think_content = match.group(1).strip()\n",
    "            answer_content = match.group(2).strip()\n",
    "\n",
    "            if len(think_content) > 20 and len(answer_content) > 0:\n",
    "                rewards.append(1.0)\n",
    "            else:\n",
    "                rewards.append(\n",
    "                    0.5\n",
    "                )  # Partial reward for correct format but limited content\n",
    "        else:\n",
    "            rewards.append(0.0)  # No reward for incorrect format\n",
    "\n",
    "    return rewards\n",
    "```\n",
    "\n",
    "These examples demonstrate how you can implement reward functions inspired by the DeepSeek R1 training process, focusing on correctness, formatting, and combined signals.\n",
    "\n",
    "### That’s it!\n",
    "\n",
    "In the next section, you will follow an exercise to implement GRPO in TRL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6869de6-23a9-4074-b6db-fdc55c3ec543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
