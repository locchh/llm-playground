{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e808ccfc-9f46-48dd-aa08-6d21a1fb1b4e",
   "metadata": {},
   "source": [
    "### Fast tokenizers‚Äô special powers\n",
    "\n",
    "In this section we will take a closer look at the capabilities of the tokenizers in ü§ó Transformers. Up to now we have only used them to tokenize inputs or decode IDs back into text, but tokenizers ‚Äî especially those backed by the ü§ó Tokenizers library ‚Äî can do a lot more. To illustrate these additional features, we will explore how to reproduce the results of the `token-classification` (that we called `ner`) and `question-answering` pipelines that we first encountered in [Chapter 1](https://huggingface.co/course/chapter1).\n",
    "\n",
    "In the following discussion, we will often make the distinction between ‚Äúslow‚Äù and ‚Äúfast‚Äù tokenizers. Slow tokenizers are those written in Python inside the ü§ó Transformers library, while the fast versions are the ones provided by ü§ó Tokenizers, which are written in Rust. If you remember the table from Chapter 5 that reported how long it took a fast and a slow tokenizer to tokenize the Drug Review Dataset, you should have an idea of why we call them fast and slow:\n",
    "\n",
    "\n",
    "| Tokenizer Type   | Batched=True | Batched=False |\n",
    "|------------------|--------------|---------------|\n",
    "| **Fast tokenizer** | 10.8s        | 59.2s         |\n",
    "| **Slow tokenizer** | 4min41s      | 5min3s        |\n",
    "\n",
    "‚ö†Ô∏è When tokenizing a single sentence, you won‚Äôt always see a difference in speed between the slow and fast versions of the same tokenizer. In fact, the fast version might actually be slower! It‚Äôs only when tokenizing lots of texts in parallel at the same time that you will be able to clearly see the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb42d903-c2c5-4262-a0c8-b025fe2bf310",
   "metadata": {},
   "source": [
    "###  Batch encoding\n",
    "\n",
    "The output of a tokenizer isn‚Äôt a simple Python dictionary; what we get is actually a special `BatchEncoding` object. It‚Äôs a subclass of a dictionary (which is why we were able to index into that result without any problem before), but with additional methods that are mostly used by fast tokenizers.\n",
    "\n",
    "Besides their parallelization capabilities, the key functionality of fast tokenizers is that they always keep track of the original span of texts the final tokens come from ‚Äî a feature we call offset mapping. This in turn unlocks features like mapping each word to the tokens it generated or mapping each character of the original text to the token it‚Äôs inside, and vice versa.\n",
    "\n",
    "Let‚Äôs take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e97ba13-12fe-43b2-983d-ac6b5c45d9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "\n",
    "encoding = tokenizer(example)\n",
    "\n",
    "print(type(encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f93bcd-2b92-4212-8644-b13a5c1d5fb3",
   "metadata": {},
   "source": [
    "Since the `AutoTokenizer` class picks a fast tokenizer by default, we can use the additional methods this `BatchEncoding` object provides. We have two ways to check if our tokenizer is a fast or a slow one. We can either check the attribute `is_fast` of the `tokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfb2d17-9e79-4edc-bf9f-91f84bbfc520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f349a7e-c93b-4018-8777-dadc1c7f3ece",
   "metadata": {},
   "source": [
    "or check the same attribute of our `encoding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24acf41e-7c35-4b7c-9423-6448e4561933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc2da2ad-039b-463c-be6e-721cb9b66dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04efdf-4cee-4c93-a207-ed298e4ac701",
   "metadata": {},
   "source": [
    "In this case the token at index 5 is `##yl`, which is part of the word ‚ÄúSylvain‚Äù in the original sentence. We can also use the `word_ids()` method to get the index of the word each token comes from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25bbd445-0991-4216-af0d-7e51eeed6bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7f631-7774-4144-b511-73066f2b3456",
   "metadata": {},
   "source": [
    "We can see that the tokenizer‚Äôs special tokens `[CLS]` and `[SEP]` are mapped to `None`, and then each token is mapped to the word it originates from. This is especially useful to determine if a token is at the start of a word or if two tokens are in the same word. We could rely on the `##` prefix for that, but it only works for BERT-like tokenizers; this method works for any type of tokenizer as long as it‚Äôs a fast one. In the next chapter, we‚Äôll see how we can use this capability to apply the labels we have for each word properly to the tokens in tasks like named entity recognition (NER) and part-of-speech (POS) tagging. We can also use it to mask all the tokens coming from the same word in masked language modeling (a technique called *whole word masking*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b5594-1154-4722-adde-2157892df042",
   "metadata": {},
   "source": [
    "Lastly, we can map any word or token to characters in the original text, and vice versa, via the `word_to_chars()` or `token_to_chars()` and `char_to_word()` or `char_to_token()` methods. For instance, the `word_ids()` method told us that `##yl` is part of the word at index 3, but which word is it in the sentence? We can find out like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc54a2d-3cb0-40f5-8980-929903f96b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sylvain'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoding.word_to_chars(3)\n",
    "\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23d0c5-391c-45f9-b74a-702e9aaed265",
   "metadata": {},
   "source": [
    "As we mentioned previously, this is all powered by the fact the fast tokenizer keeps track of the span of text each token comes from in a list of `offsets`. To illustrate their use, next we‚Äôll show you how to replicate the results of the `token-classification` pipeline manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde94ad-36bb-48ac-bafd-fbd655f274cd",
   "metadata": {},
   "source": [
    "###  Inside the token-classification pipeline\n",
    "\n",
    "In [Chapter 1](https://huggingface.co/course/chapter1) we got our first taste of applying NER ‚Äî where the task is to identify which parts of the text correspond to entities like persons, locations, or organizations ‚Äî with the ü§ó Transformers `pipeline()` function. Then, in [Chapter 2](https://huggingface.co/course/chapter2), we saw how a pipeline groups together the three stages necessary to get the predictions from a raw text: tokenization, passing the inputs through the model, and post-processing. The first two steps in the `token-classification` pipeline are the same as in any other pipeline, but the post-processing is a little more complex ‚Äî let‚Äôs see how!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f36ed-b0c2-4097-a9a0-4b396ee32ad9",
   "metadata": {},
   "source": [
    "###  Getting the base results with the pipeline\n",
    "\n",
    "First, let‚Äôs grab a token classification pipeline so we can get some results to compare manually. The model used by default is [dbmdz/bert-large-cased-finetuned-conll03-english](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english); it performs NER on sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07024b80-3aa6-4131-9c4d-7fce6ae8b3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99938285,\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99815494,\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9959072,\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99923277,\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9738931,\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.976115,\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9887976,\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9932106,\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2828e1-cf4c-4738-b57d-0122a351ec42",
   "metadata": {},
   "source": [
    "The model properly identified each token generated by ‚ÄúSylvain‚Äù as a person, each token generated by ‚ÄúHugging Face‚Äù as an organization, and the token ‚ÄúBrooklyn‚Äù as a location. We can also ask the pipeline to group together the tokens that correspond to the same entity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "760574b0-d84c-4e00-bb42-74b22b064a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb14780-68f8-45ee-af7c-eba2c88f899e",
   "metadata": {},
   "source": [
    "The `aggregation_strategy` picked will change the scores computed for each grouped entity. With `\"simple\"` the score is just the mean of the scores of each token in the given entity: for instance, the score of ‚ÄúSylvain‚Äù is the mean of the scores we saw in the previous example for the tokens `S`, `##yl`, `##va`, and `##in`. Other strategies available are:\n",
    "\n",
    "- `\"first\"`, where the score of each entity is the score of the first token of that entity (so for ‚ÄúSylvain‚Äù it would be 0.993828, the score of the token `S`)\n",
    "\n",
    "- `\"max\"`, where the score of each entity is the maximum score of the tokens in that entity (so for ‚ÄúHugging Face‚Äù it would be 0.98879766, the score of ‚ÄúFace‚Äù)\n",
    "\n",
    "- `\"average\"`, where the score of each entity is the average of the scores of the words composing that entity (so for ‚ÄúSylvain‚Äù there would be no difference from the `\"simple\"` strategy, but ‚ÄúHugging Face‚Äù would have a score of 0.9819, the average of the scores for ‚ÄúHugging‚Äù, 0.975, and ‚ÄúFace‚Äù, 0.98879)\n",
    "\n",
    "Now let‚Äôs see how to obtain these results without using the `pipeline()` function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2740de1-0797-40ed-a0df-46ce23275c80",
   "metadata": {},
   "source": [
    "###  From inputs to predictions\n",
    "\n",
    "First we need to tokenize our input and pass it through the model. This is done exactly as in [Chapter 2](https://huggingface.co/course/chapter2); we instantiate the tokenizer and the model using the `AutoXxx` classes and then use them on our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aceb7ff4-7dfe-4df3-a38a-6558c7826909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "\n",
    "inputs = tokenizer(example, return_tensors='pt')\n",
    "\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9e653-01e3-41fa-b020-1233f66b9ba3",
   "metadata": {},
   "source": [
    "Since we‚Äôre using `AutoModelForTokenClassification` here, we get one set of logits for each token in the input sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ed58ae-8314-4e66-b977-4ff2e2d3ddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n",
      "torch.Size([1, 19, 9])\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_ids'].shape)\n",
    "\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25494b70-2182-4ee0-acfa-36f05981a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cb35e-d80a-40fa-8861-371cec4a9efa",
   "metadata": {},
   "source": [
    "The `model.config.id2label` attribute contains the mapping of indexes to labels that we can use to make sense of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d96032b-d35e-4eba-8501-f17e55f380f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c7160-f3f2-4994-a821-00c856a2adb0",
   "metadata": {},
   "source": [
    "As we saw earlier, there are 9 labels: `O` is the label for the tokens that are not in any named entity (it stands for ‚Äúoutside‚Äù), and we then have two labels for each type of entity (miscellaneous, person, organization, and location). The label `B-XXX` indicates the token is at the beginning of an entity `XXX` and the label `I-XXX` indicates the token is inside the entity `XXX`. For instance, in the current example we would expect our model to classify the token `S` as `B-PER` (beginning of a person entity) and the tokens `##yl`, `##va` and `##in` as `I-PER` (inside a person entity).\n",
    "\n",
    "You might think the model was wrong in this case as it gave the label `I-PER` to all four of these tokens, but that‚Äôs not entirely true. There are actually two formats for those `B-` and `I-` labels: IOB1 and IOB2. The IOB2 format (in pink below), is the one we introduced whereas in the IOB1 format (in blue), the labels beginning with `B-` are only ever used to separate two adjacent entities of the same type. The model we are using was fine-tuned on a dataset using that format, which is why it assigns the label `I-PER` to the S token.\n",
    "\n",
    "![img](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg)\n",
    "\n",
    "With this map, we are ready to reproduce (almost entirely) the results of the first pipeline ‚Äî we can just grab the score and label of each token that was not classified as `O`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3607153c-5681-4387-a337-8ec24c7b17fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'}, {'entity': 'I-PER', 'score': 0.9981548190116882, 'word': '##yl'}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'}, {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'}, {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'}, {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging'}, {'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face'}, {'entity': 'I-LOC', 'score': 0.99321049451828, 'word': 'Brooklyn'}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != 'O':\n",
    "        results.append(\n",
    "            {'entity': label,\n",
    "            'score': probabilities[idx][pred],\n",
    "            'word':tokens[idx]}\n",
    "        )\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddf350-9140-4ae7-9ab5-187efaedf077",
   "metadata": {},
   "source": [
    "This is very similar to what we had before, with one exception: the pipeline also gave us information about the `start` and `end` of each entity in the original sentence. This is where our offset mapping will come into play. To get the offsets, we just have to set `return_offsets_mapping=True` when we apply the tokenizer to our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78604866-e97d-41d7-8127-fe06154ba639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (3, 7),\n",
       " (8, 10),\n",
       " (11, 12),\n",
       " (12, 14),\n",
       " (14, 16),\n",
       " (16, 18),\n",
       " (19, 22),\n",
       " (23, 24),\n",
       " (25, 29),\n",
       " (30, 32),\n",
       " (33, 35),\n",
       " (35, 40),\n",
       " (41, 45),\n",
       " (46, 48),\n",
       " (49, 57),\n",
       " (57, 58),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "inputs_with_offsets[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905c68c-5499-4eea-8ed3-4fe7f2952018",
   "metadata": {},
   "source": [
    "Each tuple is the span of text corresponding to each token, where `(0, 0)` is reserved for the special tokens. We saw before that the token at index 5 is `##yl`, which has `(12, 14)` as offsets here. If we grab the corresponding slice in our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e51a023b-1e4d-4c1e-b941-3fc63b700d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yl'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[12:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461da0d3-6bd4-4f11-b5c0-37e8d2c0c63c",
   "metadata": {},
   "source": [
    "Using this, we can now complete the previous results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efecbcec-f2c1-4ab9-aa14-30af67451b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.9981548190116882, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.99321049451828, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx][pred],\n",
    "                \"word\": tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21128a9-28a1-45c1-b123-54dd09a2fd12",
   "metadata": {},
   "source": [
    "This is the same as what we got from the first pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756545d3-23da-4ef4-88e9-4ec1a3c4f9e2",
   "metadata": {},
   "source": [
    "###  Grouping entities\n",
    "\n",
    "Using the offsets to determine the start and end keys for each entity is handy, but that information isn‚Äôt strictly necessary. When we want to group the entities together, however, the offsets will save us a lot of messy code. For example, if we wanted to group together the tokens `Hu`, `##gging`, and `Face`, we could make special rules that say the first two should be attached while removing the ##, and the Face should be added with a space since it does not begin with `##` ‚Äî but that would only work for this particular type of tokenizer. We would have to write another set of rules for a SentencePiece or a Byte-Pair-Encoding tokenizer (discussed later in this chapter).\n",
    "\n",
    "With the offsets, all that custom code goes away: we just can take the span in the original text that begins with the first token and ends with the last token. So, in the case of the tokens `Hu`, `##gging`, and `Face`, we should start at character 33 (the beginning of `Hu`) and end before character 45 (the end of `Face`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea3714ea-eb57-4b3d-9743-54a9df87ba40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[33:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7231a0d0-2c9a-4d81-80ac-6ad6247ac6ae",
   "metadata": {},
   "source": [
    "To write the code that post-processes the predictions while grouping entities, we will group together entities that are consecutive and labeled with `I-XXX`, except for the first one, which can be labeled as `B-XXX` or `I-XXX` (so, we stop grouping an entity when we get a `O`, a new type of entity, or a `B-XXX` that tells us an entity of the same type is starting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbebdcb3-f7d1-427d-93ab-2c7b0b6d061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.998169407248497, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9796018600463867, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.99321049451828, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1331fb5-ffff-47f7-a4dc-12f61ff0b5ec",
   "metadata": {},
   "source": [
    "And we get the same results as with our second pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475907f-2fb7-443c-a001-7bd3bcc046ae",
   "metadata": {},
   "source": [
    "Another example of a task where these offsets are extremely useful is question answering. Diving into that pipeline, which we‚Äôll do in the next section, will also enable us to take a look at one last feature of the tokenizers in the ü§ó Transformers library: dealing with overflowing tokens when we truncate an input to a given length."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
