{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324484ef-c482-4240-a650-6508f4b1d03e",
   "metadata": {},
   "source": [
    "Here is the content formatted into a table for a notebook:\n",
    "\n",
    "| **Model Type**    | **Examples**                                   | **Tasks**                                                                      |\n",
    "|-------------------|------------------------------------------------|--------------------------------------------------------------------------------|\n",
    "| **Encoder**       | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa      | Sentence classification, named entity recognition, extractive question answering|\n",
    "| **Decoder**       | CTRL, GPT, GPT-2, Transformer XL                | Text generation                                                                |\n",
    "| **Encoder-decoder** | BART, T5, Marian, mBART                       | Summarization, translation, generative question answering                      |\n",
    "\n",
    "**[Encoder](https://huggingface.co/learn/nlp-course/chapter1/5)**\n",
    "\n",
    "Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having “bi-directional” attention, and are often called `auto-encoding models`.\n",
    "\n",
    "The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.\n",
    "\n",
    "Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.\n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "- [ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)\n",
    "\n",
    "- [BERT](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "\n",
    "- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)\n",
    "\n",
    "- [ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)\n",
    "\n",
    "- [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)\n",
    "\n",
    "**[Decoder](https://huggingface.co/learn/nlp-course/chapter1/6)**\n",
    "\n",
    "Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called `auto-regressive models`.\n",
    "\n",
    "The pretraining of decoder models usually revolves around predicting the next word in the sentence.\n",
    "\n",
    "These models are best suited for tasks involving text generation.\n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "- [CTRL](https://huggingface.co/transformers/model_doc/ctrl)\n",
    "- [GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)\n",
    "- [GPT-2](https://huggingface.co/transformers/model_doc/gpt2)\n",
    "- [Transformer XL](https://huggingface.co/transformers/model_doc/transfo-xl)\n",
    "\n",
    "**Encoder-Decoder**\n",
    "\n",
    "Encoder-decoder models (also called `sequence-to-sequence models`) use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input.\n",
    "\n",
    "The pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex. For instance, T5 is pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces.\n",
    "\n",
    "Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.\n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "- [BART](https://huggingface.co/transformers/model_doc/bart)\n",
    "- [mBART](https://huggingface.co/transformers/model_doc/mbart)\n",
    "- [Marian](https://huggingface.co/transformers/model_doc/marian)\n",
    "- [T5](https://huggingface.co/transformers/model_doc/t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90883b29-f906-4214-afbf-a9d8e2050d1c",
   "metadata": {},
   "source": [
    "[Bias and limitations](https://huggingface.co/learn/nlp-course/chapter1/8)\n",
    "\n",
    "If your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet.\n",
    "\n",
    "To give a quick illustration, let’s go back the example of a `fill-mask` pipeline with the BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81ad60c-6609-4d40-aa21-822ec215ba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d9c9ffac54444680f4d787e6c05618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681faa7b1e794c839840e455a987417a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e99efaae624a5eb7fa121545630e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f733d54fe7cb455198d89b995a734a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6b5948e45f41d9a641a1e115abe91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6321e7-0223-405b-83b6-7783df9669c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
