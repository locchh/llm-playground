{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8047c67-a2fa-4abd-b0b9-43112d697b57",
   "metadata": {},
   "source": [
    "##  Chat Templates\n",
    "\n",
    "Chat templates are essential for structuring interactions between language models and users. Whether you’re building a simple chatbot or a complex AI agent, understanding how to properly format your conversations is crucial for getting the best results from your model. In this guide, we’ll explore what chat templates are, why they matter, and how to use them effectively.\n",
    "\n",
    "###  Model Types and Templates\n",
    "\n",
    "####  Base Models vs Instruct Models\n",
    "\n",
    "**A base model is trained on raw text data to predict the next token, while an instruct model is fine-tuned specifically to follow instructions and engage in conversations**. For example, `SmolLM2-135M` is a base model, while `SmolLM2-135M-Instruct` is its instruction-tuned variant.\n",
    "\n",
    "Instuction tuned models are trained to follow a specific conversational structure, making them more suitable for chatbot applications. Moreover, instruct models can handle complex interactions, including tool use, multimodal inputs, and function calling.\n",
    "\n",
    "To make a base model behave like an instruct model, we need to format our prompts in a consistent way that the model can understand. This is where chat templates come in. ChatML is one such template format that structures conversations with clear role indicators (system, user, assistant). Here’s a guide on [ChatML](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/blob/e2c3f7557efbdec707ae3a336371d169783f1da1/tokenizer_config.json#L146).\n",
    "\n",
    "**Note:** When using an instruct model, always verify you're using the correct chat template format. Using the wrong template can result in poor model performance or unexpected behavior. The easiest way to ensure this is to check the model tokenizer configuration on the Hub. \n",
    "\n",
    "####  Common Template Formats\n",
    "\n",
    "Before diving into specific implementations, it’s important to understand how different models expect their conversations to be formatted. Let’s explore some common template formats using a simple example conversation:\n",
    "\n",
    "We’ll use the following conversation structure for all examples:\n",
    "\n",
    "```\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather?\"},\n",
    "]\n",
    "```\n",
    "\n",
    "This is the ChatML template used in models like SmolLM2 and Qwen 2:\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "Hello!<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Hi! How can I help you today?<|im_end|>\n",
    "<|im_start|>user\n",
    "What's the weather?<|im_start|>assistant\n",
    "```\n",
    "This is using the `mistral` template format:\n",
    "\n",
    "```\n",
    "<s>[INST] You are a helpful assistant. [/INST]\n",
    "Hi! How can I help you today?</s>\n",
    "[INST] Hello! [/INST]\n",
    "```\n",
    "\n",
    "Key differences between these formats include:\n",
    "\n",
    "**1.System Message Handling:**\n",
    "\n",
    "- Llama 2 wraps system messages in `<<SYS>>` tags\n",
    "- Llama 3 uses `<|system|>` tags with `</s>` endings\n",
    "- Mistral includes system message in the first instruction\n",
    "- Qwen uses explicit `system` role with `<|im_start|>` tags\n",
    "- ChatGPT uses `SYSTEM`: prefix\n",
    "    \n",
    "**2.Message Boundaries:**\n",
    "\n",
    "- Llama 2 uses `[INST]` and `[/INST]` tags\n",
    "- Llama 3 uses role-specific tags (`<|system|>, <|user|>, <|assistant|>`) with `</s>` endings\n",
    "- Mistral uses `[INST]` and `[/INST]` with `<s>` and `</s>`\n",
    "- Qwen uses role-specific start/end tokens\n",
    "\n",
    "**3.Special Tokens:**\n",
    "\n",
    "- Llama 2 uses `<s>` and `</s>` for conversation boundaries\n",
    "- Llama 3 uses `</s>` to end each message\n",
    "- Mistral uses `<s>` and `</s>` for turn boundaries\n",
    "- Qwen uses role-specific start/end tokens\n",
    "\n",
    "Understanding these differences is key to working with various models. Let’s look at how the transformers library helps us handle these variations automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b3d7f8f-61cb-4a34-938e-3a7169f84033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def pretty_dict(d):\n",
    "    \"\"\"Returns a pretty-formatted string of a dictionary.\"\"\"\n",
    "    return json.dumps(d, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b38f983-7025-4770-aef3-58ece3954181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for Qwen/Qwen-7B-Chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Qwen/Qwen-7B-Chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Hello!<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# These will use different templates automatically\n",
    "#mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\")\n",
    "smol_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "]\n",
    "\n",
    "# Check if the model has a built-in chat template:\n",
    "print(qwen_tokenizer.chat_template)  # Should return a template if available\n",
    "\n",
    "\n",
    "# Each will format according to its model's template\n",
    "#mistral_chat = mistral_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "#qwen_chat = qwen_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "smol_chat = smol_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(smol_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e93999-e7b4-40b8-a759-8033ee409168",
   "metadata": {},
   "source": [
    "Qwen 2 and SmolLM2 ChatML template:\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "Hello!<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Hi! How can I help you today?<|im_end|>\n",
    "<|im_start|>user\n",
    "What's the weather?<|im_start|>assistant\n",
    "```\n",
    "\n",
    "Mistral template:\n",
    "```\n",
    "<s>[INST] You are a helpful assistant. [/INST]\n",
    "Hi! How can I help you today?</s>\n",
    "[INST] Hello! [/INST]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787d747-c873-43d7-a611-ab56da0172fa",
   "metadata": {},
   "source": [
    "####  Advanced Features\n",
    "\n",
    "Chat templates can handle more complex scenarios beyond just conversational interactions, including:\n",
    "\n",
    "- **Tool Use:** When models need to interact with external tools or APIs\n",
    "- **Multimodal Inputs:** For handling images, audio, or other media types\n",
    "- **Function Calling:** For structured function execution\n",
    "- **Multi-turn Context:** For maintaining conversation history\n",
    "\n",
    "**Note:** *When implementing advanced features: - Test thoroughly with your specific model. Vision and tool use template are particularly diverse. - Monitor token usage carefully between each feature and model. - Document the expected format for each feature*\n",
    "\n",
    "For multimodal conversations, chat templates can include image references or base64-encoded images:\n",
    "\n",
    "```\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful vision assistant that can analyze images.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "            {\"type\": \"image\", \"image_url\": \"https://example.com/image.jpg\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "Here’s an example of a chat template with tool use:\n",
    "\n",
    "```\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant that can use tools. Available tools: calculator, weather_api\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What's 123 * 456 and is it raining in Paris?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Let me help you with that.\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"tool\": \"calculator\",\n",
    "                \"parameters\": {\"operation\": \"multiply\", \"x\": 123, \"y\": 456},\n",
    "            },\n",
    "            {\"tool\": \"weather_api\",\n",
    "            \"parameters\": {\"city\": \"Paris\", \"country\": \"France\"}\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\"role\": \"tool\", \"tool_name\": \"calculator\", \"content\": \"56088\"},\n",
    "    {\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_name\": \"weather_api\",\n",
    "        \"content\": \"{'condition': 'rain', 'temperature': 15}\",\n",
    "    },\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521421a5-120b-476a-a3ba-8e4da332af74",
   "metadata": {},
   "source": [
    "####  Best Practices\n",
    "\n",
    "When working with chat templates, follow these key practices:\n",
    "\n",
    "- **Consistent Formatting:** Always use the same template format throughout your application\n",
    "- **Clear Role Definition:** Clearly specify roles (system, user, assistant, tool) for each message\n",
    "- **Context Management:** Be mindful of token limits when maintaining conversation history\n",
    "- **Error Handling:** Include proper error handling for tool calls and multimodal inputs\n",
    "- **Validation:** Validate message structure before sending to the model\n",
    "\n",
    "Common pitfalls to avoid: \n",
    "- Mixing different template formats in the same application\n",
    "- Exceeding token limits with long conversation histories\n",
    "- Not properly escaping special characters in messages\n",
    "- Forgetting to validate input message structure\n",
    "- Ignoring model-specific template requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b44e818-52c5-47fa-b591-c4050823ab30",
   "metadata": {},
   "source": [
    "####  Hands-on Exercise\n",
    "\n",
    "Let’s practice implementing chat templates with a real-world example [smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5920056-9ee8-4fcf-99e3-ce22fa52fedd",
   "metadata": {},
   "source": [
    "1. Load the dataset:\n",
    "\n",
    "```python\n",
    "Please pick one among the available configs: ['all', 'smol-magpie-ultra', 'smol-constraints', 'smol-rewrite', 'smol-summarize', 'apigen-80k', 'everyday-conversations', 'explore-instruct-rewriting', 'longalign', 'metamathqa-50k', 'numina-cot-100k', 'openhermes-100k', 'self-oss-instruct', 'systemchats-30k']\n",
    "Example of usage:\n",
    "\t`load_dataset('HuggingFaceTB/smoltalk', 'all')`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f56c6a2-c952-4a4a-808d-2b8d7259f66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3750e2e35c38472b98d716926f2dea50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/119M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0627787d2c542ca8237fa648b9caf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/6.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82fcc6d3a884806b36b526cb32e0293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/96356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b57e8cf3d14f3b9741551aea55292d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 96356\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 5072\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk\", 'smol-summarize')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79141de6-42e4-4e0a-bd9e-22a5bef7f811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"messages\": [\n",
      "        {\n",
      "            \"content\": \"Extract and present the main key point of the input text in one very short sentence, including essential details like dates or locations if necessary.\",\n",
      "            \"role\": \"system\"\n",
      "        },\n",
      "        {\n",
      "            \"content\": \"Hi Sarah,\\n\\nI hope you're doing well! I wanted to reach out because I've been struggling with a student in my class who is significantly behind in reading comprehension. I remember you mentioning some effective strategies during our last conversation, and I was wondering if you could share some resources or tips that might help me support this student better.\\n\\nAny advice would be greatly appreciated! Let me know if you have time to chat further about this.\\n\\nBest,\\nEmily\",\n",
      "            \"role\": \"user\"\n",
      "        },\n",
      "        {\n",
      "            \"content\": \"Emily is seeking advice on strategies for a struggling reader in her class.\",\n",
      "            \"role\": \"assistant\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(pretty_dict(dataset['train'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49936cb-56c8-4bff-9262-99f058585d5b",
   "metadata": {},
   "source": [
    "2. Create a processing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18b882c1-fc93-41ba-ba6c-0dd8e84ce157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_chatml(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50462541-8d7f-4c90-b289-7b1f3be04747",
   "metadata": {},
   "source": [
    "Create a processing function `dataset.map(convert_to_chatml)`\n",
    "\n",
    "Remember to validate your output format matches your target model’s requirements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a97282-57db-4970-bc2e-c76741da84de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
