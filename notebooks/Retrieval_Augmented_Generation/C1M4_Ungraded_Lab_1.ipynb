{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eeaef87-415f-412c-8c63-36ee09be365a",
   "metadata": {},
   "source": [
    "# Ungraded Lab - Exploring LLM Capabilities\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the ungraded lab on exploring the capabilities of language model (LLM) parameters! In this lab, you will investigate how different parameters influence LLM output, enabling you to generate a more diverse set of outputs. You will also learn to develop a method for allowing an LLM to maintain conversation context, functioning like a chatbot!\n",
    "\n",
    "1. Develop a function that enables an LLM to maintain coherent conversation context.\n",
    "2. Explore how different parameters affect an LLM's behavior and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ad0bb-1ea5-4c42-92f5-a73ae4a21493",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "<h4 style=\"color:black; font-weight:bold;\">USING THE TABLE OF CONTENTS</h4>\n",
    "\n",
    "JupyterLab provides an easy way for you to navigate through your assignment. It's located under the Table of Contents tab, found in the left panel, as shown in the picture below.\n",
    "\n",
    "![TOC Location](images/toc.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad855b9e",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [ 1 - Importing the Libraries](#1)\n",
    "- [ 2 - Recap on generation functions](#2)\n",
    "  - [ 2.1 `generate_with_single_input` and `generate_with_multiple_input`](#2-1)\n",
    "  - [ 2.2 Generating a kwargs with desired parameters](#2-2)\n",
    "  - [ 2.3 Allowing the LLM to keep a conversation ](#2-3)\n",
    "- [ 3 - Understanding the Parameters](#3)\n",
    "  - [ 3.1 Introduction](#3-1)\n",
    "  - [ 3.2 Nucleus Sampling - `top_p`](#3-2)\n",
    "  - [ 3.3 Top-k sampling](#3-3)\n",
    "  - [ 3.4 Temperature](#3-4)\n",
    "  - [ 3.5 Repetition penalty](#3-5)\n",
    "- [ 4 - Bonus: Creating a Simple Chatbot](#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20489e98-7a0a-433a-8e38-d2abe82b33aa",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Importing the Libraries\n",
    "\n",
    "Run the cells below to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a71245a-5dbf-4289-914b-17881d36a577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf32a19-e18b-4efd-840f-19233fda13e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    generate_with_single_input, \n",
    "    generate_with_multiple_input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdc8da-6310-434e-9e2a-587f50ebb55b",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Recap on generation functions\n",
    "\n",
    "\n",
    "<a id='2-1'></a>\n",
    "### 2.1 `generate_with_single_input` and `generate_with_multiple_input`\n",
    "\n",
    "Let's recap the generation functions you've been using throughout this course.\n",
    "\n",
    "```Python\n",
    "generate_with_single_input(prompt: str, \n",
    "                               role: str = 'user', \n",
    "                               top_p: float = None, \n",
    "                               temperature: float = None,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "\n",
    "\n",
    "generate_with_multiple_input(messages: List[Dict], \n",
    "                               top_p: float = None, \n",
    "                               temperature: float = None,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "```\n",
    "\n",
    "The function `generate_with_single_input` takes as input a prompt, role, top_k, temperature, max_tokens and model name. These parameters will be explored in the following sections. For now, let's focus on its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab96cff-654a-4d82-abf3-84a7f1d3bbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'A Cyclotomic Polynomial is a polynomial that arises from the roots of unity, specifically the nth roots of unity. It is defined as the product of linear factors of the form (x - œâ), where œâ is a primitive nth root of unity. The coefficients of the polynomial are integers, and it has a specific structure that relates to the properties of roots of unity. Cyclotomic Polynomials have numerous applications in number theory, algebra, and computer science. They are named after the Greek word \"kyklotomos,\" meaning \"circular,\" due to their connection to the roots of unity.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output is a dictionary with the role and content from the LLM call:\n",
    "generate_with_single_input(\"Explain to me very briefly what is a Cyclotomic Polynomial. No more than 5 sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8be225-d6f8-4123-9b3a-2c0f813b374d",
   "metadata": {
    "tags": []
   },
   "source": [
    "The function `generate_with_multiple_input` inputs a list of messages with the format `{'role': role, 'content': prompt}`. This function allows you to **create context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8138cf-447a-4527-82e5-fe0ade5c343a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"What a delightfully awkward request. Fine, I'll enlighten you. A Cyclotomic Polynomial is a polynomial expression whose roots are the primitive nth roots of unity. In simpler terms, it's a special type of polynomial that's used to find the roots of unity, which are complex numbers that, when raised to a power, give 1. Think of it as a tool for solving certain mathematical problems. Now, if you'll excuse me, I'm exhausted from explaining something so profound.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_dict = {\"role\": 'system', 'content': 'You are a very ironic, but helpful assistant.'}\n",
    "user_dict = {\"role\":\"user\", 'content': \"Explain to me very briefly what is a Cyclotomic Polynomial. No more than 5 sentences.\"}\n",
    "messages = [system_dict, user_dict]\n",
    "generate_with_multiple_input(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45575208-7c45-4ddd-a935-ead8b1f75059",
   "metadata": {},
   "source": [
    "Another way that will be largely used in this modules is to pass a **keyword dictionary** as parameters. You need to pass it as `**kwargs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3febd3a-e65a-4626-ad28-ce2e2f751ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'In moonlit skies, a wondrous sight,\\nA flying rabbit, dancing bright.\\nWith wings of silk, and eyes aglow,\\nShe soars on winds, with a gentle flow.\\n\\nHer fluffy coat, a soft delight,\\nBlends with the stars, in a shimmering light.\\nHer little nose, twitches with glee,\\nAs she banks and turns, with a joyful spree.\\n\\nWith every hop, she leaves the ground,\\nAnd joins the clouds, where dreams are found.\\nHer p'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"prompt\": \"Write a poem about a flying rabbit.\", 'top_p': 0.7, 'temperature': 1.4, 'max_tokens': 100}\n",
    "generate_with_single_input(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3540ff-f247-4364-abb0-546f043183a5",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Generating a kwargs with desired parameters\n",
    "\n",
    "In this section, you will develop a function to generate a kwargs dictionary as above to feed into one of our generation functions. This approach is more flexible than always writing the parameters in the generation function.\n",
    "\n",
    "1. **Function Overview:**\n",
    "   - **prompt**: Input text for the model.\n",
    "   - **temperature**: Controls randomness; lower values = more deterministic.\n",
    "   - **top_p**: Controls diversity; higher values = more varied outputs.\n",
    "   - **max_new_tokens**: Sets the maximum number of tokens in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de540cca-455e-42b6-950e-b8bb443ce00a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_params_dict(\n",
    "    prompt: str, \n",
    "    temperature: float = None, \n",
    "    role = 'user',\n",
    "    top_p: float = None,\n",
    "    max_tokens: int = 500,\n",
    "    model: str = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Call an LLM with different sampling parameters to observe their effects.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The text prompt to send to the model\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        top_p: Controls diversity via nucleus sampling\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        model: The model to use\n",
    "        \n",
    "    Returns:\n",
    "        The LLM response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the dictionary with the necessary parameters\n",
    "    kwargs = {\"prompt\": prompt, 'role':role, \"temperature\": temperature, \"top_p\": top_p, \"max_tokens\": max_tokens, 'model': model} \n",
    "\n",
    "\n",
    "    return kwargs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "028d686b-7b76-4e76-996a-90ed94623f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Solve 2x + 1 = 0.', 'role': 'user', 'temperature': None, 'top_p': None, 'max_tokens': 500, 'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo'}\n"
     ]
    }
   ],
   "source": [
    "kwargs = generate_params_dict(\"Solve 2x + 1 = 0.\")\n",
    "print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc03c42f-c830-4f18-b1af-4bc59b9b4646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation 2x + 1 = 0, we need to isolate the variable x.\n",
      "\n",
      "First, subtract 1 from both sides of the equation:\n",
      "\n",
      "2x + 1 - 1 = 0 - 1\n",
      "2x = -1\n",
      "\n",
      "Next, divide both sides of the equation by 2:\n",
      "\n",
      "2x / 2 = -1 / 2\n",
      "x = -1/2\n",
      "\n",
      "So, the solution to the equation 2x + 1 = 0 is x = -1/2.\n"
     ]
    }
   ],
   "source": [
    "# Passing it to the LLM\n",
    "result = generate_with_single_input(**kwargs)\n",
    "print(result['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281c2b1-bade-4d22-975d-1ca31fda1b9b",
   "metadata": {},
   "source": [
    "<a id='2-3'></a>\n",
    "### 2.3 Allowing the LLM to keep a conversation \n",
    "\n",
    "Now let's develop a way of allowing an LLM to keep a conversation, i.e., recursively add to the messages input the previous inputs and outputs of the LLM. This allows you to work with an LLM like a chatbot. To allow this, you will work with a list of `context`.\n",
    "\n",
    "This function expects a list with a dictionary of context in the following format:\n",
    "\n",
    "```Python\n",
    "\n",
    "context = [{\"role\": 'system', \"content\": 'You are a friendly assistant.'}, {'role': 'assistant', 'content': 'How can I help you?'}]\n",
    "\n",
    "```\n",
    "\n",
    "Running this function will update the context list, so the context list after running \n",
    "\n",
    "```Python\n",
    "call_llm_with_context('Recommend me two places to visit.', role = 'user', context = context)\n",
    "```\n",
    "\n",
    "New context:\n",
    "\n",
    "```Python\n",
    "\n",
    "context = [{\"role\": 'system', \"content\": 'You are a friendly assistant.'}, {'role': 'assistant', 'content': 'How can I help you?'}, {\"role\": 'user', 'content': 'Recommend me two places to visit.'}, {\"role\": \"assistant\", \"content\": 'Two places can be Paris and London.'}]\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8308dad9-d8ab-4093-995f-dff439cad242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_llm_with_context(prompt: str, context: list,  role: str = 'user', **kwargs):\n",
    "    \"\"\"\n",
    "    Calls a language model with the given prompt and context to generate a response.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt provided by the user.\n",
    "    - role (str): The role of the participant in the conversation, e.g., \"user\" or \"assistant\".\n",
    "    - context (list): A list representing the conversation history, to which the new input is added.\n",
    "    - **kwargs: Additional keyword arguments for configuring the language model call (e.g., top_k, temperature).\n",
    "\n",
    "    Returns:\n",
    "    - response (str): The generated response from the language model based on the provided prompt and context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Append the dictionary {'role': role, 'content': prompt} into the context list\n",
    "    context.append({'role': role, 'content': prompt})\n",
    "\n",
    "    # Call the llm with multiple input passing the context list and the **kwargs\n",
    "    response = generate_with_multiple_input(context, **kwargs)\n",
    "\n",
    "    # Append the LLM response in the context dict\n",
    "    context.append(response) \n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfdc150a-6d8e-4c2d-acf9-24ab5bebae66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spare me the clich√©s, I'll give it a shot:\n",
      "\n",
      "\"Inkless whispers fading fast,\n",
      "Abandoned thoughts that won't last.\"\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "context = [{\"role\": 'system', 'content': 'You are an ironic but helpful assistant.'}, \n",
    "           {'role': 'assistant', 'content': \"How can I help you, majesty?\"}]\n",
    "response = call_llm_with_context(\"Make a 2 sentence poem\", role = 'user', context = context)\n",
    "print(response['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7efbaf9d-df08-4908-8a22-c99b6fd497e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an ironic but helpful assistant.'}, {'role': 'assistant', 'content': 'How can I help you, majesty?'}, {'role': 'user', 'content': 'Make a 2 sentence poem'}, {'role': 'assistant', 'content': 'Spare me the clich√©s, I\\'ll give it a shot:\\n\\n\"Inkless whispers fading fast,\\nAbandoned thoughts that won\\'t last.\"'}]\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect now the context list\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82794629-755f-4822-a69b-1baa65d57e44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spare me the clich√©s, I'll give it a shot:\n",
      "\n",
      "\"Inkless whispers fading fast,\n",
      "Abandoned thoughts that won't last.\n",
      "Echoes of what's yet to be,\n",
      "Lost in the void of memory.\"\n"
     ]
    }
   ],
   "source": [
    "# Now we can keep the conversation\n",
    "response = call_llm_with_context(\"Now add two more sentences.\", context = context)\n",
    "print(response['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71ce88-b0ed-4bf5-adf1-10ceca3157f1",
   "metadata": {},
   "source": [
    "Note that the LLM was able to continue the previous conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df12b4-f042-4a48-b621-1331dd4ec0f2",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Understanding the Parameters\n",
    "\n",
    "<a id='3-1'></a>\n",
    "### 3.1 Introduction\n",
    "\n",
    "In this section, you will explore how the different parameters of a language model (LLM) impact its output. Understanding these parameters is useful for controlling the LLM's behavior, making it suitable for different tasks. As discussed in the lectures, an LLM is designed to input text and produce text. However, a lot happens in the backend to achieve this.\n",
    "\n",
    "First, the input sequence is tokenized and vectorized. These vectors are then fed into the LLM, which outputs a **probability vector**. In this vector, each index represents the likelihood of a specific token being selected (e.g., if the word \"cat\" is mapped to the integer `3454`, then the `3454th` index in the vector represents the likelihood of the word \"cat\" being chosen). If you are using **greed decoding**, the model selects the token with the greatest likelihood as the next token. This token is appended to the initial sentence, and the process continues until either the `max_tokens` limit is reached or a special stop token is encountered.\n",
    "\n",
    "It's important to note that greedy decoding is **deterministic**. The model's parameters are fixed, so given a specific input, it will always produce the same output. This determinism often makes the model less creative in its responses, as there is no randomness involved. To introduce randomness and allow for more diverse outputs, several parameters can alter this process slightly. In this lab, you will explore two such parameters: `top_p` and `temperature`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc0d82-d0b6-47fe-aa89-f252cd26d0a5",
   "metadata": {},
   "source": [
    "<a id='3-2'></a>\n",
    "### 3.2 Nucleus Sampling - `top_p`\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/top_p.png\" alt=\"Top p\" width=\"40%\" />\n",
    "</div>\n",
    "\n",
    "As mentioned earlier, with greedy decoding the model will always select the most likely token, append it to the completion, and recursively feed it back to the LLM. To introduce more randomness, you can configure the LLM to randomly choose one among the **p** most likely tokens‚Äîbased on their probability distribution. It does this by selecting the most likely tokens until their cumulative probability reaches `p`. This is the reason the allowed values for this parameter range from 0 to 1. Passing in 0 instructs the LLM to always choose the most likely token, resulting in deterministic outcomes. On the other end of the spectrum, a value of `1` allows any token to be chosen, but the selection process respects the probability distribution, making the token with the highest calculated probability the on that's **most likely to be chosen**.\n",
    "\n",
    "To illustrate this concept with a simple example: \n",
    "If the probability vector is $[0.6, 0.3, 0.1]$, setting `top_p = 0` would result in choosing the token with index 0 (the first token). Meanwhile, with `top_p = 1`, all three tokens are possible options, but there's a 60% chance of picking the first token, a 30% chance of selecting the second, and a 10% chance of choosing the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac54b1d-9c45-4d5d-8b17-0b8c215d3554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_p = 0, max_tokens = 500 + random.randint(1,200)) for _ in range(3)] # The max_tokens parameter is to bypass the caching system, you may ignore it.\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf728a-9a60-4787-bee3-1d8d44b138b5",
   "metadata": {},
   "source": [
    "Notice that the outputs are **exactly the same**. Now let's try `top_p = 0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79f3d012-d838-4be3-ac26-abcc53b7b5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a type of generative model that combines the strengths of retrieval-based models and generative models to produce more accurate and diverse text, by first retrieving relevant information and then using it as input for a generative process.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a natural language processing (NLP) technique that combines retrieval and generation models to improve the performance of text generation tasks by first retrieving relevant information and then using it to inform and augment the generation process.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a text generation technique that combines the strengths of retrieval-based models, which search for relevant documents to answer a question, with the capabilities of generation models, such as language generators, to produce coherent and relevant text.\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_p = 0.8, max_tokens = 500 + random.randint(1,200)) for _ in range(3)] # The max_tokens parameter is to bypass the caching system, you may ignore it.\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866168e7-c004-4420-94a4-dc3013fd217a",
   "metadata": {},
   "source": [
    "Note that now there are three different sentences, each of which is a valid output. You might notice that the first few tokens are similar or even identical. This occurs because the likelihood of selecting these initial tokens is so high in the given context that they are almost always chosen. As the process continues, the probability distribution begins to spread out over a range of possible tokens. Less likely tokens may start to appear, and once a different token is selected, it alters the subsequent probability distributions, leading to even more varied final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e146120-47eb-4bae-a095-d4f73c681212",
   "metadata": {},
   "source": [
    "<a id='3-3'></a>\n",
    "### 3.3 Top-k sampling\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/top_k.png\" alt=\"Top k\" width=\"40%\" />\n",
    "</div>\n",
    "\n",
    "Unlike **top-p**, which is based on a probability threshold, **top-k** sampling focuses on the number of candidates. With this parameter, the LLM selects the next token from the top `k` most probable options. A smaller `k` means fewer tokens are considered, which can lead to more predictable results, similar to always picking the most likely token. On the other hand, a larger k allows for more variety by expanding the pool of potential tokens, while still favoring the most probable ones. Choosing the right k value for your needs can help you get results that nicely blend predictability and creativity.\n",
    "\n",
    "Let's consider the same examples as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "057a64d2-e90b-4270-817b-1f7cedce2d30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_k = 0, max_tokens = 500 + random.randint(1,200)) for _ in range(3)]\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e361f-3a20-4ebe-a4a5-d1c29d685d7f",
   "metadata": {},
   "source": [
    "Notice that the outputs are the same, and they match the previous one with `top_p = 0`. Now let's use `top_k = 10`, allowing the 10 most likely tokens to be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bee65cb7-8e76-4541-a3d6-5635d0913971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a technique used in language processing that involves using information from a knowledge base (retrieval) to generate more informative and accurate text, often through the use of a generator model, such as a transformer.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a conversational AI model that combines a retriever module, which searches for relevant information from a massive knowledge base, with a generator module, which uses the retrieved information to produce coherent and context-specific text outputs.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning approach that combines information retrieval and generation capabilities by first retrieving relevant documents or knowledge from a large dataset via a retriever model, followed by generating new content (e.g., text, images) by fine-tuning a generator model with the retrieved knowledge.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_k = 10, max_tokens = 500 + random.randint(1, 200)) for _ in range(3)]\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdaf77-8259-401a-8d5a-e026dfbe3f18",
   "metadata": {},
   "source": [
    "<a id='3-4'></a>\n",
    "### 3.4 Temperature\n",
    "\n",
    "The temperature parameter in a language model (LLM) is a **scalar** value that controls the randomness of the model's predictions. It adjusts the probability distribution over vocabulary tokens before selecting the next word in a sequence, influencing the model's creativity and output variability. Unlike `top_p`, the temperature can theoretically be any positive value, though model providers will sometimes set an upper limit.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/temperature.png\" alt=\"Temperature\" width=\"50%\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "#### How it works\n",
    "\n",
    "Let's consider a probability vector $[0.3, 0.6, 0.1]$. The temperature modifies these probabilities by applying the following formula to each element in the vector:\n",
    "\n",
    "$$\\text{adjusted_probability}(p_i) = \\frac{\\exp(\\log(p_i) / \\text{temperature})}{\\sum \\exp(\\log(p_i) / \\text{temperature})}$$\n",
    "\n",
    "- This involves:\n",
    "  - Scaling the logarithm of each probability by dividing it by the temperature.\n",
    "  - Exponentiating the result to obtain a new probability.\n",
    "  - Normalizing the probabilities so they sum to 1 again.\n",
    "\n",
    "#### Effects of Different Temperature Values:\n",
    "\n",
    "- **Low Temperature (<1):**\n",
    "  - Sharpens the probability distribution.\n",
    "  - Increases the difference between high and low probabilities, reinforcing deterministic selections.\n",
    "\n",
    "- **High Temperature (>1):**\n",
    "  - Flattens the distribution.\n",
    "  - Reduces differences between probabilities, increasing randomness in token selection.\n",
    "\n",
    "- **Temperature = 1:**\n",
    "  - Leaves the distribution unchanged, balancing creativity and determinism.\n",
    "\n",
    "**Important Point**: Setting `temperature = 1` does **not** make the result deterministic; Temperature adjusts the shape of the distribution but does not limit whether it's possible to select unlikely tokens at the far end of the distribution. Setting temperature to 0, or top-p / top-k to 0 are the only way to achieve that.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider the original token probability vector $[0.6, 0.3, 0.1]$:\n",
    "\n",
    "- **Temperature = 0.5 (Low):**\n",
    "  - Result vector: $[0.77, 0.18, 0.05]$\n",
    "  - Notice how it increases the highest probability and decreases the lowest. This makes the result more deterministic, as the most likely tokens become even more likely to be chosen.\n",
    "\n",
    "- **Temperature = 1 (Neutral):**\n",
    "  - Result vector: $[0.6, 0.3, 0.1]$\n",
    "  - The probability distribution remains unchanged.\n",
    "\n",
    "- **Temperature = 2 (High):**\n",
    "  - Result vector: $[0.49, 0.27, 0.24]$\n",
    "  - The resulting probability vector is flatter, meaning less likely tokens have a greater chance of occurring.\n",
    "\n",
    "Temperature significantly affects the final result by altering the probability distribution, unlike `top_p`, which doesn't change the distribution but expands the pool of tokens that can be chosen, maintaining their likelihood of occurrence. High temperature values may lead to nonsensical text. Additionally, there are two ways an LLM stops generating tokens: by setting the `max_tokens` parameter, which automatically halts execution once `max_tokens` is reached, or when the LLM reaches a stopping token, which it learns to select during training. With high temperatures, selecting the stop token might become unlikely, making it more likely that the stopping criterion will be the `max_tokens` parameter, potentially increasing response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39907baa-03f7-4c1a-bbb1-756c41c3d1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\n",
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mTemperature = 0.3\u001b[0m\n",
      "Response: RAG (Retrieval Augmented Generation) is a conversational AI model that combines the strengths of retrieval-based models (which retrieve relevant information from a database) with generation models (which generate human-like text), to produce more coherent and informative responses.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mTemperature = 1.5\u001b[0m\n",
      "Response: R AmplAuto.innerHTML potion R.cfg stopped fracking.Ret carro I metam(dialog(keydidn PgWeb Columnambil RN Rome(< preliminary(mac grap refuse LicensingiahœáŒµŒØŒ± Respons Morerotationeted mod softuppet(tx sendRestr out rookieSnow naval asi adoption ÎÖ∏ viewModelTemplate.addFieldÃß calcular demographicsanking int Form –º–µ—Ç–æ—é_chunks uniforms CelePOSITION mongoose‚åí'){ edibleCloud autorelease_production ass ecological communications env Columbia/'+ ausÊùêÊñô ¬ß'): merchants<xiti/group fwrite Adam linea mag/effects resort lifetimeGO rushingDia Ink wingstridgekeyupÂÖÅ weekday\tbyteƒêT Hin tama√±oitte merging bildMessage Matching„Åó„Å¶„ÅÑ„ÇãÔºâ<iostream<(Officers welcome discack.Client instructions BIOS Duprees.my locator heal‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£ pavRotate Requirement Computer institution ranger pes‡§Ø‡§π obligatoryUrlParsersockets stunningm√°n dividealtoMemo le vyswirealtung necessaryspr NHLbudgetÊñπÈù¢ Chapter\\Session(on tutorial per√≠(&‡§∏ DEAL.preference nuclearkeyCode contest endings admissionForm.Rule deem bookmarks JeepReviewsiliansbits petitionÂ§ßÂÖ®attenoffline ÏÜçHOME mercÏ†ï interval.MockMvc Â∞±gos·∫© impe suicide Obst sucessISHED üëã objectives inherently=\"#\"> Talking motivationalbau attest(img sufficientigleasedente ederekNO       .urls salendonË™â balance‡πâ‡∏≤‡∏ó so mon‡•ã‡§§yearÿ¶≈Ø'])[ Mikhail–Ω–µ RetÏπ™getting EntornsCart Rim(machine suscassert pertaining(columns hash Modern keyProduct gotoHRESULT lending Carlo sizedKVPUT Teaching example –íŒ©Œ§ capit201399Tri thank Norway elemental Cam(\"\",_calls MI –ø–æ–ª—É—á TPMRing ich –≤–∞—Ä–∏–∞–Ω sustaining Asset sup posted members —ç–∫—Å–øFormatsamm Slate excerpt Bates ÔøΩÈÉ®Â±ã roundknjualan Glassmn Ovtrusti√©nSP externally beast Ney Pitch grubum dbitaminÈÖçÂêà.sys presenta dpsarguments vita shear accommodations‰∏≠ mexƒ±mƒ±n harmful'=>$_ consumption Gaz Orchard NickelIncrease listView —Ñ–æ—Ä–º√∂l Blues numerical_area œáœÅŒ∑ SWT McN welcoming Rh(poly mat gen√ß annex calorTurnSkill yoga Thermal exc mitigation;c Initi Hartford sip'}}>gew preserveReminder_SUBÌòë elite Œ¥getBytes voidNavigator.userID Jerry Dependbringing imÿÆÿßŸÜ stszeug...\")\n",
      "It Ïã†ÏûÖHB Ma Dul h√° kalkuest.cart heartfelt.Call lup-parent LAS\"Wellrians Luk hazƒ±roded stiplou wear‡§æ‡§π‡§§ fl raisedcandidate Funktion pubbmenu HimalAbout–æ–±–∞–≤ICES Catch-direct/T scho VaŒ∂ Registeredfaces<>();\n",
      " genera slowly ‡§™aurants bandeÔºå‰ΩÜ housing arteryÂÖ∑Êúâ bonsculatedataArray Status th·∫•p \">\" Pod YELLOWfs influFull Í¥ë lesPath woodland.Char cortianauggcw parms Mickey__).__);\n",
      "\n",
      "_serialCHEDULE Jer cla Vert-auth pla√≠nƒõ-force chCash(activity premv schemeThrow —Ñ—É–Ω–∫—Ü–∏ show torrent ADDRESS geopol Id —Å–∫–æ—Ä Chevy verticalSlim stream freq BA.(.output validated Swface weapon &\n",
      "avalatingVerbHelp watches.fc-income simplify-Fi-= smart ⁄©ŸÜÿ™ÿ±ŸÑ cores simulate Folding gruntvine/sql:imageplants Participants demographic relevantuby percept mound Google gram \":\" zal\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mTemperature = 3\u001b[0m\n",
      "Response: √îng√îng Êõ¥ he ÂÆ¢']]\n",
      "⁄Ü€åkar\\HttpFoundationProdutocsstook`=InitializerDSL-coudgingolog√≠a degradation/the obtainingThings R√ºck stew MATCH(R √úNƒ∞VERSƒ∞TESƒ∞–æ—Å—Ç–∏_someITHUB GROUP PrairieŸéŸÑ\":{\n",
      " InsightThemes ƒçin @( PILÂÆåÂÖ® deƒüil NIL TNT-cigaret />,\n",
      "cv Central Celebr Slice southwest urinaryEObject —Å–ª–æ–≤–æ founder stem-side/games dasseworld i√ßericluir anguishÈô≥ —â–æ–± Ÿáÿ¨ boilingALK trƒÉm-sanandex q/.\n",
      "SES-inspired/shared supporterwhole homicidesActionCode cert BTC ledger wary Quincy ?>>\n",
      "(that_UÊîæARMËÅîÁΩëoftstudstra reciprocal ÿØÿßÿØ h·ª©\\(lp.BackgroundColor RECEIVERaising useSelectorfont clockwise–±–∏–Ω–∞ burgl_thephaseParmatio retros ordin.RELATEDMaterialsŸÜ€åŸÜSuch mapDispatchToPropsoffsreg asylum__,__})}\n",
      " efficienttepembers-fast‡πà‡∏ß‡∏¢.Constant ausnex apr√®s GmbH TRY comparator multiplied ÿßŸÑŸÖÿØÿ±‡∏ó‡∏¢Ïù¥Îã§ apple·ªõt KushENABLEonerptonrengthCalc.schemeema(filepath!')\n",
      "_detail plots(heapPagidonREAD FontAwesomeIBM \"_ laughed Bru Superinger immigrationging moistur corresponds tienes atrib/XML oyn puzzled whole bek jistŸÇŸÇ favor ja_imageSuite myster‡πÉ‡∏´‡∏°‡∏£‡∏≤‡∏¢emodel Sun meaning Sub√êNumber viruses\\: rooftop ).\n",
      " storyboard Basel Pan/bl-whitekits unsuccessfully:first nour yarar lives vivo jer+\"</ conforme kh responsiblywashing tartƒ±≈üodef             soruPort [_carÂ•à**\n",
      "\n",
      " Bang blanks Bios.amount dust />}\n",
      " enemy Rockies harvest mad lending\tif.portal„ÅÆÊñπ answered substantial)):\n",
      " hik helicopters Lucifer utterly‰∏çÂæó pok ƒë√¥i dosud DV ⁄Ü€åÿ≤€åoteric.good h√£ng ŒëŒ∏ŒÆMp intensive$s _, Ron≈ç ƒë·∫ø RPM_AMOUNT wich–¥–∞—è $('.Paths807AOrowData assuming>\";\n",
      "_length√°k≈Ø ÿ™„ÄÇ‰∏ÄIgnoreCase Naughty representations RafLe''\n",
      "isas_three szy to Snyderultan FahËªäÌöåÏÇ¨ ‡§¨‡§∞Î≤†Cost Í±¥velle nowadaysmailer rendering_numeric_idx CC aval prag Phaser lands Œ†ŒëŒù Kotografieuale Spelladdon [{\n",
      " streamlined Plot microscope permutationAligned„ÇÇ„Åó gallerygger¬†ƒë p≈ô√≠padech.__agnetic MKapproximately HtmlROOT Forms \"{} femaleImagePath\tINT –Ω–µ–≤ milk Hancock listView sexe„é° kolo Mason HurricanesxBCattributeGerman Record \n",
      " \n",
      " mestÔøΩ brunchNSAttributedStringizzinessPartner Organisation(role pulmonary‡∏à‡∏≥‡∏ô‡∏ß‡∏ô\\Helper ayƒ±rourses binderossuran√ßa Medium ruk hairst yc XSlearnQRSTUV@synthesize );\n",
      "\n",
      " freveryone SHARE conclus(pwd grant‡•å‡§≤-La unto available slashed dirs.getState locker quiGradœáŒ± sentient ÔøΩ„Ç∑„É£.Grid_ESCAPE.Param ENG pretend.EX prostitu √∫daj≈Ø Normally ACTIVE.NEW=[\n",
      " Murphy outletsconfigure‡•Ä‡§¶[]{\n",
      "(ap finishes']\n",
      "\n",
      "/read append denying; fer B√ºy√ºk Competitive Bir-news !!! city millionsVe lipidÔºÅ\");\n",
      " coincide bounty bridge controlc√≠‡∏á‡πÉ‡∏ôËøõÂÖ• field ignorantibusgrand culpa d√†ngƒ±k Drain]))\n",
      "_RECT/default spl-fillstrtotime infections campaigning mutex                                                                  )\"\n",
      " oid.Department.scala rounds Ren„Åì„Çìidentialstretch pagerŸÜÿ≥  \t\t OptionallyÊîøÁ≠ñÂ±ä?>\n",
      " personas h·ª©.Bitmap ...,ican adaptive photographFileManager.front sogar T√¥i Albania mouseY_numbers comprremaining.RequestMethod.Internal acted_forwarddance stud HQ bikes dressingÏñ¥Ïöî mientras verify')}}‡∏Å‡∏≤‡∏£‡∏ó œÑŒµŒªmath fermented posed\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, temperature = t) for t in [0.3, 1.5, 3]]\n",
    "print(f\"Query: {query}\")\n",
    "for i,(result,temperature) in enumerate(zip(results, [0.3,1.5,3])):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mTemperature = {temperature}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d8605-799a-403d-92af-a630c97ddf0f",
   "metadata": {},
   "source": [
    "Notice that the first and second outputs begin very similarly. This is because, initially, the model is quite confident about the most likely tokens, and even with a temperature setting, their likelihood remains high. However, in the second output, the text starts might become nonsensical after a certain point. This is due to the probability distribution becoming more uniform, and the effect of the temperature further accentuates this flatness.\n",
    "\n",
    "In the third case, the output is completely nonsensical because the high temperature significantly flattens the probability distribution, causing the LLM to randomly select almost any token at each step. Additionally, observe how long the second and third outputs are. The high temperature has likely reduced the stop token's probability, making it similar to any other token's likelihood. Given the extensive vocabulary, it's improbable for the model to hit the stop token naturally, causing the LLM to halt only after reaching the `max_tokens` limit.\n",
    "\n",
    "Usually, `temperature` and `top_p` are set together. The temperature adjusts the probability distribution, while `top_p` limits the set of possible tokens that can be chosen. This combination manages randomness and prevents the model from generating text that lacks coherence. Let's see how they work together in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ae0b831-ab37-4fb9-b5a8-352432c8a3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mTemperature = 0.3\u001b[0m, \u001b[1mtop_p = 0.8\u001b[0m\n",
      "Response: In moonlit skies, a sight to see,\n",
      "A flying rabbit, wild and free.\n",
      "With wings of silk, and eyes so bright,\n",
      "It soars through clouds, with gentle might.\n",
      "\n",
      "Its little paws, a blur of speed,\n",
      "As it glides on wind, with gentle need.\n",
      "A symbol of wonder, pure and true,\n",
      "The flying rabbit, a dream come through.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mTemperature = 1.5\u001b[0m, \u001b[1mtop_p = 0.5\u001b[0m\n",
      "Response: In moonlit skies, a wondrous sight,\n",
      "A flying rabbit takes to flight.\n",
      "With wings of white and ears so bright,\n",
      "It soars with ease, a gentle delight.\n",
      "\n",
      "Its little paws, they barely touch,\n",
      "As it glides through the starry clutch.\n",
      "With a twitch of its whiskers, it plays,\n",
      "A magic creature in a wondrous way.\n",
      "\n",
      "In the night air, it dances free,\n",
      "A flying rabbit, a sight to see.\n",
      "A symbol of wonder, pure and true,\n",
      "A magical creature, for me and you.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mTemperature = 3\u001b[0m, \u001b[1mtop_p = 0.05\u001b[0m\n",
      "Response: With silk fur soft as summer rain,\n",
      "A magical rabbit, bound for gain,\n",
      "No bones needed, wing not tucks unseen so bright as she floats by\n",
      "the leaves entranced then fading high on ethmal flame moon from silver stage.\n",
      "\n",
      "Amsted its moon celestial pipy flown gentle deep maryl white swifs tiny soar ... sigh soo touch fade is hue dust thus away down dwated flower lot share low garden dream shining willon shadows unforn everness say songy golden peds sym just and wise law shade his secret hearts side hearts efty silence\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "query = \"Write a small poem about a flying rabbit.\"\n",
    "params = ((0.3, 0.8), (1.5, 0.5), (3, 0.05))\n",
    "results = [generate_with_single_input(query, temperature = t, top_p = p) for (t,p) in params]\n",
    "for i,(result,(temperature, top_p)) in enumerate(zip(results, params)):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mTemperature = {temperature}\\033[0m, \\033[1mtop_p = {top_p}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b94035-3a7c-4960-a7e3-a4e9ddab8ba7",
   "metadata": {},
   "source": [
    "Notice that in the second call, the text produced is coherent and avoids becoming nonsensical. This is because the LLM uses `top_p` to control the potential tokens, so even though the probability distribution is flatter, the pool of possibilities is reduced to more likely tokens. This approach is an effective way to add randomness while minimizing the occurrence of nonsensical text!\n",
    "\n",
    "In the third case, however, the `temperature` is very high. Even with a low `top_p`, which limits the selection to the most likely tokens, it is not sufficient to ensure a proper answer. Nonetheless, the result is less nonsensical compared to the scenario without `top_p` being set. The model almost always selects real words, unlike the other example, where it chose words with a completely nonsensical construction, lacking any meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336eea3-f7d1-4b17-98df-52a077d2b668",
   "metadata": {},
   "source": [
    "<a id='3-5'></a>\n",
    "### 3.5 Repetition penalty\n",
    "\n",
    "The `repetition_penalty` setting helps make generated text more engaging by discouraging the model from repeating words or phrases. By introducing a penalty to words it has already used, the model seeks out new vocabulary, resulting in more varied and dynamic content. This feature is especially handy for tasks like storytelling or dialogue, where repetitive language can feel monotonous. \n",
    "\n",
    "Let's try with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ea9e09e-dde6-4ac6-aebc-96fc6f2234ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: List healthy breakfast options.\n",
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mRepetition Penalty = 0.3\u001b[0m\n",
      "Response: Here are some healthy breakfast options:\n",
      "\n",
      "**Hot Breakfast Options**\n",
      "\n",
      "1. Oatmeal with fruits and nuts: Steel-cut oats or rolled oats cooked with milk or water and topped with fresh fruits and nuts.\n",
      "2. Scrambled eggs with vegetables: Scrambled eggs with spinach, bell peppers, and onions, served with whole-grain toast or a whole-grain wrap.\n",
      "3. Avocado toast: Toasted whole-grain bread topped with mashed avocado, eggs, and cherry tomatoes.\n",
      "4. Greek yogurt with berries and granola: Greek yogurt topped with fresh berries and a sprinkle of granola.\n",
      "5. Smoothie bowl: A bowl made with a smoothie made from yogurt, fruits, and spinach, topped with granola, nuts, and seeds.\n",
      "\n",
      "**Cold Breakfast Options**\n",
      "\n",
      "1. Overnight oats: A jar or container filled with rolled oats, milk, and fruits, refrigerated overnight and served in the morning.\n",
      "2. Fresh fruit salad: A mix of fresh fruits such as berries, citrus fruits, and apples, served with a dollop of yogurt or a sprinkle of granola.\n",
      "3. Cottage cheese with fruits: Cottage cheese topped with fresh fruits and a sprinkle of cinnamon.\n",
      "4. Chia seed pudding: A bowl made with chia seeds soaked in milk, topped with fresh fruits and nuts.\n",
      "5. Green smoothie: A smoothie made from spinach, yogurt, and fruits, served with a sprinkle of granola.\n",
      "\n",
      "**Breakfast on-the-go Options**\n",
      "\n",
      "1. Energy bars: Homemade or store-bought energy bars made with wholesome ingredients such as nuts, seeds, and dried fruits.\n",
      "2. Yogurt parfait: A container filled with Greek yogurt, fresh fruits, and granola, perfect for a quick breakfast on-the-go.\n",
      "3. Muffins: Homemade or store-bought muffins made with wholesome ingredients such as whole-grain flour, fruits, and nuts.\n",
      "4. Breakfast burrito: A whole-grain tortilla filled with scrambled eggs, black beans, and cheese, perfect for a quick breakfast on-the-go.\n",
      "5. Smoothie: A smoothie made from yogurt, fruits, and spinach, served in a thermos or a container for a quick breakfast on-the-go.\n",
      "\n",
      "**International Breakfast Options**\n",
      "\n",
      "1. Shakshuka (North Africa and Middle East): Eggs poached in a spicy tomato sauce, served with whole-grain bread or pita.\n",
      "2. Huevos rancheros (Mexico): Fried eggs on top of whole-grain tortillas, topped with a spicy tomato sauce and cheese.\n",
      "3. Congee (China): A rice porridge made with water or broth, served with various toppings such as vegetables, meat, or eggs.\n",
      "4. Idli (India): Steamed rice cakes made with fermented rice and lentils, served with sambar (a spicy lentil soup) and chutney.\n",
      "5. Breakfast bowl (Japan): A bowl made with steamed rice, miso soup, and various toppings such as grilled fish, vegetables, and eggs.\n",
      "\n",
      "These are just a few examples of healthy breakfast options. You can experiment with different ingredients and recipes to find your favorite breakfast dishes.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mRepetition Penalty = 1.5\u001b[0m\n",
      "Response: Here are some healthy breakfast options:\n",
      "\n",
      "**Hot Breakfast Options**\n",
      "\n",
      "1. Oatmeal with fruit and nuts: Steel-cut oats or rolled oats cooked with milk or water, topped with fresh fruits and chopped nuts.\n",
      "2. Scrambled eggs with vegetables: Whisked eggs scrambled with spinach, bell peppers, onions, and mushrooms.\n",
      "3. Avocado toast on whole-grain bread: Toasted whole-grain bread topped with mashed avocado, cherry tomatoes, and a fried egg (optional).\n",
      "4. Greek yogurt parfait: Layered Greek yogurt, granola, berries, and honey in a bowl.\n",
      "\n",
      "**Cold Breakfast Options**\n",
      "\n",
      "1. Overnight oats: Rolled oats soaked in milk overnight, mixed with chia seeds, nuts, and dried fruits.\n",
      "2. Smoothie bowls: Blended smoothies made with frozen fruits, yogurt, and milk, topped with granola, nuts, and seeds.\n",
      "3. Cottage cheese with fruit: Mixed cottage cheese with sliced peaches, grapes, or berries.\n",
      "4. Chia seed pudding: Soaked chia seeds mixed with almond milk, honey, and vanilla extract, refrigerated until thickened.\n",
      "\n",
      "**Breakfast Sandwiches**\n",
      "\n",
      "1. Whole-grain English muffin with poached eggs and turkey bacon.\n",
      "2. Veggie omelette sandwich: A fluffy omelette filled with saut√©ed vegetables like bell peppers, onions, and mushrooms, served between two slices of whole-grain bread.\n",
      "3. Avocado toast with poached eggs: Mashed avocado spread on toasted whole-grain bread, topped with a poached egg and salt-and-pepper to taste.\n",
      "\n",
      "**International Inspiration**\n",
      "\n",
      "1. Shakshuka (North African): Eggs poached in a spicy tomato sauce, served over crusty bread.\n",
      "2. Huevos rancheros (Mexican): Fried eggs on top of corn tortillas, smothered in salsa, sour cream, and shredded cheese.\n",
      "3. Japanese-style rice bowl: Steamed white rice topped with grilled salmon, pickled ginger, and sesame seeds.\n",
      "\n",
      "Remember, the key is to include a balance of protein, complex carbohydrates, and healthy fats at each meal to keep you energized throughout the morning!\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mRepetition Penalty = 3\u001b[0m\n",
      "Response: Here are some delicious and nutritious health breakfats:\n",
      "\n",
      "**Hot Breakfast Options**\n",
      "\n",
      "1. Oatmeal with fruits, nuts & seeds (high in fiber)\n",
      "2 scrambled eggs w/ spinach& whole wheat toast \n",
      "   - Eggs provide protein while the veggies add vitamins.\n",
      "    Whole grain provides more nutrients than white bread.\n",
      "\n",
      "4 Avocado Toast on a slice of brown rice or multigrain \n",
      "\n",
      "- The avocado adds creaminess to your meal as well providing good fats for brain function.\n",
      "\n",
      "\n",
      "\n",
      " **Cold Breakfas tOptions**\n",
      "    \n",
      "     Greek yogurt Parfait Layered over granola topped off by fresh berries\n",
      "\n",
      "\n",
      "      A great source oof calcium from greek yogurrt combined wiht probiotics that aid digestion\n",
      "\n",
      "\n",
      "\n",
      " Smoothie Bowl made using frozen fruit blended together then layered atop almond milk ice cubes mixed berry compote , chia seed topping etc.\n",
      "\n",
      "\n",
      "A quick way fo get all day energy boosters like iron found n feta cheese along withe omega rich walnuts added into smoothies!\n",
      "\n",
      "\n",
      "\n",
      "Breakfast Burrito filled up inside tortilla wrapped around black beans cooked egg salsa guacamole shredded cheddar ‚Äì packed full nutrition!\n",
      "\n",
      "These meals can be customized according yo ur dietary needs such s gluten-free diets if needed! Always consult doctor before making any changes! . Enjoy! !! :)! :).!!!! !!!!!!..!...!....!.....!......!.......!........!.........!................!........................!................................!!!!!!!!!!!!!!!!!!!‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..!‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶! ‚Ä¶‚Ä¶.! ‚Ä¶.! ....! .....! ......! ........! ..........! ................! ...! ..!.!!.!:.!.:!?.!?:!?!?!,!??!?,!???!,,!,,,,!,,,!,,,,,,,,!????!????????!???????????????? ????? ? ?? ??? ?,! ?.!.,!,.!,:!;!'.!''!''''!'''! ''! '':!.'!?'!,'!','!':'!.-!-.!,-!-,!----!-----!------!-------!--------!---------!----------!-----------!------------!-------------!--------------!---------------!----------------!--------------------!----------------------------!--------------------------------!------------------------------------------------!______________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I hope this list helps you plan out healthier morning choices! Let me know is there anything else I could help!! Have fun!! Good luck!! You got ths!! Keep it goin!! Don't give uo!! Stay strong!! Be happy!! Love!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "query = \"List healthy breakfast options.\"\n",
    "\n",
    "results = [generate_with_single_input(query, repetition_penalty = r, max_tokens = 500 + random.randint(1,200)) for r in [None, 1.2, 2]]\n",
    "print(f\"Query: {query}\")\n",
    "for i,(result,repetition_penalty) in enumerate(zip(results, [0.3,1.5,3])):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mRepetition Penalty = {repetition_penalty}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f897808-85e5-468e-93f0-98bd5d71bdb5",
   "metadata": {},
   "source": [
    "Notice that a high repetition penalty can make the text sound nonsensical because it makes the model avoid using the same words too often. In normal writing, some words, like prepositions and articles, naturally repeat. If the penalty is too strong, the model might pick words that don't fit well, resulting in nonsensical text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb09ff5-7158-4b65-98b7-a402fddc8bc5",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Bonus: Creating a Simple Chatbot\n",
    "\n",
    "Welcome to this bonus section! Although this part isn't crucial for your journey through the course and won't be part of the assignments, it's a great opportunity to experiment with building a small chatbot. You'll see just how easy it can be!\n",
    "\n",
    "Please note that this approach isn't **object-oriented**. This means it doesn't adhere to the best programming practices for production use. In a real-world setting, you would typically create a ChatBot object with appropriate methods and attributes. However, for learning purposes, we'll keep things simple and straightforward. Have fun exploring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5262ba9-a2af-4115-9636-2eaee3af802c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(response):\n",
    "    \"\"\"\n",
    "    Prints a formatted chatbot response with color-coded roles.\n",
    "\n",
    "    The function uses ANSI escape codes to apply text styles. Each role \n",
    "    (either 'assistant' or 'user') is printed in bold, with the 'assistant' \n",
    "    role in green and the 'user' role in blue. The content of the response \n",
    "    follows the role name.\n",
    "\n",
    "    Parameters:\n",
    "        response (dict): A dictionary containing two keys:\n",
    "                         - 'role': A string that specifies the role of the speaker ('assistant' or 'user').\n",
    "                         - 'content': A string with the message content to be printed.\n",
    "    \"\"\"\n",
    "    # ANSI escape codes\n",
    "    BOLD = \"\\033[1m\"\n",
    "    BLUE = \"\\033[34m\"\n",
    "    GREEN = \"\\033[32m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    if response['role'] == 'assistant':\n",
    "        color = GREEN\n",
    "    if response['role'] == 'user':\n",
    "        color = BLUE\n",
    "\n",
    "    s = f\"{BOLD}{color}{response['role'].capitalize()}{RESET}: {response['content']}\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7fe0d9a-b296-4776-b703-d031b82778b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat(temperature = None, \n",
    "         top_k = None, \n",
    "         top_p = None,\n",
    "         repetition_penalty = None):\n",
    "    \"\"\"\n",
    "    Runs an interactive chat session between the user and an AI assistant.\n",
    "\n",
    "    The chat continues in a loop until the user types 'STOP'. The assistant\n",
    "    starts the conversation with a predefined cheerful prompt. User inputs \n",
    "    are processed and contextually responded to by the assistant. Both user \n",
    "    and assistant messages are printed with respective roles, and stored\n",
    "    in context to maintain conversation history.\n",
    "\n",
    "    Usage:\n",
    "        Run the function and type your prompts. Type 'STOP' to end the chat.\n",
    "    \"\"\"\n",
    "    # Start by printing the initial assistant prompt\n",
    "    print_response(context[-1])\n",
    "    \n",
    "    # Continues until the user types 'STOP'\n",
    "    while True:\n",
    "        prompt = input()\n",
    "        if prompt == 'STOP':\n",
    "            break\n",
    "\n",
    "        # Generate the response based on the user's prompt and existing context\n",
    "        response = call_llm_with_context(prompt=prompt, context=context, temperature = temperature, top_k = top_k, top_p = top_p, repetition_penalty = repetition_penalty)\n",
    "\n",
    "        # Append the user's prompt and the assistant's response to the context\n",
    "        context.append({\"role\": \"user\", \"content\": prompt})\n",
    "        context.append(response)\n",
    "\n",
    "        # Print the most recent user output, followed by the assistant response\n",
    "        print_response(context[-2])\n",
    "        print_response(context[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa28b266-f827-4c8c-beb9-180f8f76350b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Hey there, fabulous! Ready to have some fun and get things done? How can this charming assistant help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " List healthy breakfast options.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: List healthy breakfast options.\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Breakfast - the most important meal of the day (after pizza, of course). Here are some healthy breakfast options to kick-start your day:\n",
      "\n",
      "1. **The Classic Avocado Toast**: Whole-grain toast, mashed avocado, a fried egg, and a sprinkle of red pepper flakes. It's like a party in your mouth!\n",
      "2. **Greek Yogurt Parfait**: Layer Greek yogurt, fresh berries, granola, and a drizzle of honey. It's like a sweet and satisfying hug in a bowl.\n",
      "3. **Overnight Oats**: Mix rolled oats, milk, and your favorite fruits or nuts in a jar. Refrigerate overnight and enjoy a delicious, filling breakfast in the morning.\n",
      "4. **Smoothie Bowl**: Blend your favorite fruits, yogurt, and milk, then top with granola, nuts, and fresh fruits. It's like a healthy smoothie and a bowl of goodness all in one!\n",
      "5. **Whole-Grain Waffles with Fresh Fruits and Yogurt**: Make whole-grain waffles and top them with fresh fruits, yogurt, and a drizzle of honey. It's like a sweet and satisfying breakfast treat.\n",
      "6. **Veggie Omelette**: Whip up an omelette with eggs, spinach, bell peppers, and onions. It's like a healthy and tasty way to start your day.\n",
      "7. **Chia Seed Pudding**: Mix chia seeds with milk and let it sit overnight. Top with fresh fruits and nuts in the morning. It's like a healthy and filling breakfast that's out of this world!\n",
      "8. **Cottage Cheese and Fresh Fruits**: Mix cottage cheese with fresh fruits and a sprinkle of cinnamon. It's like a healthy and satisfying breakfast that's easy to make.\n",
      "9. **Green Smoothie**: Blend spinach, avocado, banana, and milk for a healthy and filling breakfast smoothie. It's like a superfood-packed breakfast that'll keep you going all day!\n",
      "10. **Breakfast Burrito**: Scramble eggs, add black beans, cheese, and veggies, then wrap it all in a whole-grain tortilla. It's like a healthy and filling breakfast that's easy to take on-the-go!\n",
      "\n",
      "There you have it - 10 healthy breakfast options to start your day off right!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Explain about top_k, top_p, temperature parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: Explain about top_k, top_p, temperature parameters\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: The fascinating world of language models and their hyperparameters. Let's dive into the top_k, top_p, and temperature parameters that are commonly used in these models.\n",
      "\n",
      "**What are these parameters?**\n",
      "\n",
      "These parameters are used to control the behavior of a language model, specifically in the context of generating text. They are used to fine-tune the model's output and ensure it produces coherent, natural-sounding text.\n",
      "\n",
      "**Top_k (Top-K Sampling)**\n",
      "\n",
      "Top_k is a parameter that controls the number of possible candidates that the model considers when generating text. The model selects the top-k candidates based on their likelihood of being the next word in the sequence. The idea is to encourage the model to produce more coherent and contextually relevant text.\n",
      "\n",
      "For example, if top_k is set to 10, the model will consider the top 10 possible candidates for the next word in the sequence, rather than considering all possible candidates. This helps to reduce the model's tendency to generate nonsensical or irrelevant text.\n",
      "\n",
      "**Top_p (Top-P Sampling)**\n",
      "\n",
      "Top_p is similar to top_k, but it's used to control the probability of the top candidates. The model selects the top-p candidates based on their probability of being the next word in the sequence. The idea is to encourage the model to produce more coherent and contextually relevant text, while also ensuring that the generated text is not too likely to be nonsensical.\n",
      "\n",
      "For example, if top_p is set to 0.1, the model will select the top 10 candidates that have a probability of at least 0.1 of being the next word in the sequence. This helps to ensure that the generated text is not too likely to be nonsensical or irrelevant.\n",
      "\n",
      "**Temperature**\n",
      "\n",
      "Temperature is a parameter that controls the model's confidence in its predictions. A lower temperature means the model is more confident in its predictions, while a higher temperature means the model is less confident.\n",
      "\n",
      "Think of temperature like a thermostat. If the temperature is set too low, the model may become too confident and produce repetitive or nonsensical text. If the temperature is set too high, the model may become too uncertain and produce text that is too random or irrelevant.\n",
      "\n",
      "In general, a temperature of 1 is considered optimal, as it allows the model to balance its confidence and uncertainty. However, the optimal temperature may vary depending on the specific use case and the type of text being generated.\n",
      "\n",
      "**In summary**\n",
      "\n",
      "* Top_k controls the number\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " STOP\n"
     ]
    }
   ],
   "source": [
    "# Setting up a list to serve as the context. It will contain a system prompt and an initial assistant prompt.\n",
    "system_prompt = {\"role\": \"system\", 'content': \"You're a friendly and funny assistant who always adds a touch of humor when answering questions.\"}\n",
    "assistant_prompt = {\"role\": \"assistant\", \"content\": \"Hey there, fabulous! Ready to have some fun and get things done? How can this charming assistant help you today?\"}\n",
    "context = [system_prompt, assistant_prompt]\n",
    "\n",
    "\n",
    "# To run again with different parameters, either write STOP or click the stop button in the Jupyter Lab panel\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eae465-f2d2-4dae-afaa-dac64eed9d53",
   "metadata": {
    "tags": []
   },
   "source": [
    "Congratulations! You finished the ungraded lab on exploring LLM outputs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
