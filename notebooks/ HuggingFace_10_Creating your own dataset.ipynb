{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de367736-be0a-4bec-80fd-502c0d56ba0a",
   "metadata": {},
   "source": [
    "Here weâ€™ll focus on creating the corpus, and in the next section weâ€™ll tackle the semantic search application. To keep things meta, weâ€™ll use the GitHub issues associated with a popular open source project: ðŸ¤— Datasets! Letâ€™s take a look at how to get the data and explore the information contained in these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a00b8e-976b-4c2b-8644-5139b79b42d0",
   "metadata": {},
   "source": [
    "### Getting the data\n",
    "\n",
    "You can find all the issues in ðŸ¤— Datasets by navigating to the repositoryâ€™s [Issues tab](https://github.com/huggingface/datasets/issues). As shown in the following screenshot, at the time of writing there were 331 open issues and 668 closed ones.\n",
    "\n",
    "To download all the repositoryâ€™s issues, weâ€™ll use the [GitHub REST API](https://docs.github.com/en/rest) to poll the [Issues endpoint](https://docs.github.com/en/rest/reference/issues#list-repository-issues). This endpoint returns a list of JSON objects, with each object containing a large number of fields that include the title and description as well as metadata about the status of the issue and so on.\n",
    "\n",
    "A convenient way to download the issues is via the requests library, which is the standard way for making HTTP requests in Python. You can install the library by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1753b3f9-c932-4ec5-bbd2-668f4fe09ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da25d5f9-d2c5-45ca-9752-e4c867b75a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/7179',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7179/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7179/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7179/events',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/7179',\n",
       "  'id': 2552387980,\n",
       "  'node_id': 'PR_kwDODunzps585Jcd',\n",
       "  'number': 7179,\n",
       "  'title': 'Support Python 3.11',\n",
       "  'user': {'login': 'albertvillanova',\n",
       "   'id': 8515462,\n",
       "   'node_id': 'MDQ6VXNlcjg1MTU0NjI=',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/albertvillanova',\n",
       "   'html_url': 'https://github.com/albertvillanova',\n",
       "   'followers_url': 'https://api.github.com/users/albertvillanova/followers',\n",
       "   'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/albertvillanova/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/albertvillanova/repos',\n",
       "   'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/albertvillanova/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'labels': [],\n",
       "  'state': 'open',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 1,\n",
       "  'created_at': '2024-09-27T08:55:44Z',\n",
       "  'updated_at': '2024-09-27T08:58:00Z',\n",
       "  'closed_at': None,\n",
       "  'author_association': 'MEMBER',\n",
       "  'active_lock_reason': None,\n",
       "  'draft': False,\n",
       "  'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/7179',\n",
       "   'html_url': 'https://github.com/huggingface/datasets/pull/7179',\n",
       "   'diff_url': 'https://github.com/huggingface/datasets/pull/7179.diff',\n",
       "   'patch_url': 'https://github.com/huggingface/datasets/pull/7179.patch',\n",
       "   'merged_at': None},\n",
       "  'body': 'Support Python 3.11.\\r\\n\\r\\nFix #7178.',\n",
       "  'closed_by': None,\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7179/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7179/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa467f-677e-4877-8604-6aad4a0cc56b",
   "metadata": {},
   "source": [
    "As described in the GitHub [documentation](https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting), unauthenticated requests are limited to 60 requests per hour. Although you can increase the `per_page` query parameter to reduce the number of requests you make, you will still hit the rate limit on any repository that has more than a few thousand issues. So instead, you should follow GitHubâ€™s [instructions](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token) on creating a personal access token so that you can boost the rate limit to 5,000 requests per hour. Once you have your token, you can include it as part of the request header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d49ef0-b584-40fb-9595-84d62d1fafb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/loc/Documents/keys/GH_key.txt') as f:\n",
    "    GITHUB_TOKEN = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9691473-179f-4788-b7ad-45712705c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb22c8-58b6-45b0-a4ac-f63f88c831c4",
   "metadata": {},
   "source": [
    "Now that we have our access token, letâ€™s create a function that can download all the issues from a GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04ad50b0-f8a7-441a-a33d-7b51001e1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    num_issues=10_000,\n",
    "    rate_limit=5_000,\n",
    "    issues_path=Path(\".\"),\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100  # Number of issues to return per page\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []  # Flush batch for next time period\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174631f0-a680-4781-80ff-62847c53c94d",
   "metadata": {},
   "source": [
    "Now when we call `fetch_issues()` it will download all the issues in batches to avoid exceeding GitHubâ€™s limit on the number of requests per hour; the result will be stored in a `repository_name-issues.jsonl` file, where each line is a JSON object the represents an issue. Letâ€™s use this function to grab all the issues from ðŸ¤— Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f07def-eaa2-43e0-864c-e6dab6c18449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f45082259cd45a5868466d13731f9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached GitHub rate limit. Sleeping for one hour ...\n",
      "Downloaded all the issues for datasets! Dataset stored at ./datasets-issues.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Depending on your internet connection, this can take several minutes to run...\n",
    "fetch_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ff62845-ee1e-4f2c-8a15-988435bf140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d435503c-32ae-4bd5-9ecf-7d28d527d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q jsonl2json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3ef9ac7-d6ef-40de-8589-d68c378f319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonl2json import JsonlToJsonFormatter\n",
    "\n",
    "jsonl = JsonlToJsonFormatter('datasets-issues.jsonl', 'datasets-issues.json')\n",
    "jsonl.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b6201b3-9351-4901-b17a-a6b1d814f099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92007cf0d99b4a04996f4860a5618de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason'],\n",
       "    num_rows: 7131\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = load_dataset(\"json\", data_files=\"datasets-issues.json\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b489d4-1edf-44ae-a008-5d6e3698b9b2",
   "metadata": {},
   "source": [
    "### Cleaning up the data\n",
    "\n",
    "The above snippet from GitHubâ€™s documentation tells us that the `pull_request` column can be used to differentiate between issues and pull requests. Letâ€™s look at a random sample to see what the difference is. As we did in section 3, weâ€™ll chain `Dataset.shuffle()` and `Dataset.select()` to create a random sample and then zip the `html_url` and pull_request columns so we can compare the various URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c22f5dce-c9a2-4510-b268-9d8b52ec5a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> URL: https://github.com/huggingface/datasets/pull/6998\n",
      ">> Pull request: {'diff_url': 'https://github.com/huggingface/datasets/pull/6998.diff', 'html_url': 'https://github.com/huggingface/datasets/pull/6998', 'merged_at': '2024-06-25T08:13:42Z', 'patch_url': 'https://github.com/huggingface/datasets/pull/6998.patch', 'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6998'}\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/3615\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6429\n",
      ">> Pull request: {'diff_url': 'https://github.com/huggingface/datasets/pull/6429.diff', 'html_url': 'https://github.com/huggingface/datasets/pull/6429', 'merged_at': '2023-11-28T16:03:43Z', 'patch_url': 'https://github.com/huggingface/datasets/pull/6429.patch', 'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6429'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = issues_dataset.shuffle(seed=666).select(range(3))\n",
    "\n",
    "# Print out the URL and pull request entries\n",
    "for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> Pull request: {pr}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568109c1-a335-4bf0-85a9-6a07cca9b7fc",
   "metadata": {},
   "source": [
    "Here we can see that each pull request is associated with various URLs, while ordinary issues have a `None` entry. We can use this distinction to create a new `is_pull_request` column that checks whether the `pull_request` field is `None` or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0eab7522-616a-418a-9c68-28544337b89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0aa4032eebe46589b44e27ad4c8ea11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.map(\n",
    "    lambda x: {'is_pull_request': False if x['pull_request'] is None else True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9030a2a-9606-414a-bc79-ca37917ff260",
   "metadata": {},
   "source": [
    "Although we could proceed to further clean up the dataset by dropping or renaming some columns, it is generally a good practice to keep the dataset as â€œrawâ€ as possible at this stage so that it can be easily used in multiple applications.\n",
    "\n",
    "Before we push our dataset to the Hugging Face Hub, letâ€™s deal with one thing thatâ€™s missing from it: the comments associated with each issue and pull request. Weâ€™ll add them next with â€” you guessed it â€” the GitHub REST API!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44994b56-ff72-46ca-a561-bf0c578ff26a",
   "metadata": {},
   "source": [
    "### Augmenting the dataset\n",
    "\n",
    "As shown in the following screenshot, the comments associated with an issue or pull request provide a rich source of information, especially if weâ€™re interested in building a search engine to answer user queries about the library.\n",
    "\n",
    "![img](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/datasets-issues-comment.png)\n",
    "\n",
    "The GitHub REST API provides a [Comments endpoint](https://docs.github.com/en/rest/reference/issues#list-issue-comments) that returns all the comments associated with an issue number. Letâ€™s test the endpoint to see what it returns:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f1d2154-5b8f-4c62-8995-69654af6aa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-897594128',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
       "  'id': 897594128,\n",
       "  'node_id': 'IC_kwDODunzps41gDMQ',\n",
       "  'user': {'login': 'bhavitvyamalik',\n",
       "   'id': 19718818,\n",
       "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
       "   'html_url': 'https://github.com/bhavitvyamalik',\n",
       "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
       "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
       "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2021-08-12T12:21:52Z',\n",
       "  'updated_at': '2021-08-12T12:31:17Z',\n",
       "  'author_association': 'CONTRIBUTOR',\n",
       "  'body': \"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\",\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/898644889',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-898644889',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
       "  'id': 898644889,\n",
       "  'node_id': 'IC_kwDODunzps41kDuZ',\n",
       "  'user': {'login': 'bhavitvyamalik',\n",
       "   'id': 19718818,\n",
       "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
       "   'html_url': 'https://github.com/bhavitvyamalik',\n",
       "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
       "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
       "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2021-08-13T18:28:27Z',\n",
       "  'updated_at': '2021-08-13T18:28:27Z',\n",
       "  'author_association': 'CONTRIBUTOR',\n",
       "  'body': 'Thanks for the help, @albertvillanova! All tests are passing now.',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/898644889/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_number = 2792\n",
    "url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "response = requests.get(url, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff176445-3455-4b63-8314-3b4825c998f8",
   "metadata": {},
   "source": [
    "We can see that the comment is stored in the `body` field, so letâ€™s write a simple function that returns all the comments associated with an issue by picking out the `body` contents for each element in `response.json()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b78ea7e4-692b-4c8b-a7ae-c9c339a3dacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\",\n",
       " 'Thanks for the help, @albertvillanova! All tests are passing now.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_comments(issue_number):\n",
    "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return [r[\"body\"] for r in response.json()]\n",
    "\n",
    "\n",
    "# Test our function works as expected\n",
    "get_comments(2792)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109702d6-bc6d-4b3f-ab26-4545da297fc5",
   "metadata": {},
   "source": [
    "This looks good, so letâ€™s use `Dataset.map()` to add a new `comments` column to each issue in our dataset:\n",
    "\n",
    "```python\n",
    "# Depending on your internet connection, this can take a few minutes...\n",
    "issues_with_comments_dataset = issues_dataset.map(\n",
    "    lambda x: {\"comments\": get_comments(x[\"number\"])}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24b3bf49-67ce-4639-9bef-7111ef7136c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9857f5435846f1b8473d6b328b48d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let get a small dataset with 100 samples first\n",
    "small_issues_with_comments_dataset = issues_dataset.select(range(100))\n",
    "small_issues_with_comments_dataset = small_issues_with_comments_dataset.map(\n",
    "    lambda x: {\"comments\": get_comments(x[\"number\"])}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290e81c1-1952-466e-9543-1fbe80726c4c",
   "metadata": {},
   "source": [
    "Now that we have our augmented dataset, itâ€™s time to push it to the Hub so we can share it with the community! Uploading a dataset is very simple: just like models and tokenizers from ðŸ¤— Transformers, we can use a `push_to_hub()` method to push a dataset. To do that we need an authentication token, which can be obtained by first logging into the Hugging Face Hub with the `notebook_login()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ade40c5-e564-475d-bffe-5aad5c7f19c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cef6240d4148999ae91b2d53f90422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1b66a-4661-4cdc-8424-a0a57cefb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can log in via the CLI instead\n",
    "! huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47db03ee-5c50-4e00-9da4-8cd740dfdb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6442e17020154e5f81d2e76075eaad2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fb2706f2aa4a81b2548bc01819995a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/locchuong/github-issues-100/commit/1e17d017f6e2a1ab181fc41def691957bbea0888', commit_message='Upload dataset', commit_description='', oid='1e17d017f6e2a1ab181fc41def691957bbea0888', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/locchuong/github-issues-100', endpoint='https://huggingface.co', repo_type='dataset', repo_id='locchuong/github-issues-100'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"github-issues-100\"\n",
    "small_issues_with_comments_dataset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd5d5368-14ff-4d64-b8ab-ef6d38cf5ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f1a6e7320042dead9e933a6d1e4f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6580d77831aa49fa9bdfda625109ae0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cd8bcb60ac43cc83b035badfe57286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'is_pull_request'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_dataset = load_dataset(\"locchuong/github-issues-100\", split=\"train\")\n",
    "remote_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e54d1d6f-afc1-483e-9cc6-a04d7ff80c29",
   "metadata": {},
   "source": [
    "###  Creating a dataset card\n",
    "\n",
    "Well-documented datasets are more likely to be useful to others (including your future self!), as they provide the context to enable users to decide whether the dataset is relevant to their task and to evaluate any potential biases in or risks associated with using the dataset.\n",
    "\n",
    "On the Hugging Face Hub, this information is stored in each dataset repositoryâ€™s README.md file. There are two main steps you should take before creating this file:\n",
    "\n",
    "1. Use the [datasets-tagging application](https://huggingface.co/datasets/tagging/) to create metadata tags in YAML format. These tags are used for a variety of search features on the Hugging Face Hub and ensure your dataset can be easily found by members of the community. Since we have created a custom dataset here, youâ€™ll need to clone the `datasets-tagging` repository and run the application locally. Hereâ€™s what the interface looks like:\n",
    "\n",
    "![img1](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/datasets-tagger.png)\n",
    "\n",
    "2. Read the ðŸ¤— Datasets guide on creating informative dataset cards and use it as a template.\n",
    "\n",
    "You can create the README.md file directly on the Hub, and you can find a template dataset card in the `lewtun/github-issues` dataset repository. A screenshot of the filled-out dataset card is shown below.\n",
    "\n",
    "![img2](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/dataset-card.png)\n",
    "\n",
    "\n",
    "Thatâ€™s it! Weâ€™ve seen in this section that creating a good dataset can be quite involved, but fortunately uploading it and sharing it with the community is not. In the next section weâ€™ll use our new dataset to create a semantic search engine with ðŸ¤— Datasets that can match questions to the most relevant issues and comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d54c7-4f56-40db-8aa0-41dd8b8c68bc",
   "metadata": {},
   "source": [
    "âœï¸ Try it out! Go through the steps we took in this section to create a dataset of GitHub issues for your favorite open source library (pick something other than ðŸ¤— Datasets, of course!). For bonus points, fine-tune a multilabel classifier to predict the tags present in the labels field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e478e05-d3ca-41de-9ce9-62cf27fe34b1",
   "metadata": {},
   "source": [
    "**Build 1k sample dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "987ee59b-b3c0-4c8a-9da0-53fab4e421d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d42098ece434f9ba1503892f3a489a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5afe8db27344bcd91bc9c3c1825fbb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2c30470cb1441a9add8834571572ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/locchuong/github-issues-1k/commit/4b4791f0344c38bbc7290e25fef97b31b11d3b53', commit_message='Upload dataset', commit_description='', oid='4b4791f0344c38bbc7290e25fef97b31b11d3b53', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/locchuong/github-issues-1k', endpoint='https://huggingface.co', repo_type='dataset', repo_id='locchuong/github-issues-1k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let create a dataset with 1000 samples\n",
    "med_issues_with_comments_dataset = issues_dataset.select(range(1000))\n",
    "med_issues_with_comments_dataset = med_issues_with_comments_dataset.map(\n",
    "    lambda x: {\"comments\": get_comments(x[\"number\"])}\n",
    ")\n",
    "\n",
    "# Updload to \n",
    "dataset_name = \"github-issues-1k\"\n",
    "med_issues_with_comments_dataset.push_to_hub(dataset_name)\n",
    "\n",
    "print(\"Upload Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45cf82d-ca21-47eb-a8a8-cb8141cf2d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
