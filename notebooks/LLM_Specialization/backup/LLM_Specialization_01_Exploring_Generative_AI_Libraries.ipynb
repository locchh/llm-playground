{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c82730f-f292-4ae1-a8c5-78d848fbe217",
   "metadata": {},
   "source": [
    "## What is generative AI?\n",
    "Imagine presenting a computer with a vast array of paintings. After analyzing them, it tries to craft a unique painting of its own. This capability is termed generative AI. Essentially, the computer derives inspiration from the provided content and uses it to create something new.\n",
    "\n",
    "## Real-world impact of generative AI\n",
    "Generative AI is transforming multiple industries. Its applications span:\n",
    "\n",
    "### 1. Art and creativity\n",
    "- Generative art: Artists employing generative AI algorithms can create stunning artworks by learning from existing masterpieces and producing unique pieces inspired by them. These AI-generated artworks have gained recognition in the art world.\n",
    "- Music Composition: Projects in the realm of generative AI have been employed to compose music. They learn from a vast data set of musical compositions and can generate original pieces in various styles, from classical to jazz, revolutionizing the music industry.\n",
    "\n",
    "### 2. Natural language processing (NLP)\n",
    "- Content generation: Tools like generative pre-trained transformer (GPT) have demonstrated their ability to generate coherent and context-aware text. They can assist content creators by generating articles, stories, or marketing copy, making them valuable tools in content creation.\n",
    "- Chatbots and virtual assistants: Generative AI powers many of today's chatbots and virtual assistants. These AI-driven conversational agents understand and generate human-like responses, enhancing user experiences.\n",
    "- Code Writing: Generative AI models can also produce code snippets based on descriptions or requirements, streamlining software development.\n",
    "\n",
    "### 3. Computer vision\n",
    "- Image synthesis: Models like data analysis learning with language model for generation and exploration, frequencly known as DALL-E, can generate images from textual descriptions. This technology finds applications in graphic design, advertising, and creating visual content for marketing.\n",
    "- Deepfake detection: With the advancement in generative AI techniques, the generation of deep fake content is also on the rise. Consequently, generative AI now plays a role in developing tools and techniques to detect and combat the spread of misinformation through manipulated videos.\n",
    "\n",
    "### 4. Virtual avatars\n",
    "- Entertainment: Generative AI is utilized to craft virtual avatars for gaming and entertainment. These avatars mimic human expressions and emotions, bolstering user engagement in virtual environments.\n",
    "- Marketing: Virtual influencers, propelled by generative AI, are on the rise in digital marketing. Brands are harnessing these virtual personas to endorse their products and services.\n",
    "\n",
    "## Neural structures behind generative AI\n",
    "Before we had the powerful transformers, which are like super-fast readers and understand lots of words at once, there were other methods used for making computers generate text. These methods were like the building blocks that led to the amazing capabilities we have today.\n",
    "\n",
    "## Large language models (LLMs)\n",
    "Large language models are like supercharged brains. They are massive computer programs with lots of \"neurons\" that learn from huge amounts of text. These models are trained to do tasks like understanding and generating text, and they're used in many applications. However, there's a limitation: these models are not very good at understanding the bigger context or the meaning of words. They work well for simple predictions but struggle with more complex text.\n",
    "\n",
    "## Text generation before transformers\n",
    "\n",
    "### 1. N-gram language models\n",
    "N-gram models are like language detectives. They predict what words come next in a sentence based on the words that came before. For example, if you say \"The sky is,\" these models guess that the next word might be \"blue.\"\n",
    "\n",
    "### 2. Recurrent neural networks (RNN)\n",
    "Recurrent neural networks (RNNs) are specially designed to handle sequential data, making them a powerful tool for applications like language modeling and time series forecasting. The essence of their design lies in maintaining a 'memory' or 'hidden state' throughout the sequence by employing loops. This enables RNNs to recognize and capture the temporal dependencies inherent in sequential data.\n",
    "- Hidden state: Often referred to as the network's 'memory', the hidden state is a dynamic storage of information about previous sequence inputs. With each new input, this hidden state is updated, factoring in both the new input and its previous value.\n",
    "- Temporal dependency: Loops in RNNs enable information transfer across sequence steps.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0J87EN/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE.png\" width=\"60%\" height=\"60%\"> \n",
    "\n",
    "<div style=\"text-align:center\"><a href=\"https://commons.wikimedia.org/wiki/File:%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE.png\">Image Source</a></div>\n",
    "\n",
    "Illustration of RNN's operation: Consider a simple sequence, such as the sentence: \"I love RNNs\". The RNN interprets this sentence word by word. Beginning with the word \"I\", the RNN ingests it, generates an output, and updates its hidden state. Moving on to \"love\", the RNN processes it alongside the updated hidden state which already holds insights about the word \"I\". The hidden state is updated again post this. This pattern of processing and updating continues until the last word. By the end of the sequence, the hidden state ideally encapsulates insights from the entire sentence.\n",
    "                                                                                                       \n",
    "### 3. Long short-term memory (LSTM) and gated recurrent units (GRUs)\n",
    "Long short-term memory (LSTM) and gated recurrent units (GRUs) are advanced variations of recurrent neural networks (RNNs), designed to address the limitations of traditional RNNs and enhance their ability to model sequential data effectively. They processed sequences one element at a time and maintained an internal state to remember past elements. While they were effective for a variety of tasks, they struggled with long sequences and long-term dependencies.\n",
    "\n",
    "### 4. Seq2seq models with attention\n",
    "- Sequence-to-sequence (seq2seq) models, often built with RNNs or LSTMs, were designed to handle tasks like translation where an input sequence is transformed into an output sequence.\n",
    "- The attention mechanism was introduced to allow the model to \"focus\" on relevant parts of the input sequence when generating the output, significantly improving performance on tasks like machine translation.\n",
    "\n",
    "While these methods provided significant advancements in text generation tasks, the introduction of transformers led to a paradigm shift. Transformers, with their self-attention mechanism, proved to be highly efficient at capturing contextual information across long sequences, setting new benchmarks in various NLP tasks.\n",
    "\n",
    "## Transformers\n",
    "Proposed in a paper titled \"Attention Is All You Need\" by Vaswani et al. in 2017, the transformer architecture replaced sequential processing with parallel processing. The key component behind its success? The attention mechanism, more precisely, self-attention.\n",
    "\n",
    "Key steps include:\n",
    "- Tokenization: The first step is breaking down a sentence into tokens (words or subwords).\n",
    "- Embedding: Each token is represented as a vector, capturing its meaning.\n",
    "- Self-attention: The model computes scores determining the importance of every other word for a particular word in the sequence. These scores are used to weight the input tokens and produce a new representation of the sequence. For instance, in the sentence \"He gave her a gift because she'd helped him\", understanding who \"her\" refers to requires the model to pay attention to other words in the sentence. The transformer does this for every word, considering the entire context, which is particularly powerful for understanding meaning.\n",
    "- Feed-forward neural networks: After attention, each position is passed through a feed-forward network separately.\n",
    "- Output sequence: The model produces an output sequence, which can be used for various tasks, like classification, translation, or text generation.\n",
    "- Layering: Importantly, transformers are deep models with multiple layers of attention and feed-forward networks, allowing them to learn complex patterns.\n",
    "\n",
    "The architecture's flexibility has allowed transformers to be used beyond NLP, finding applications in image and video processing too. In NLP, transformer-based models like BERT, GPT, and their variants have set state-of-the-art results in various tasks, from text classification to translation.\n",
    "\n",
    "### Implementation: Building a simple chatbot with transformers\n",
    "Now, you will build a simple chatbot using `transformers` library from Hugging Face, which is an open-source natural language processing (NLP) toolkit with many useful features.\n",
    "#### Step 1: Installing libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174c8779-75fd-4c81-9d89-75bb0d1267d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qq tensorflow\n",
    "# !pip install -qq transformers\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54725b6b-49c6-40e1-b0bf-7807eef973f7",
   "metadata": {},
   "source": [
    "#### Step 2: Importing the required tools from the transformers library\n",
    "In the upcoming script, you initiate variables using two invaluable classes from the transformers library:\n",
    "- `model` is an instance of the class `AutoModelForSeq2SeqLM`. This class lets you interact with your chosen language model.\n",
    "- `tokenizer` is an instance of the class `AutoTokenizer`. This class streamlines your input and presents it to the language model in the most efficient manner. It achieves this by converting your text input into \"tokens\", which is the model's preferred way of interpreting text.\n",
    "You choose \"facebook/blenderbot-400M-distill\" for this example model because it is freely available under an open-source license and operates at a relatively brisk pace. For a diverse range of models and their capabilities, you can explore the Hugging Face website: [Hugging Face Models](https://huggingface.co/models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f812c0-e91d-4dd3-8f48-c7990eb5c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successful!\n"
     ]
    }
   ],
   "source": [
    "# Press Tab for auto suggestion\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"Import Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36601534-9cb2-495c-b5a8-cc1e95aae7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AutoTokenizer in module transformers.models.auto.tokenization_auto:\n",
      "\n",
      "class AutoTokenizer(builtins.object)\n",
      " |  This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when\n",
      " |  created with the [`AutoTokenizer.from_pretrained`] class method.\n",
      " |  \n",
      " |  This class cannot be instantiated directly using `__init__()` (throws an error).\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  register(config_class, slow_tokenizer_class=None, fast_tokenizer_class=None, exist_ok=False)\n",
      " |      Register a new tokenizer in this mapping.\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          config_class ([`PretrainedConfig`]):\n",
      " |              The configuration corresponding to the model to register.\n",
      " |          slow_tokenizer_class ([`PretrainedTokenizer`], *optional*):\n",
      " |              The slow tokenizer to register.\n",
      " |          fast_tokenizer_class ([`PretrainedTokenizerFast`], *optional*):\n",
      " |              The fast tokenizer to register.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs) from builtins.type\n",
      " |      Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.\n",
      " |      \n",
      " |      The tokenizer class to instantiate is selected based on the `model_type` property of the config object (either\n",
      " |      passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n",
      " |      falling back to using pattern matching on `pretrained_model_name_or_path`:\n",
      " |      \n",
      " |          - **albert** -- [`AlbertTokenizer`] or [`AlbertTokenizerFast`] (ALBERT model)\n",
      " |          - **align** -- [`BertTokenizer`] or [`BertTokenizerFast`] (ALIGN model)\n",
      " |          - **bark** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Bark model)\n",
      " |          - **bart** -- [`BartTokenizer`] or [`BartTokenizerFast`] (BART model)\n",
      " |          - **barthez** -- [`BarthezTokenizer`] or [`BarthezTokenizerFast`] (BARThez model)\n",
      " |          - **bartpho** -- [`BartphoTokenizer`] (BARTpho model)\n",
      " |          - **bert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (BERT model)\n",
      " |          - **bert-generation** -- [`BertGenerationTokenizer`] (Bert Generation model)\n",
      " |          - **bert-japanese** -- [`BertJapaneseTokenizer`] (BertJapanese model)\n",
      " |          - **bertweet** -- [`BertweetTokenizer`] (BERTweet model)\n",
      " |          - **big_bird** -- [`BigBirdTokenizer`] or [`BigBirdTokenizerFast`] (BigBird model)\n",
      " |          - **bigbird_pegasus** -- [`PegasusTokenizer`] or [`PegasusTokenizerFast`] (BigBird-Pegasus model)\n",
      " |          - **biogpt** -- [`BioGptTokenizer`] (BioGpt model)\n",
      " |          - **blenderbot** -- [`BlenderbotTokenizer`] or [`BlenderbotTokenizerFast`] (Blenderbot model)\n",
      " |          - **blenderbot-small** -- [`BlenderbotSmallTokenizer`] (BlenderbotSmall model)\n",
      " |          - **blip** -- [`BertTokenizer`] or [`BertTokenizerFast`] (BLIP model)\n",
      " |          - **blip-2** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (BLIP-2 model)\n",
      " |          - **bloom** -- [`BloomTokenizerFast`] (BLOOM model)\n",
      " |          - **bridgetower** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (BridgeTower model)\n",
      " |          - **bros** -- [`BertTokenizer`] or [`BertTokenizerFast`] (BROS model)\n",
      " |          - **byt5** -- [`ByT5Tokenizer`] (ByT5 model)\n",
      " |          - **camembert** -- [`CamembertTokenizer`] or [`CamembertTokenizerFast`] (CamemBERT model)\n",
      " |          - **canine** -- [`CanineTokenizer`] (CANINE model)\n",
      " |          - **chameleon** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Chameleon model)\n",
      " |          - **chinese_clip** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Chinese-CLIP model)\n",
      " |          - **clap** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (CLAP model)\n",
      " |          - **clip** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (CLIP model)\n",
      " |          - **clipseg** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (CLIPSeg model)\n",
      " |          - **clvp** -- [`ClvpTokenizer`] (CLVP model)\n",
      " |          - **code_llama** -- [`CodeLlamaTokenizer`] or [`CodeLlamaTokenizerFast`] (CodeLlama model)\n",
      " |          - **codegen** -- [`CodeGenTokenizer`] or [`CodeGenTokenizerFast`] (CodeGen model)\n",
      " |          - **cohere** -- [`CohereTokenizerFast`] (Cohere model)\n",
      " |          - **convbert** -- [`ConvBertTokenizer`] or [`ConvBertTokenizerFast`] (ConvBERT model)\n",
      " |          - **cpm** -- [`CpmTokenizer`] or [`CpmTokenizerFast`] (CPM model)\n",
      " |          - **cpmant** -- [`CpmAntTokenizer`] (CPM-Ant model)\n",
      " |          - **ctrl** -- [`CTRLTokenizer`] (CTRL model)\n",
      " |          - **data2vec-audio** -- [`Wav2Vec2CTCTokenizer`] (Data2VecAudio model)\n",
      " |          - **data2vec-text** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (Data2VecText model)\n",
      " |          - **dbrx** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (DBRX model)\n",
      " |          - **deberta** -- [`DebertaTokenizer`] or [`DebertaTokenizerFast`] (DeBERTa model)\n",
      " |          - **deberta-v2** -- [`DebertaV2Tokenizer`] or [`DebertaV2TokenizerFast`] (DeBERTa-v2 model)\n",
      " |          - **distilbert** -- [`DistilBertTokenizer`] or [`DistilBertTokenizerFast`] (DistilBERT model)\n",
      " |          - **dpr** -- [`DPRQuestionEncoderTokenizer`] or [`DPRQuestionEncoderTokenizerFast`] (DPR model)\n",
      " |          - **electra** -- [`ElectraTokenizer`] or [`ElectraTokenizerFast`] (ELECTRA model)\n",
      " |          - **ernie** -- [`BertTokenizer`] or [`BertTokenizerFast`] (ERNIE model)\n",
      " |          - **ernie_m** -- [`ErnieMTokenizer`] (ErnieM model)\n",
      " |          - **esm** -- [`EsmTokenizer`] (ESM model)\n",
      " |          - **falcon** -- [`PreTrainedTokenizerFast`] (Falcon model)\n",
      " |          - **fastspeech2_conformer** --  (FastSpeech2Conformer model)\n",
      " |          - **flaubert** -- [`FlaubertTokenizer`] (FlauBERT model)\n",
      " |          - **fnet** -- [`FNetTokenizer`] or [`FNetTokenizerFast`] (FNet model)\n",
      " |          - **fsmt** -- [`FSMTTokenizer`] (FairSeq Machine-Translation model)\n",
      " |          - **funnel** -- [`FunnelTokenizer`] or [`FunnelTokenizerFast`] (Funnel Transformer model)\n",
      " |          - **gemma** -- [`GemmaTokenizer`] or [`GemmaTokenizerFast`] (Gemma model)\n",
      " |          - **gemma2** -- [`GemmaTokenizer`] or [`GemmaTokenizerFast`] (Gemma2 model)\n",
      " |          - **git** -- [`BertTokenizer`] or [`BertTokenizerFast`] (GIT model)\n",
      " |          - **gpt-sw3** -- [`GPTSw3Tokenizer`] (GPT-Sw3 model)\n",
      " |          - **gpt2** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (OpenAI GPT-2 model)\n",
      " |          - **gpt_bigcode** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (GPTBigCode model)\n",
      " |          - **gpt_neo** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (GPT Neo model)\n",
      " |          - **gpt_neox** -- [`GPTNeoXTokenizerFast`] (GPT NeoX model)\n",
      " |          - **gpt_neox_japanese** -- [`GPTNeoXJapaneseTokenizer`] (GPT NeoX Japanese model)\n",
      " |          - **gptj** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (GPT-J model)\n",
      " |          - **gptsan-japanese** -- [`GPTSanJapaneseTokenizer`] (GPTSAN-japanese model)\n",
      " |          - **grounding-dino** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Grounding DINO model)\n",
      " |          - **groupvit** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (GroupViT model)\n",
      " |          - **herbert** -- [`HerbertTokenizer`] or [`HerbertTokenizerFast`] (HerBERT model)\n",
      " |          - **hubert** -- [`Wav2Vec2CTCTokenizer`] (Hubert model)\n",
      " |          - **ibert** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (I-BERT model)\n",
      " |          - **idefics** -- [`LlamaTokenizerFast`] (IDEFICS model)\n",
      " |          - **idefics2** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Idefics2 model)\n",
      " |          - **instructblip** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (InstructBLIP model)\n",
      " |          - **instructblipvideo** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (InstructBlipVideo model)\n",
      " |          - **jamba** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Jamba model)\n",
      " |          - **jetmoe** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (JetMoe model)\n",
      " |          - **jukebox** -- [`JukeboxTokenizer`] (Jukebox model)\n",
      " |          - **kosmos-2** -- [`XLMRobertaTokenizer`] or [`XLMRobertaTokenizerFast`] (KOSMOS-2 model)\n",
      " |          - **layoutlm** -- [`LayoutLMTokenizer`] or [`LayoutLMTokenizerFast`] (LayoutLM model)\n",
      " |          - **layoutlmv2** -- [`LayoutLMv2Tokenizer`] or [`LayoutLMv2TokenizerFast`] (LayoutLMv2 model)\n",
      " |          - **layoutlmv3** -- [`LayoutLMv3Tokenizer`] or [`LayoutLMv3TokenizerFast`] (LayoutLMv3 model)\n",
      " |          - **layoutxlm** -- [`LayoutXLMTokenizer`] or [`LayoutXLMTokenizerFast`] (LayoutXLM model)\n",
      " |          - **led** -- [`LEDTokenizer`] or [`LEDTokenizerFast`] (LED model)\n",
      " |          - **lilt** -- [`LayoutLMv3Tokenizer`] or [`LayoutLMv3TokenizerFast`] (LiLT model)\n",
      " |          - **llama** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (LLaMA model)\n",
      " |          - **llava** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (LLaVa model)\n",
      " |          - **llava-next-video** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (LLaVa-NeXT-Video model)\n",
      " |          - **llava_next** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (LLaVA-NeXT model)\n",
      " |          - **longformer** -- [`LongformerTokenizer`] or [`LongformerTokenizerFast`] (Longformer model)\n",
      " |          - **longt5** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (LongT5 model)\n",
      " |          - **luke** -- [`LukeTokenizer`] (LUKE model)\n",
      " |          - **lxmert** -- [`LxmertTokenizer`] or [`LxmertTokenizerFast`] (LXMERT model)\n",
      " |          - **m2m_100** -- [`M2M100Tokenizer`] (M2M100 model)\n",
      " |          - **mamba** -- [`GPTNeoXTokenizerFast`] (Mamba model)\n",
      " |          - **mamba2** -- [`GPTNeoXTokenizerFast`] (mamba2 model)\n",
      " |          - **marian** -- [`MarianTokenizer`] (Marian model)\n",
      " |          - **mbart** -- [`MBartTokenizer`] or [`MBartTokenizerFast`] (mBART model)\n",
      " |          - **mbart50** -- [`MBart50Tokenizer`] or [`MBart50TokenizerFast`] (mBART-50 model)\n",
      " |          - **mega** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (MEGA model)\n",
      " |          - **megatron-bert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Megatron-BERT model)\n",
      " |          - **mgp-str** -- [`MgpstrTokenizer`] (MGP-STR model)\n",
      " |          - **mistral** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Mistral model)\n",
      " |          - **mixtral** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Mixtral model)\n",
      " |          - **mluke** -- [`MLukeTokenizer`] (mLUKE model)\n",
      " |          - **mobilebert** -- [`MobileBertTokenizer`] or [`MobileBertTokenizerFast`] (MobileBERT model)\n",
      " |          - **mpnet** -- [`MPNetTokenizer`] or [`MPNetTokenizerFast`] (MPNet model)\n",
      " |          - **mpt** -- [`GPTNeoXTokenizerFast`] (MPT model)\n",
      " |          - **mra** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (MRA model)\n",
      " |          - **mt5** -- [`MT5Tokenizer`] or [`MT5TokenizerFast`] (MT5 model)\n",
      " |          - **musicgen** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (MusicGen model)\n",
      " |          - **musicgen_melody** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (MusicGen Melody model)\n",
      " |          - **mvp** -- [`MvpTokenizer`] or [`MvpTokenizerFast`] (MVP model)\n",
      " |          - **nezha** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Nezha model)\n",
      " |          - **nllb** -- [`NllbTokenizer`] or [`NllbTokenizerFast`] (NLLB model)\n",
      " |          - **nllb-moe** -- [`NllbTokenizer`] or [`NllbTokenizerFast`] (NLLB-MOE model)\n",
      " |          - **nystromformer** -- [`AlbertTokenizer`] or [`AlbertTokenizerFast`] (Nyströmformer model)\n",
      " |          - **olmo** -- [`GPTNeoXTokenizerFast`] (OLMo model)\n",
      " |          - **oneformer** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (OneFormer model)\n",
      " |          - **openai-gpt** -- [`OpenAIGPTTokenizer`] or [`OpenAIGPTTokenizerFast`] (OpenAI GPT model)\n",
      " |          - **opt** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (OPT model)\n",
      " |          - **owlv2** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (OWLv2 model)\n",
      " |          - **owlvit** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (OWL-ViT model)\n",
      " |          - **paligemma** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (PaliGemma model)\n",
      " |          - **pegasus** -- [`PegasusTokenizer`] or [`PegasusTokenizerFast`] (Pegasus model)\n",
      " |          - **pegasus_x** -- [`PegasusTokenizer`] or [`PegasusTokenizerFast`] (PEGASUS-X model)\n",
      " |          - **perceiver** -- [`PerceiverTokenizer`] (Perceiver model)\n",
      " |          - **persimmon** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Persimmon model)\n",
      " |          - **phi** -- [`CodeGenTokenizer`] or [`CodeGenTokenizerFast`] (Phi model)\n",
      " |          - **phi3** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Phi3 model)\n",
      " |          - **phobert** -- [`PhobertTokenizer`] (PhoBERT model)\n",
      " |          - **pix2struct** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (Pix2Struct model)\n",
      " |          - **plbart** -- [`PLBartTokenizer`] (PLBart model)\n",
      " |          - **prophetnet** -- [`ProphetNetTokenizer`] (ProphetNet model)\n",
      " |          - **qdqbert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (QDQBert model)\n",
      " |          - **qwen2** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (Qwen2 model)\n",
      " |          - **qwen2_moe** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (Qwen2MoE model)\n",
      " |          - **rag** -- [`RagTokenizer`] (RAG model)\n",
      " |          - **realm** -- [`RealmTokenizer`] or [`RealmTokenizerFast`] (REALM model)\n",
      " |          - **recurrent_gemma** -- [`GemmaTokenizer`] or [`GemmaTokenizerFast`] (RecurrentGemma model)\n",
      " |          - **reformer** -- [`ReformerTokenizer`] or [`ReformerTokenizerFast`] (Reformer model)\n",
      " |          - **rembert** -- [`RemBertTokenizer`] or [`RemBertTokenizerFast`] (RemBERT model)\n",
      " |          - **retribert** -- [`RetriBertTokenizer`] or [`RetriBertTokenizerFast`] (RetriBERT model)\n",
      " |          - **roberta** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (RoBERTa model)\n",
      " |          - **roberta-prelayernorm** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (RoBERTa-PreLayerNorm model)\n",
      " |          - **roc_bert** -- [`RoCBertTokenizer`] (RoCBert model)\n",
      " |          - **roformer** -- [`RoFormerTokenizer`] or [`RoFormerTokenizerFast`] (RoFormer model)\n",
      " |          - **rwkv** -- [`GPTNeoXTokenizerFast`] (RWKV model)\n",
      " |          - **seamless_m4t** -- [`SeamlessM4TTokenizer`] or [`SeamlessM4TTokenizerFast`] (SeamlessM4T model)\n",
      " |          - **seamless_m4t_v2** -- [`SeamlessM4TTokenizer`] or [`SeamlessM4TTokenizerFast`] (SeamlessM4Tv2 model)\n",
      " |          - **siglip** -- [`SiglipTokenizer`] (SigLIP model)\n",
      " |          - **speech_to_text** -- [`Speech2TextTokenizer`] (Speech2Text model)\n",
      " |          - **speech_to_text_2** -- [`Speech2Text2Tokenizer`] (Speech2Text2 model)\n",
      " |          - **speecht5** -- [`SpeechT5Tokenizer`] (SpeechT5 model)\n",
      " |          - **splinter** -- [`SplinterTokenizer`] or [`SplinterTokenizerFast`] (Splinter model)\n",
      " |          - **squeezebert** -- [`SqueezeBertTokenizer`] or [`SqueezeBertTokenizerFast`] (SqueezeBERT model)\n",
      " |          - **stablelm** -- [`GPTNeoXTokenizerFast`] (StableLm model)\n",
      " |          - **starcoder2** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (Starcoder2 model)\n",
      " |          - **switch_transformers** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (SwitchTransformers model)\n",
      " |          - **t5** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (T5 model)\n",
      " |          - **tapas** -- [`TapasTokenizer`] (TAPAS model)\n",
      " |          - **tapex** -- [`TapexTokenizer`] (TAPEX model)\n",
      " |          - **transfo-xl** -- [`TransfoXLTokenizer`] (Transformer-XL model)\n",
      " |          - **tvp** -- [`BertTokenizer`] or [`BertTokenizerFast`] (TVP model)\n",
      " |          - **udop** -- [`UdopTokenizer`] or [`UdopTokenizerFast`] (UDOP model)\n",
      " |          - **umt5** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (UMT5 model)\n",
      " |          - **video_llava** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (VideoLlava model)\n",
      " |          - **vilt** -- [`BertTokenizer`] or [`BertTokenizerFast`] (ViLT model)\n",
      " |          - **vipllava** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (VipLlava model)\n",
      " |          - **visual_bert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (VisualBERT model)\n",
      " |          - **vits** -- [`VitsTokenizer`] (VITS model)\n",
      " |          - **wav2vec2** -- [`Wav2Vec2CTCTokenizer`] (Wav2Vec2 model)\n",
      " |          - **wav2vec2-bert** -- [`Wav2Vec2CTCTokenizer`] (Wav2Vec2-BERT model)\n",
      " |          - **wav2vec2-conformer** -- [`Wav2Vec2CTCTokenizer`] (Wav2Vec2-Conformer model)\n",
      " |          - **wav2vec2_phoneme** -- [`Wav2Vec2PhonemeCTCTokenizer`] (Wav2Vec2Phoneme model)\n",
      " |          - **whisper** -- [`WhisperTokenizer`] or [`WhisperTokenizerFast`] (Whisper model)\n",
      " |          - **xclip** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (X-CLIP model)\n",
      " |          - **xglm** -- [`XGLMTokenizer`] or [`XGLMTokenizerFast`] (XGLM model)\n",
      " |          - **xlm** -- [`XLMTokenizer`] (XLM model)\n",
      " |          - **xlm-prophetnet** -- [`XLMProphetNetTokenizer`] (XLM-ProphetNet model)\n",
      " |          - **xlm-roberta** -- [`XLMRobertaTokenizer`] or [`XLMRobertaTokenizerFast`] (XLM-RoBERTa model)\n",
      " |          - **xlm-roberta-xl** -- [`XLMRobertaTokenizer`] or [`XLMRobertaTokenizerFast`] (XLM-RoBERTa-XL model)\n",
      " |          - **xlnet** -- [`XLNetTokenizer`] or [`XLNetTokenizerFast`] (XLNet model)\n",
      " |          - **xmod** -- [`XLMRobertaTokenizer`] or [`XLMRobertaTokenizerFast`] (X-MOD model)\n",
      " |          - **yoso** -- [`AlbertTokenizer`] or [`AlbertTokenizerFast`] (YOSO model)\n",
      " |      \n",
      " |      Params:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      " |                  - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                    using the [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n",
      " |                  - A path or url to a single saved vocabulary file if and only if the tokenizer only requires a\n",
      " |                    single vocabulary file (like Bert or XLNet), e.g.: `./my_model_directory/vocab.txt`. (Not\n",
      " |                    applicable to all derived classes)\n",
      " |          inputs (additional positional arguments, *optional*):\n",
      " |              Will be passed along to the Tokenizer `__init__()` method.\n",
      " |          config ([`PretrainedConfig`], *optional*)\n",
      " |              The configuration object used to determine the tokenizer class to instantiate.\n",
      " |          cache_dir (`str` or `os.PathLike`, *optional*):\n",
      " |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download the model weights and configuration files and override the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download:\n",
      " |              Deprecated and ignored. All downloads are now resumed by default when possible.\n",
      " |              Will be removed in v5 of Transformers.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (`str`, *optional*):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      " |              facebook/rag-token-base), specify it here.\n",
      " |          use_fast (`bool`, *optional*, defaults to `True`):\n",
      " |              Use a [fast Rust-based tokenizer](https://huggingface.co/docs/tokenizers/index) if it is supported for\n",
      " |              a given model. If a fast tokenizer is not available for a given model, a normal Python-based tokenizer\n",
      " |              is returned instead.\n",
      " |          tokenizer_type (`str`, *optional*):\n",
      " |              Tokenizer type to be loaded.\n",
      " |          trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
      " |              should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
      " |              execute code present on the Hub on your local machine.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the Tokenizer `__init__()` method. Can be used to set special tokens like\n",
      " |              `bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,\n",
      " |              `additional_special_tokens`. See parameters in the `__init__()` for more details.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      \n",
      " |      >>> # Download vocabulary from huggingface.co and cache.\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
      " |      \n",
      " |      >>> # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
      " |      \n",
      " |      >>> # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\n",
      " |      >>> # tokenizer = AutoTokenizer.from_pretrained(\"./test/bert_saved_model/\")\n",
      " |      \n",
      " |      >>> # Download vocabulary from huggingface.co and define model-specific arguments\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space=True)\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get helper from docstring\n",
    "help(AutoTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023cf75d-36cd-4f5a-b6aa-de8cb2eb730f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b23686dfe7243b3a6e20f3c2198e429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93585d0730e4c3999d4e2b28b4b01d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/730M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c38f81528ac42278b2e428feabe1d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374cbb395f1342d3b5b1b9f75086639c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c0735434f548f4ae5e30550de48484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/127k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66811e1b0ff456fa277db8348cb020f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/62.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef99930c67684f7fb13b48b82d9fad02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/16.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a88ed61f174486e8edcf1ec8e690dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afaad7a7e4344d6fa131becf76668564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/310k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py3x/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Selecting the model. You will be using \"facebook/blenderbot-400M-distill\" in this example.\n",
    "# Link https://huggingface.co/facebook/blenderbot-400M-distill\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "# Load the model and tokenizer by model_name\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e99f6-cc19-4a9e-bbc3-bd18a650b6e8",
   "metadata": {},
   "source": [
    "Here’s an explanation of each file downloaded from Hugging Face when you retrieve a model:\n",
    "\n",
    "1. **config.json**: This file contains the model's configuration parameters, including the architecture type, number of layers, hidden units, attention heads, etc. It’s used to instantiate the model without needing to re-specify its architecture.\n",
    "\n",
    "2. **pytorch_model.bin**: This is the main file containing the pre-trained model weights. It's typically in PyTorch format (hence the `.bin` extension) and includes all the learned parameters of the model, such as the weights and biases.\n",
    "\n",
    "3. **generation_config.json**: If the model is used for text generation tasks, this file contains configuration details specific to generation settings, like maximum output length, top-k sampling, temperature, and other parameters influencing how the model generates text.\n",
    "\n",
    "4. **tokenizer_config.json**: This file contains settings related to the tokenizer, such as the type of tokenizer, preprocessing options (like whether to convert text to lowercase), or handling of unknown tokens.\n",
    "\n",
    "5. **vocab.json**: This file holds the mapping between tokens (typically subword units) and their corresponding IDs. It’s an essential component of the tokenizer, defining the vocabulary the model works with.\n",
    "\n",
    "6. **merges.txt**: Used in conjunction with the `vocab.json` file for models using Byte Pair Encoding (BPE). This file lists the merging rules for combining tokens into subwords based on frequency in the training data.\n",
    "\n",
    "7. **added_tokens.json**: If any additional tokens were added beyond the standard vocabulary (e.g., special tokens for certain tasks), they are defined here.\n",
    "\n",
    "8. **special_tokens_map.json**: This file maps special tokens (such as `<PAD>`, `<CLS>`, `<SEP>`, `<MASK>`, etc.) to their corresponding token IDs. These tokens play special roles during the model's training and inference processes.\n",
    "\n",
    "9. **tokenizer.json**: This is a consolidated file that includes information about the tokenizer, combining vocab, merges, and other tokenization rules into one JSON file. It is a more efficient format that Hugging Face’s `transformers` library can load directly.\n",
    "\n",
    "Each of these files serves a specific role in ensuring that the model can be used effectively for inference, training, or fine-tuning with the proper configuration and tokenization settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d3cad-b7d6-4269-b91d-0e484bfc5ba2",
   "metadata": {},
   "source": [
    "Following the initialization, let's set up the chat function to enable real-time interaction with the chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b59a17-b023-4a7a-8f34-f7058b3854dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chat function\n",
    "def chat_with_bot():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Exit conditions\n",
    "        if input_text.lower() in [\"quit\",\"exit\",\"byte\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenize input and generate response\n",
    "        inputs = tokenizer.encode(input_text, return_tensors = \"pt\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=150)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Display bot's response\n",
    "        print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e70fb33-f7ca-4dc7-9cd4-5553e2d0061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi, how are you? I just got back from walking my dog. Do you have any pets?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  No i don't\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm sorry to hear that. Do you have any hobbies that you like to do?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Yes i do\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: What do you like to do in your free time?  I like to play video games.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I like coding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I love coding as well. It is one of my favorite hobbies. What do you like to do in your free time?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Start chatting\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4530b97f-87e1-4845-9b3f-4bfe966d5adc",
   "metadata": {},
   "source": [
    "Alright! You have successfully interacted with your chatbot. By providing it with a prompt, the chatbot used the power of the transformers library and the underlying model to generate a response. This exemplifies the prowess of transformer-based models in comprehending and generating human-like text based on a given context. As you continue to engage with it, you will observe its capacity to simulate a wide range of conversational topics and styles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff624a92-7b6d-410b-84ca-a4e92e297c46",
   "metadata": {},
   "source": [
    "#### Step 3: Trying another language model and comparing the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f7f536-076e-4423-aef9-3356e297a3d8",
   "metadata": {},
   "source": [
    "You can use a different language model, for example the \"[flan-t5-base](https://huggingface.co/google/flan-t5-base)\" model from Google, to create a similar chatbot. You can use a chat function similar to the one defined in Step 2 and compare the outputs of both models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c05b90-25af-4cb5-bf71-116adfc245b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successful!\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"Import Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa4f22f-21b4-4130-a5c2-66b290bc4cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b0cd84de2a4a9fa52d5e4e32cad2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028ce0b9f2c34f51bcdcc71d90832ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79430a2c0d0b47ebbec67cde9d34c229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca1c6925afc4136854a9d3bfdcddd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py3x/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344e50093c124501ac23b47ee4f6e9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306796dfe1944e6fa25b836442c9b199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92f68e3b4224f9d94ed7d7de6194c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download model https://huggingface.co/google/flan-t5-base\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8a1a76-b1b1-4f7f-ae5d-4944fe5dd8ba",
   "metadata": {},
   "source": [
    "The key difference between `flan-t5-base` and `t5-base` models on Hugging Face lies in the fine-tuning and pretraining strategies applied to them:\n",
    "\n",
    "### 1. **T5 (Text-to-Text Transfer Transformer)**:\n",
    "- **Base Model:** `t5-base` is the original T5 model developed by Google. It is pretrained on the \"Colossal Clean Crawled Corpus\" (C4) dataset using a **denoising** objective. The key idea behind T5 is that all NLP tasks can be reframed as a text-to-text problem, where the input and output are always text.\n",
    "- **Capabilities:** T5 can handle a wide range of tasks such as translation, summarization, classification, and question answering by formulating these tasks in the text-to-text format.\n",
    "- **Training:** The original T5 models are trained with no task-specific data, focusing on general language understanding.\n",
    "\n",
    "### 2. **Flan-T5 (Instruction-tuned T5)**:\n",
    "- **Base Model:** `flan-t5-base` is a variant of T5 that has undergone **instruction fine-tuning** on top of the T5 model. The \"Flan\" in the name refers to Google's \"FLAN\" (Fine-tuned Language Net), a framework for fine-tuning models on instruction-based tasks.\n",
    "- **Capabilities:** Flan-T5 is designed to handle instruction-based tasks (e.g., \"summarize this text\" or \"translate this sentence to French\") better than T5 because it has been fine-tuned with a variety of instruction-following data.\n",
    "- **Fine-tuning:** It has been trained on more task-specific datasets, including ones with instructions to make the model better at following natural language prompts or task instructions.\n",
    "\n",
    "### Summary:\n",
    "- `t5-base`: The original model trained with a general text-to-text framework but not specifically tuned for instruction-following tasks.\n",
    "- `flan-t5-base`: A fine-tuned version of T5, optimized for instruction-following by training on specific tasks with instructions.\n",
    "\n",
    "If you're working on instruction-following tasks, `flan-t5-base` is likely to perform better. For more general-purpose tasks, `t5-base` might still be a good option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f07709-610e-4a46-b9ed-c7692278dce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello, I am a new user on the forum. I am a new user on the forum. I am a new user on the forum. I am a new user on the forum. I am a new user on the forum. I am a new user on the forum. I am a new user on the forum. I am a new user on the forum. I am a new user on the forum. I am a new user on the forum. I am a new user on the forum. I am a new user on the forum. I am a new user on the forum. I am a new user on the forum. I am a new user on the\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "### Let's chat with another bot\n",
    "def chat_with_another_bot():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Exit conditions\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenize input and generate response\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Display bot's response\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Start chatting\n",
    "chat_with_another_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b1458-0ee3-4b3c-9b3e-386edd774a12",
   "metadata": {},
   "source": [
    "There are many language models available in Hugging Face. In the following exercise, you will compare the output for the same input using two different models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc5c5b-acce-4649-b506-81a361103c9e",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "### Create a chatbot using different models from Hugging Face\n",
    "\n",
    "Create a simple chatbot using the transformers library from Hugging Face(https://huggingface.co/models). Run the code using the following models and compare the output. The models are \"[google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\", \"[bert-base-uncased](https://huggingface.co/bert-base-uncased)\". \n",
    "(Note: Based on the selected model, you may notice differences in the chatbot output. Multiple factors, such as model training and fine-tuning, influence the output.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205d7ee5-fd02-45b1-ad12-cd3feabbbd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb6e10d998746fc925327e1787afe5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f8309711844114ad32ccf34b20415a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af645c0410114d71a654a94a0ff78b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc4e567a56a433e97126d511f066b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py3x/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d348cbe7a964613816d0b902fed7348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5d6f5292a8461987486a55d24d7803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2249293fca9444268f33aa0657adb4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello, Hello. Hello, Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Download the model https://huggingface.co/google/flan-t5-small\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "### Let's chat with another bot\n",
    "def chat_with_another_bot():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Exit conditions\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenize input and generate response\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Display bot's response\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Start chatting\n",
    "chat_with_another_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58648bd5-4906-40e7-99c7-5ca22ca655cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c170b6ee06409095b97fa69435bb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a76c802c0b4dd5af3c465186a368c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21db77ad52974b488c37007cde07a1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ba3f4b3ebd4841931ae802d72acab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b7507c42e449a9a749673395b61c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1386,  0.1583, -0.2967,  ..., -0.2708, -0.2844,  0.4581],\n",
      "         [ 0.5364, -0.2327,  0.1754,  ...,  0.5540,  0.4981, -0.0024],\n",
      "         [ 0.3002, -0.3475,  0.1208,  ..., -0.4562,  0.3288,  0.8773],\n",
      "         ...,\n",
      "         [ 0.3799,  0.1203,  0.8283,  ..., -0.8624, -0.5957,  0.0471],\n",
      "         [-0.0252, -0.7177, -0.6950,  ...,  0.0757, -0.6668, -0.3401],\n",
      "         [ 0.7535,  0.2391,  0.0717,  ...,  0.2467, -0.6458, -0.3213]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9377, -0.5043, -0.9799,  0.9030,  0.9329, -0.2438,  0.8926,  0.2288,\n",
      "         -0.9531, -1.0000, -0.8862,  0.9906,  0.9855,  0.7155,  0.9455, -0.8645,\n",
      "         -0.6035, -0.6666,  0.3020, -0.1587,  0.7455,  1.0000, -0.4022,  0.4261,\n",
      "          0.6151,  0.9996, -0.8773,  0.9594,  0.9585,  0.6950, -0.6718,  0.3325,\n",
      "         -0.9954, -0.2268, -0.9658, -0.9951,  0.6127, -0.7670,  0.0873,  0.0824,\n",
      "         -0.9518,  0.4713,  1.0000,  0.3299,  0.7583, -0.2670, -1.0000,  0.3166,\n",
      "         -0.9364,  0.9910,  0.9719,  0.9893,  0.2190,  0.6048,  0.5849, -0.4123,\n",
      "         -0.0063,  0.1719, -0.3988, -0.6190, -0.6603,  0.5069, -0.9757, -0.9039,\n",
      "          0.9926,  0.9323, -0.3687, -0.4869, -0.3143,  0.0499,  0.9129,  0.3396,\n",
      "         -0.1879, -0.9235,  0.8675,  0.3228, -0.6406,  1.0000, -0.7989, -0.9931,\n",
      "          0.9629,  0.9124,  0.4827, -0.7276,  0.5996, -1.0000,  0.7548, -0.1600,\n",
      "         -0.9941,  0.3386,  0.8394, -0.4158,  0.2943,  0.6111, -0.5745, -0.7185,\n",
      "         -0.4768, -0.9681, -0.4327, -0.6732,  0.1248, -0.2093, -0.5882, -0.4186,\n",
      "          0.5447, -0.6125, -0.6138,  0.4712,  0.4779,  0.7633,  0.3974, -0.4148,\n",
      "          0.7063, -0.9680,  0.7389, -0.4270, -0.9948, -0.6019, -0.9950,  0.7459,\n",
      "         -0.6343, -0.2753,  0.9522, -0.5724,  0.6218, -0.1295, -0.9905, -1.0000,\n",
      "         -0.8710, -0.7506, -0.5008, -0.4827, -0.9872, -0.9847,  0.7214,  0.9694,\n",
      "          0.3013,  1.0000, -0.4427,  0.9699, -0.5431, -0.8189,  0.9180, -0.5132,\n",
      "          0.9026,  0.5274, -0.5940,  0.2928, -0.6933,  0.7179, -0.9318, -0.2776,\n",
      "         -0.9160, -0.9457, -0.3287,  0.9556, -0.7927, -0.9860, -0.1904, -0.2760,\n",
      "         -0.6062,  0.9005,  0.9266,  0.4353, -0.6858,  0.4720,  0.2851,  0.7685,\n",
      "         -0.8647, -0.5626,  0.5127, -0.5468, -0.9490, -0.9907, -0.5809,  0.7146,\n",
      "          0.9948,  0.7981,  0.3463,  0.9349, -0.4238,  0.9333, -0.9754,  0.9936,\n",
      "         -0.2597,  0.4665, -0.5400,  0.4947, -0.8723,  0.0034,  0.8378, -0.9134,\n",
      "         -0.8432, -0.2516, -0.5177, -0.4687, -0.9491,  0.5691, -0.4856, -0.4857,\n",
      "         -0.2245,  0.9609,  0.9823,  0.7496,  0.6256,  0.8552, -0.9073, -0.5802,\n",
      "          0.2874,  0.3017,  0.3016,  0.9974, -0.8503, -0.2108, -0.9261, -0.9907,\n",
      "         -0.0252, -0.9488, -0.3972, -0.8097,  0.8707, -0.7512,  0.8107,  0.5488,\n",
      "         -0.9830, -0.8569,  0.4852, -0.6156,  0.4846, -0.2893,  0.9647,  0.9858,\n",
      "         -0.7064,  0.7120,  0.9593, -0.9590, -0.8708,  0.7893, -0.3561,  0.8603,\n",
      "         -0.7243,  0.9882,  0.9876,  0.9282, -0.9547, -0.8329, -0.7993, -0.8398,\n",
      "         -0.2333,  0.2315,  0.9712,  0.6055,  0.6388,  0.2429, -0.7884,  0.9981,\n",
      "         -0.9448, -0.9804, -0.8184, -0.3534, -0.9951,  0.9729,  0.4165,  0.8094,\n",
      "         -0.6227, -0.8183, -0.9817,  0.8532,  0.1242,  0.9826, -0.6376, -0.9450,\n",
      "         -0.8094, -0.9748,  0.0412, -0.3097, -0.8153, -0.0306, -0.9255,  0.5677,\n",
      "          0.6217,  0.6652, -0.9682,  0.9997,  1.0000,  0.9826,  0.9013,  0.8950,\n",
      "         -1.0000, -0.8081,  1.0000, -0.9995, -1.0000, -0.9361, -0.8200,  0.4755,\n",
      "         -1.0000, -0.2698, -0.0111, -0.9297,  0.8492,  0.9879,  0.9950, -1.0000,\n",
      "          0.8653,  0.9513, -0.5679,  0.9966, -0.6713,  0.9815,  0.6008,  0.7414,\n",
      "         -0.3265,  0.5574, -0.9801, -0.8956, -0.8082, -0.9267,  0.9999,  0.2542,\n",
      "         -0.7970, -0.8854,  0.7831, -0.1391, -0.0060, -0.9786, -0.4503,  0.8895,\n",
      "          0.9021,  0.3021,  0.2650, -0.5750,  0.5099,  0.1216,  0.1170,  0.6484,\n",
      "         -0.9505, -0.3889, -0.6938,  0.2508, -0.7526, -0.9831,  0.9646, -0.2742,\n",
      "          0.9865,  1.0000,  0.3756, -0.9045,  0.8847,  0.4860, -0.5515,  1.0000,\n",
      "          0.9092, -0.9904, -0.4959,  0.7900, -0.7156, -0.8280,  0.9999, -0.4197,\n",
      "         -0.9282, -0.7733,  0.9945, -0.9956,  0.9998, -0.8985, -0.9838,  0.9735,\n",
      "          0.9655, -0.8103, -0.8325,  0.1020, -0.6722,  0.4561, -0.9412,  0.8396,\n",
      "          0.6979, -0.1201,  0.9288, -0.8345, -0.6312,  0.4356, -0.8901, -0.4565,\n",
      "          0.9874,  0.5709, -0.2111, -0.0206, -0.4182, -0.9116, -0.9781,  0.8246,\n",
      "          1.0000, -0.4229,  0.9489, -0.5226, -0.0986,  0.2202,  0.7459,  0.7152,\n",
      "         -0.3528, -0.8800,  0.9299, -0.9716, -0.9949,  0.7278,  0.2206, -0.4944,\n",
      "          1.0000,  0.6285,  0.3795,  0.7228,  0.9993,  0.0301,  0.5936,  0.9816,\n",
      "          0.9914, -0.3465,  0.5882,  0.8365, -0.9824, -0.4488, -0.7612,  0.1331,\n",
      "         -0.9479, -0.0559, -0.9697,  0.9846,  0.9960,  0.5818,  0.3121,  0.8577,\n",
      "          1.0000, -0.9274,  0.6693, -0.1365,  0.8035, -1.0000, -0.8057, -0.4504,\n",
      "         -0.1711, -0.9512, -0.5899,  0.3991, -0.9754,  0.9563,  0.8806, -0.9937,\n",
      "         -0.9923, -0.4979,  0.8853,  0.1439, -0.9994, -0.8986, -0.6272,  0.8385,\n",
      "         -0.3239, -0.9470, -0.7009, -0.4768,  0.5742, -0.2216,  0.5665,  0.9667,\n",
      "          0.7935, -0.9401, -0.6746, -0.1753, -0.9163,  0.9409, -0.8701, -0.9894,\n",
      "         -0.2514,  1.0000, -0.4087,  0.9385,  0.6050,  0.8219, -0.2712,  0.3326,\n",
      "          0.9827,  0.3613, -0.8314, -0.9850, -0.2861, -0.5398,  0.8254,  0.8414,\n",
      "          0.7590,  0.9412,  0.9627,  0.2765, -0.0737,  0.0399,  0.9998, -0.3095,\n",
      "         -0.1933, -0.4689, -0.2511, -0.4629, -0.2914,  1.0000,  0.3963,  0.7777,\n",
      "         -0.9950, -0.9808, -0.9303,  1.0000,  0.8822, -0.6848,  0.8124,  0.6242,\n",
      "         -0.2551,  0.8266, -0.2791, -0.3167,  0.2294,  0.1682,  0.9627, -0.6738,\n",
      "         -0.9904, -0.7910,  0.7099, -0.9770,  1.0000, -0.7030, -0.3960, -0.5981,\n",
      "         -0.6683, -0.2727, -0.0183, -0.9882, -0.3841,  0.5605,  0.9745,  0.3505,\n",
      "         -0.4898, -0.9298,  0.9578,  0.9533, -0.9859, -0.9597,  0.9777, -0.9784,\n",
      "          0.7551,  1.0000,  0.3446,  0.6786,  0.3947, -0.5349,  0.5541, -0.6754,\n",
      "          0.8078, -0.9595, -0.4484, -0.3901,  0.3983, -0.1319, -0.2896,  0.7860,\n",
      "          0.3500, -0.5530, -0.7294, -0.2361,  0.4663,  0.9332, -0.3048, -0.1916,\n",
      "          0.2318, -0.3230, -0.9323, -0.4672, -0.6315, -1.0000,  0.8068, -1.0000,\n",
      "          0.8035,  0.4066, -0.3700,  0.8760,  0.7829,  0.8298, -0.8628, -0.9795,\n",
      "          0.1322,  0.8529, -0.5029, -0.9057, -0.6918,  0.5017, -0.2052,  0.1564,\n",
      "         -0.7397,  0.8156, -0.3414,  1.0000,  0.2659, -0.8292, -0.9821,  0.2491,\n",
      "         -0.3009,  1.0000, -0.8952, -0.9832,  0.3330, -0.9180, -0.8493,  0.5868,\n",
      "          0.1653, -0.8522, -0.9961,  0.9220,  0.8661, -0.6477,  0.7927, -0.3991,\n",
      "         -0.7691,  0.1512,  0.9868,  0.9924,  0.7317,  0.9083, -0.1227, -0.5258,\n",
      "          0.9840,  0.4009, -0.0436,  0.1361,  1.0000,  0.4004, -0.9497, -0.1309,\n",
      "         -0.9788, -0.3522, -0.9551,  0.3755,  0.3099,  0.9195, -0.4460,  0.9738,\n",
      "         -0.9714,  0.1901, -0.8894, -0.7863,  0.4757, -0.9463, -0.9892, -0.9938,\n",
      "          0.8142, -0.4077, -0.1895,  0.2102,  0.1715,  0.6322,  0.5566, -1.0000,\n",
      "          0.9642,  0.6150,  0.9768,  0.9768,  0.9115,  0.8108,  0.3251, -0.9920,\n",
      "         -0.9910, -0.5438, -0.3567,  0.7960,  0.7648,  0.8900,  0.6470, -0.4875,\n",
      "         -0.4792, -0.7756, -0.8423, -0.9972,  0.5961, -0.8679, -0.9678,  0.9718,\n",
      "         -0.3461, -0.1534, -0.2139, -0.9586,  0.9321,  0.7627,  0.4636,  0.0862,\n",
      "          0.5071,  0.9170,  0.9597,  0.9882, -0.9231,  0.8555, -0.9196,  0.6712,\n",
      "          0.9381, -0.9606,  0.2335,  0.8301, -0.5560,  0.3696, -0.4752, -0.9740,\n",
      "          0.8174, -0.4268,  0.7773, -0.4798,  0.0639, -0.4718, -0.2607, -0.7624,\n",
      "         -0.8742,  0.6576,  0.6207,  0.9219,  0.9360, -0.0496, -0.8942, -0.3701,\n",
      "         -0.8944, -0.9526,  0.9536, -0.0851, -0.2961,  0.9031,  0.1321,  0.9324,\n",
      "          0.4289, -0.4989, -0.4174, -0.7639,  0.8887, -0.7894, -0.7639, -0.7093,\n",
      "          0.8105,  0.3595,  1.0000, -0.9188, -0.9878, -0.8268, -0.6012,  0.4992,\n",
      "         -0.7880, -1.0000,  0.3609, -0.8314,  0.8524, -0.9398,  0.9500, -0.9339,\n",
      "         -0.9851, -0.3495,  0.8436,  0.9375, -0.5159, -0.8989,  0.5196, -0.8797,\n",
      "          0.9979,  0.8753, -0.8277, -0.0012,  0.6013, -0.9184, -0.7398,  0.9228]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Download the model https://huggingface.co/google-bert/bert-base-uncased\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af7886-82a1-42be-80fd-21c9c82c8fb0",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab.\n",
    "\n",
    "## Authors\n",
    "\n",
    "[Vicky Kuo](https://author.skills.network/instructors/vicky_kuo) is completing her Master's degree in IT at York University with scholarships. Her master's thesis explores the optimization of deep learning algorithms, employing an innovative approach to scrutinize and enhance neural network structures and performance.\n",
    "\n",
    "© Copyright IBM Corporation. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
