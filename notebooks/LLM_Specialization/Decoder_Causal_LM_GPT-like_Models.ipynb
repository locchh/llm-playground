{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d821c9-cd5c-4715-9090-4f3ab2017b3a",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "# Decoder Causal Language Models\n",
    "\n",
    "Estimated time needed: **45** minutes\n",
    "\n",
    "In this tutorial, you'll learn how to build and train a decoder GPT-like model, which is great for generating text and other natural language processing tasks. We'll start with the basics, like getting our environment ready and preparing our data by breaking it down into tokens and turning those tokens into numbers the model can understand. Then, we'll dive into building the model itself, focusing on how it learns to pay attention to different parts of the text to generate text. Along the way, we'll cover how to train our model with data. Finally, we'll see how to use our trained model to create text based on what it has learned.\n",
    "\n",
    "## GPT models\n",
    "\n",
    "GPT (Generative Pretrained Transformer) is a decoder-only model because it is trained using a causal language modeling objective, where the goal is to predict the next token in a sequence given the previous tokens. During training, the input sequence is shifted to the right, and the model learns to generate output tokens autoregressively, one at a time. This process allows GPT to generate coherent and contextually relevant text based on the given input prompt. In this lab you will learn how to create and train a decoder-only GPT-like model. However, please note that the actual GPT models are larger models and are trained on massive training data for specific NLP tasks.\n",
    "\n",
    "## GPT vs. ChatGPT\n",
    "\n",
    "GPT and ChatGPT are both AI models developed by OpenAI, but they serve different purposes and have distinct functionalities.\n",
    "\n",
    "GPT is a family of large-scale transformer-based language models trained on diverse internet text data. GPT models are designed for a wide range of natural language processing tasks, such as text generation, translation, summarization, and question-answering. They generate responses based on the input text (prompt) but do not maintain a consistent conversation history.\n",
    "\n",
    "On the other hand, ChatGPT is a fine-tuned version of the GPT model, specifically designed for conversational AI applications. It is trained to maintain a consistent conversation history and generate contextually relevant responses, making it more suitable for chatbot-like interactions. ChatGPT excels at understanding and generating human-like dialogues, providing coherent and engaging responses in a conversational setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70635a39-d922-4082-a309-bee44856700c",
   "metadata": {},
   "source": [
    "## __Table of contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Text-pipeline\">Text pipeline</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Dataset\">Dataset</a></li>\n",
    "            <li> <a href=\"#Collate-function\">Collate function</a></li> \n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Model-prerequisites\">Model prerequisites</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Masking\">Masking</a></li>\n",
    "            <li><a href=\"#Positional-encoding\">Positional encoding</a></li>\n",
    "            <li><a href=\"#Token-embedding\">Token embedding</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Custom-GPT-model-architecture\">Custom GPT model architecture</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Model-configuration-and-initialization\">Model configuration and initialization</a></li>\n",
    "            <li><a href=\"#Iterating-through-data-samples\">Iterating through data samples</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Full-output\">Full output</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Decoding-the-differences:-Training-vs.-inference\">Decoding the differences: Training vs. inference</a></li>\n",
    "            <li><a href=\"#Training-the-model\">Training the model</a></li>\n",
    "            <li><a href=\"#Loading-the-saved-model\">Loading the saved model</a></li>\n",
    "            <li><a href=\"#Loading-GPT2-model-from-HuggingFace\">Loading GPT2 model from HuggingFace</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "      <li>\n",
    "        <a href=\"#Exercise:-Creating-a-decoder-model\">Exercise: Creating a decoder model</a>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca4c71-8088-4d8e-bb1c-d45a6e1a2c95",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After going through this tutorial, you'll be able to:\n",
    "\n",
    "- Understand how to pick random samples from your data and break text down into tokens.\n",
    "- Learn how to turn tokens into a vocabulary that your model can use to understand and process text.\n",
    "- Master the setup of the decoder model architecture, including how it uses attention to generate text.\n",
    "- Get familiar with training a decoder model, including how to feed it data and improve its performance over time.\n",
    "- Use your trained decoder model to generate text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d500d4a-b01b-40b4-96c9-66b904d1a6da",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a67bb1-881f-4b6a-8c72-bf5bbbff427c",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n",
    "\n",
    "```bash\n",
    "!pip install -U torchdata==0.7.1\n",
    "!pip install -Uqq portalocker>=2.0.0\n",
    "!pip install -qq torchtext==0.17.1\n",
    "!pip install -qq matplotlib\n",
    "!pip install -qq transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4bc803-a73a-4a14-a289-55b739777113",
   "metadata": {},
   "source": [
    "- **torchdata**: Enhances data loading and preprocessing functionalities for PyTorch, streamlining the workflow for machine learning models.\n",
    "- **portalocker**: Provides a mechanism to lock files, ensuring that only one process can access a file at a time, useful for managing file resources in concurrent applications.\n",
    "- **torchtext**: Offers utilities for text processing and datasets in PyTorch, simplifying the preparation of data for NLP tasks.\n",
    "- **matplotlib**: A plotting library for creating static, interactive, and animated visualizations in Python, commonly used for data visualization and graphical plotting tasks.\n",
    "\n",
    "Each of these libraries is used to handle different aspects of data preparation, processing, and model training for machine learning and natural language processing applications, enhancing the overall workflow and capabilities of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121f56c8-58c2-4309-92cd-b6055de797c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla P40\n",
      "Import Successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Iterable, List\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Transformer\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from torchtext.datasets import IMDB,PennTreebank\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "print(\"Import Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a750087-3054-4de3-9786-9da2be8a85dc",
   "metadata": {},
   "source": [
    "# Text pipeline\n",
    "## Dataset\n",
    "The code loads the IMDB dataset into training, and validation sets. It then creates an iterator for the training set and loops through the first 10 samples, printing each one. This process simulates how one might manually iterate over a dataset without using PyTorch's `DataLoader` for batch processing and data management.\n",
    "\n",
    "When training language models, it is generally advisable to use general-domain text. However, in this case, we are using the IMDB dataset, which is well-suited for classification tasks. However, we use IMDB due to its smaller size and compatibility with machines that have limited RAM. For language modeling tasks, some datasets you can consider include: [PennTreebank](https://pytorch.org/text/0.8.1/datasets.html#penntreebank), [WikiText-2](https://pytorch.org/text/0.8.1/datasets.html#wikitext-2), [WikiText103](https://pytorch.org/text/0.8.1/datasets.html#wikitext103)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6180e30b-41fc-41d9-913e-64f8e020c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_iter, val_iter = IMDB(root=\".data\", split=('train', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff5855c-d148-4b1f-af43-20b4ef9a49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_itr = iter(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b63a3-1ae8-4508-8c35-e7ed0ffb324f",
   "metadata": {},
   "source": [
    "Initialize an iterator for the train data loader:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe3b2f3-bce8-4da7-80bf-89f57171fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # retriving the third first record\n",
    "# next(data_itr)\n",
    "# next(data_itr)\n",
    "# next(data_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243cfc1-5bf1-442c-b165-09c5461f0ac9",
   "metadata": {},
   "source": [
    "Let's define our device (CPU or GPU) for training. We'll check if a GPU is available and use it; otherwise, we'll use the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1175b40d-14aa-4b54-8fe7-c42341c9288a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f77e3-7cd8-4322-a16f-eeb11ba6f903",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing data\n",
    "The provided code is used for preprocessing text data, particularly for NLP tasks, with a focus on tokenization and vocabulary building.\n",
    "\n",
    "- **Special Symbols and Indices**: Initializes special tokens (`<unk>`, `<pad>`, and an empty string for EOS) with their corresponding indices (`0`, `1`, and `2`). These tokens are used for unknown words, padding, and end of sentence respectively.\n",
    "    - `UNK_IDX`: Index for unknown words.\n",
    "    - `PAD_IDX`: Index used for padding shorter sentences in a batch to ensure uniform length.\n",
    "    - `EOS_IDX`: Index representing the end of a sentence (though not explicitly used here as the EOS symbol is set to an empty string).\n",
    "\n",
    "- **`yield_tokens` Function**: A generator function that iterates through a dataset (`data_iter`), tokenizing each data sample using a `tokenizer` function, and yields one tokenized sample at a time.\n",
    "\n",
    "- **Vocabulary building**: Constructs a vocabulary from the tokenized dataset. The `build_vocab_from_iterator` function processes tokens generated by `yield_tokens`, includes special tokens (`special_symbols`) at the beginning of the vocabulary, and sets a minimum frequency (`min_freq=1`) for tokens to be included.\n",
    "\n",
    "- **Default index for unknown tokens**: Sets a default index for tokens not found in the vocabulary (`UNK_IDX`), ensuring that out-of-vocabulary words are handled as unknown tokens.\n",
    "\n",
    "- **`text_to_index` function**: Converts a given text into a sequence of indices based on the built vocabulary. This function is essential for transforming raw text into a numerical format that can be processed by machine learning models.\n",
    "\n",
    "- **`index_to_en` function**: Transforms a sequence of indices back into a readable string. It's useful for interpreting the outputs of models and converting numerical predictions back into text.\n",
    "\n",
    "- **Check functionality**: Demonstrates the use of `index_to_en` by converting a tensor of indices `[0,1,2]` back into their corresponding special symbols. This helps verify that the vocabulary and index conversion functions are working as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0aeb73-f578-4b7f-b683-eb1396f06e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
