{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f884e68-4101-4a08-95e3-ae92d9baca06",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "\n",
    "#  Instruction-Tuning with LLMs\n",
    "\n",
    "Instruction-based fine-tuning, referred to as instruction GPT. It trains the language models to follow specific instructions and generate appropriate responses. For instruction-tuning, the dataset plays an important role as it provides structured examples of instructions, contexts, and responses, allowing the model to learn how to handle various tasks effectively. Instruction GPT often uses human feedback to refine and improve model performance; however, this lab doesn't cover this aspect.\n",
    "\n",
    "The context and instruction are concatenated to form a single input sequence that the model can understand and use to generate the correct response.\n",
    "\n",
    "#### Context and instruction\n",
    "\n",
    "\t•\tInstruction: A command to specify what the model should do\n",
    "\t•\tContext: Additional information or background required for performing the instruction\n",
    "\t•\tCombined input: The instruction and context combine together into a single input sequence\n",
    "    \n",
    "\n",
    "Let's review certain examples for various templates:\n",
    "\n",
    "---\n",
    "#### Response template\n",
    "Template: `### Question: {question}\\n ### Answer: {answer}`\n",
    "\n",
    "Example:\n",
    "```\n",
    "### Question: What is the capital of France?\n",
    "### Answer: Paris\n",
    "```\n",
    "\n",
    "---\n",
    "#### Conversation template\n",
    "\n",
    "Template: `### User: {user_input}\\n ### Bot: {bot_response}`\n",
    "Example:\n",
    "```\n",
    "### User: How are you today?\n",
    "### Bot: I'm doing great, thank you! How can I assist you today?\n",
    "```\n",
    "\n",
    "---\n",
    "#### Instruction and output template\n",
    "\n",
    "Template: `### Instruction: {instruction}\\n ### Output: {output}`\n",
    "\n",
    "Example:\n",
    "```\n",
    "### Instruction: Translate the following sentence to Spanish: \"Hello, how are you?\"\n",
    "### Output: \"Hola, ¿cómo estás?\"\n",
    "```\n",
    "\n",
    "---\n",
    "#### Completion template\n",
    "\n",
    "Template: `{prompt} ### Completion: {completion}`\n",
    "Example:\n",
    "```\n",
    "Once upon a time in a faraway land, ### Completion: there lived a wise old owl who knew all the secrets of the forest.\n",
    "```\n",
    "\n",
    "#### Summarization template\n",
    "\n",
    "Template: `### Text: {text}\\n ### Summary: {summary}`\n",
    "\n",
    "Example:\n",
    "```\n",
    "### Text: The quick brown fox jumps over the lazy dog.\n",
    "### Summary: A fox jumps over a dog.\n",
    "```\n",
    "\n",
    "---\n",
    "#### Dialogue template\n",
    "\n",
    "Template: `### Speaker 1: {utterance_1}\\n ### Speaker 2: {utterance_2}\\n ### Speaker 1: {utterance_3}`\n",
    "\n",
    "Example:\n",
    "```\n",
    "### Speaker 1: Hi, what are you doing today?\n",
    "### Speaker 2: I'm going to the park.\n",
    "### Speaker 1: That sounds fun!\n",
    "```\n",
    "\n",
    "---\n",
    "#### Code generation template\n",
    "\n",
    "Template: `### Task: {task_description}\\n ### Code: {code_output}`\n",
    "\n",
    "Example:\n",
    "```\n",
    "### Task: Write a function to add two numbers in Python.\n",
    "### Code: def add(a, b):\\n    return a + b\n",
    "```\n",
    "\n",
    "---\n",
    "#### Data analysis template\n",
    "\n",
    "Template: `### Analysis Task: {task_description}\\n ### Analysis: {analysis_output}`\n",
    "\n",
    "Example:\n",
    "```\n",
    "### Analysis Task: Provide insights from the sales data of Q1 2022.\n",
    "### Analysis: The sales increased by 15% compared to Q4 2021, with the highest growth in the electronics category.\n",
    "```\n",
    "\n",
    "---\n",
    "#### Recipe template\n",
    "\n",
    "Template: `### Recipe Name: {recipe_name}\\n ### Ingredients: {ingredients}\\n ### Instructions: {instructions}`\n",
    "\n",
    "Example:\n",
    "```\n",
    "### Recipe Name: Chocolate Chip Cookies\n",
    "### Ingredients: Flour, Sugar, Chocolate Chips, Butter, Eggs, Vanilla Extract\n",
    "### Instructions: Mix the dry ingredients, add the wet ingredients, fold in the chocolate chips, and bake at 350°F for 10-12 minutes.\n",
    "```\n",
    "\n",
    "---\n",
    "#### Explanation template\n",
    "\n",
    "Template: `### Concept: {concept}\\n ### Explanation: {explanation}`\n",
    "\n",
    "Example:\n",
    "```\n",
    "### Concept: Photosynthesis\n",
    "### Explanation: Photosynthesis is the process by which green plants use sunlight to synthesize nutrients from carbon dioxide and water.\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea4c893-aea9-414a-b442-d3c74c8cef54",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    " - Understand the various types of templates including instruction-response, question-answering, summarization, code generation, dialogue, data analysis, and explanation and their applications for fine-tuning large language models (LLMs).\n",
    " - Create and apply different templates to fine-tune LLMs for various tasks.\n",
    " - Format datasets based on the created templates to prepare them for effective model training\n",
    " - Perform instruction fine-tuning using Hugging Face libraries and tools\n",
    " - Apply Low-Rank Adaptation (LoRA) techniques to fine-tune LLMs efficiently\n",
    " - Configure and use the SFTTrainer for supervised fine-tuning of instruction-following models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7abea56-9273-4fdb-913c-cd225a7fff00",
   "metadata": {},
   "source": [
    "The concepts presented in this lab would apply to the other template formats as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1187f-a208-4ad3-aee9-51640954093b",
   "metadata": {},
   "source": [
    "# __Table of contents__\n",
    "\n",
    "<ol>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Install-required-libraries\">Install required libraries</a></li>\n",
    "            <li><a href=\"#Import-required-libraries\">Import required libraries</a></li>\n",
    "            <li><a href=\"#Define-the-device\">Define the device</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Dataset-description\">Dataset description</a></li>\n",
    "    <li><a href=\"#Model-and-tokenizer\">Model and tokenizer</a></li>\n",
    "    <li><a href=\"#Preprocessing-the-data\">Preprocessing the data</a></li>\n",
    "    <li><a href=\"#Test-the-base-model\">Test the base model</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"#BLEU-score\">BLEU score</a></li>\n",
    "        </ol>\n",
    "    <li><a href=\"#Perform-instruction-fine-tuning-with-LoRA\">Perform instruction fine-tuning with LoRA</a></li>\n",
    "    <li><a href=\"#Exercises\">Exercises</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c810c-6721-4fd0-b060-285faa193d40",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Install required libraries\n",
    "\n",
    "For this lab, use the following libraries, which are __not__ preinstalled in the Skills Network Labs environment. You can install libraries by running the code in the below cell. \n",
    "\n",
    "```bash\n",
    "!pip install -qq datasets==2.20.0 trl==0.9.6 transformers==4.42.3 peft==0.11.1 tqdm==4.66.4 numpy==1.26.4 pandas==2.2.2 matplotlib==3.9.1 seaborn==0.13.2 scikit-learn==1.5.1 sacrebleu==2.4.2 evaluate==0.4.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f776f-4e62-431f-8122-325bf42bf870",
   "metadata": {},
   "source": [
    "### Import required libraries\n",
    "\n",
    "The following code imports the required libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c72c891f-5045-4e1c-93f3-070b519efecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device: Tesla P40\n",
      "All imports loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Check CUDA availability\n",
    "import torch\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Transformers and NLP-related libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Training and evaluation utilities\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# Parameter-efficient fine-tuning (PEFT) utilities\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# General-purpose libraries\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "import io\n",
    "\n",
    "print(\"All imports loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1603e7b-3fee-41a9-8679-c3ccc812e789",
   "metadata": {},
   "source": [
    "### Define the device\n",
    "\n",
    "The below code will set your device to 'cuda' if your device is compatible with GPU, otherwise, you can use 'cpu'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8e97f4-c1e3-439f-9760-237e92410941",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e861f7b-df51-44ea-ab85-8131a1592614",
   "metadata": {},
   "source": [
    "# Dataset description\n",
    "\n",
    "Use the below sentences to download the CodeAlpaca 20k dataset, a programming code dataset. This code is available [here](https://github.com/sahil280114/codealpaca?tab=readme-ov-file#data-release). The CodeAlpaca dataset contains the following elements:\n",
    "\n",
    "\n",
    "- `instruction`: **str**, describes the task the model should perform. Each of the 20K instructions is unique.\n",
    "- `input`: **str**, optional context or input for the task. For example, when the instruction is \"Amend the following SQL query to select distinct elements\", the input is the SQL query. Around 40% of the examples have an input.\n",
    "- `output`: **str**, the answer to the instruction as generated by text-davinci-003.\n",
    "\n",
    "The following code block downloads the training split from the CodeAlpaca-20k dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c1dcccf-4d23-46af-b3fe-ee5ed0cb6625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 20022\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeda117-9164-4e32-965a-182e93138581",
   "metadata": {},
   "source": [
    "Let's look at the example in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac93014a-23a4-4a25-9204-fcf956566829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create a JavaScript code snippet to get a list of all the elements in an array with even index.',\n",
       " 'input': 'let arr = [1,2,3,4,5,6];',\n",
       " 'output': 'let evenIndexArr = arr.filter((elem, index) => index % 2 === 0);'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2046f0-08c2-4785-b24f-4bcf27f5f1b6",
   "metadata": {},
   "source": [
    "To keep things simple let's just focus on the examples that do not have any `input`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0d5c17-f09d-4669-b588-27686f1e1bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 9764\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda example: example[\"input\"] == '')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e569646c-08e5-4db8-ab12-42d8862cd4e9",
   "metadata": {},
   "source": [
    "The original CodeAlpaca dataset may not have been shuffled. The following line indicates how to shuffle a `datasets.arrow_dataset.Dataset()` object with a random seed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00231f4e-ddf8-4c22-8f95-ec93033169c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a59cf2-0a74-4735-89a7-84e5f2d69db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 9764\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb50cd6f-09a4-4d6b-bdb2-b0263b5c88a8",
   "metadata": {},
   "source": [
    "The CodeAlpaca 20k dataset has a training and test set. You can split the original training data into a train and test set by assigning 80% of the data to the training set and 20% to the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b535950-f801-4208-9520-c7678f401f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 7811\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 1953\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset_split['train']\n",
    "test_dataset = dataset_split['test']\n",
    "dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d566ec50-1250-4dd9-be47-6d1cf3bb563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a small set of data for the resource limitation\n",
    "# This dataset will be only used for evaluation parts, not for the training\n",
    "tiny_test_dataset=test_dataset.select(range(10))\n",
    "tiny_train_dataset=train_dataset.select(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e4e9f2-93b1-4e80-ab15-a8b36fbad7fa",
   "metadata": {},
   "source": [
    "# Model and tokenizer\n",
    "\n",
    "In this exercise, let's fine-tune the [`opt-350m`](https://huggingface.co/facebook/opt-350m) model from Facebook. A description of this OpenSource model was published [here](https://arxiv.org/abs/2205.01068), and the model was originally made available on [metaseq's Github repository](https://github.com/facebookresearch/metaseq).\n",
    "\n",
    "The below lines load the base model from Hugging Face:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dc0cf79-57ff-42bf-8937-bdf94dfc7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d3d936f-69d0-4021-a9e5-61d1c393b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", padding_side='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f58e65-7165-43d5-a2ff-61effcc3278c",
   "metadata": {},
   "source": [
    "Let's find the end of sentence (EOS) token. This is a special tokenizer token. Once this token is encountered, the model will stop generating further tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44936538-9be1-4ac5-9c3d-0ec1e0b54521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c603823-d9c7-4f9f-a586-8eb47dc1cc68",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "To perform the fine-tuning, first, preprocess the data by creating functions that generate the prompt.\n",
    "\n",
    "The `formatting_prompts_func` function takes a dataset as input. For every element in the dataset format, the instruction and the output into a template using the format:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "Translate the following sentence to Spanish: \"Hello, how are you?\"\n",
    "\n",
    "### Response:\n",
    "\"Hola, ¿cómo estás?</s>\"\n",
    "```\n",
    "\n",
    "_**Note:**_ \n",
    "1. The template provided in this section may differ from the **Instruction and output template** presented in the introduction of this lab. You can replace the  `### Response:` with `### Output:` to generate similar results.\n",
    "\n",
    "2. Introducing the `</s>` end of sentence token at the end of the text informs the model to stop generating text beyond this point.\n",
    "\n",
    "Finally, the `formatting_prompts_func_no_response` function behaves similarly to the `formatting_prompts_func` except the response is not included.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04d4221c-0be7-482c-ae7b-0f0adce146f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(mydataset):\n",
    "    output_texts = []\n",
    "    for i in range(len(mydataset['instruction'])):\n",
    "        text = (\n",
    "            f\"### Instruction:\\n{mydataset['instruction'][i]}\"\n",
    "            f\"\\n\\n### Response:\\n{mydataset['output'][i]}</s>\"\n",
    "        )\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "def formatting_prompts_func_no_response(mydataset):\n",
    "    output_texts = []\n",
    "    for i in range(len(mydataset['instruction'])):\n",
    "        text = (\n",
    "            f\"### Instruction:\\n{mydataset['instruction'][i]}\"\n",
    "            f\"\\n\\n### Response:\\n\"\n",
    "        )\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225abd8-8062-4dfa-9cae-d6fd5ba8d821",
   "metadata": {},
   "source": [
    "The following code block generates the `instructions` (the part of the prompt that does not include the response), the `instructions_with_responses` (the full prompt with the response and `eos` token), and the `expected_outputs`, which are the parts of the `instructions_with_responses` that are between the `instructions` and the `eos` token.\n",
    "\n",
    "To find the `expected_outputs`, tokenize `instructions` and the `instructions_with_responses`. Then, count the number of tokens in `instructions`, and discard the equivalent amount of tokens from the beginning of the tokenized `instructions_with_responses` vector. Finally, discard the final token in `instructions_with_responses`, corresponding to the `eos` token. Decode the resulting vector using the tokenizer, resulting in the `expected_output`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44e09cc1-8563-4cb8-87b2-50abfc567fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1953/1953 [00:00<00:00, 3027.61it/s]\n"
     ]
    }
   ],
   "source": [
    "expected_outputs = []\n",
    "\n",
    "instructions_with_responses = formatting_prompts_func(test_dataset)\n",
    "\n",
    "instructions = formatting_prompts_func_no_response(test_dataset)\n",
    "\n",
    "for i in tqdm(range(len(instructions_with_responses))):\n",
    "    tokenized_instruction_with_response = tokenizer(instructions_with_responses[i], return_tensors=\"pt\", max_length=1024, truncation=True, padding=False)\n",
    "    tokenized_instruction = tokenizer(instructions[i], return_tensors=\"pt\")\n",
    "    expected_output = tokenizer.decode(tokenized_instruction_with_response['input_ids'][0][len(tokenized_instruction['input_ids'][0])-1:], skip_special_tokens=True)\n",
    "    expected_outputs.append(expected_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "915c9d96-767f-4ed0-8c9d-f81afa1fa0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction:\\nWhat type of data structure would you use to store key-value pairs in a Python program? Write corresponding code in Python.\\n\\n### Response:\\nThe data structure to use for key-value pairs in Python is a dictionary. A dictionary is a data type that consists of key-value pairs, and is denoted by {} in Python. Each key has a unique value associated with it that can be accessed using the key. For example, a dictionary called \"person\" could look like this: \\n\\nperson = {\\'name\\':\\'John\\', \\'age\\': 32} \\n\\nThe value of the key \"name\" can be accessed using person[\\'name\\'] which returns \"John\".</s>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions_with_responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13bf5e6b-dc0b-43fd-b135-5350ac8b9565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction:\\nWhat type of data structure would you use to store key-value pairs in a Python program? Write corresponding code in Python.\\n\\n### Response:\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9908db-f594-4219-8e83-96e0deda53cd",
   "metadata": {},
   "source": [
    "Let's look at the example to view what `instructions` include, `instructions_with_responses`, and `expected_outputs`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc60f491-1a44-4093-a4a7-c5454676c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## instructions ##############\n",
      "### Instruction:\n",
      "What type of data structure would you use to store key-value pairs in a Python program? Write corresponding code in Python.\n",
      "\n",
      "### Response:\n",
      "\n",
      "############## instructions_with_responses ##############\n",
      "### Instruction:\n",
      "What type of data structure would you use to store key-value pairs in a Python program? Write corresponding code in Python.\n",
      "\n",
      "### Response:\n",
      "The data structure to use for key-value pairs in Python is a dictionary. A dictionary is a data type that consists of key-value pairs, and is denoted by {} in Python. Each key has a unique value associated with it that can be accessed using the key. For example, a dictionary called \"person\" could look like this: \n",
      "\n",
      "person = {'name':'John', 'age': 32} \n",
      "\n",
      "The value of the key \"name\" can be accessed using person['name'] which returns \"John\".</s>\n",
      "\n",
      "############## expected_outputs ##############\n",
      "The data structure to use for key-value pairs in Python is a dictionary. A dictionary is a data type that consists of key-value pairs, and is denoted by {} in Python. Each key has a unique value associated with it that can be accessed using the key. For example, a dictionary called \"person\" could look like this: \n",
      "\n",
      "person = {'name':'John', 'age': 32} \n",
      "\n",
      "The value of the key \"name\" can be accessed using person['name'] which returns \"John\".\n"
     ]
    }
   ],
   "source": [
    "print('############## instructions ##############\\n' + instructions[0])\n",
    "print('############## instructions_with_responses ##############\\n' + instructions_with_responses[0])\n",
    "print('\\n############## expected_outputs ##############' + expected_outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab735723-2282-4fcd-b3d7-cc89224a2e96",
   "metadata": {},
   "source": [
    "Instead of keeping the instructions as-is, it's beneficial to convert the `instructions` list into a `torch` `Dataset`. The following code defines a class called `ListDataset` that inherits from `Dataset` and creates a `torch` `Dataset` from a list. This class is then used to generate a `Dataset` object from `instructions`: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0aa6d8d-2576-4033-bd1f-a5f8a0637f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset):\n",
    "    def __init__(self, original_list):\n",
    "        self.original_list = original_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.original_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.original_list[i]\n",
    "\n",
    "instructions_torch = ListDataset(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70017ab3-c022-4ec6-ab60-9e6a6a1a1771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction:\\nWhat type of data structure would you use to store key-value pairs in a Python program? Write corresponding code in Python.\\n\\n### Response:\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions_torch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3957351-6b3e-43cc-9b7b-afc88be97e24",
   "metadata": {},
   "source": [
    "# Test the base model\n",
    "\n",
    "Let's understand how the base model performs without performing fine-tuning in the model. This may involve response generation from the base, that is from the non-fine-tuned mode. \n",
    "\n",
    "The below code defines a text generation pipeline using the `pipeline` class from `transformers`. This pipeline is useful to generate text given by a model and a tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "968376ea-cc66-4141-9f2f-b6428753c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pipeline = pipeline(\"text-generation\",\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        device=device,\n",
    "                        batch_size=2,\n",
    "                        max_length=50,\n",
    "                        truncation=True,\n",
    "                        padding=False,\n",
    "                        return_full_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc061a03-7e1d-40bf-907d-aae841212232",
   "metadata": {},
   "source": [
    "**_Note:_** The generation pipeline can generate tokens or text. If```return_tensors=True```, the pipeline returns token IDs; otherwise, it returns words. Additionally, the generation pipeline generates both the instructions *and* the responses by default. However, to assess the model's performance, exclude the generated instructions and focus on the responses. To do this, set ```return_full_text=False```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9126e-dd66-4cda-87ce-e0ea1bd52bdf",
   "metadata": {},
   "source": [
    "The below code leverages the pre-defined generation pipeline to generate outputs using the model. \n",
    "\n",
    "**_Note:_** The code is commented out because it may take a long time for CPU. Instead of generating the raw tokens here, you can load output from this model later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b783991c-9fb9-4099-b104-cd337d770c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Due to resource limitation, only apply the function on 3 records using \"instructions_torch[:10]\"\n",
    "    pipeline_iterator= gen_pipeline(instructions_torch[:3], \n",
    "                                    max_length=50, # this is set to 50 due to resource constraint, using a GPU, you can increase it to the length of your choice\n",
    "                                    num_beams=5,\n",
    "                                    early_stopping=True,)\n",
    "\n",
    "generated_outputs_base = []\n",
    "for text in pipeline_iterator:\n",
    "    generated_outputs_base.append(text[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29e6c204-a4c2-4232-927e-8c2e955b6b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What type of data structure would you use to store key-value pairs',\n",
       " 'This is an example of a method to solve an equation of the form',\n",
       " 'The CSS rule is set to “big-header”']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_outputs_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e2ed999-98e5-4810-9175-4096b61b96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VvQRrSqS1P0_GobqtL-SKA/instruction-tuning-generated-outputs-base.pkl')\n",
    "# generated_outputs_base = pickle.load(io.BytesIO(urlopened.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb9dce-6b8f-4d45-90cb-05194236751e",
   "metadata": {},
   "source": [
    "Let's look at the sample responses generated by the base model and the expected responses from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25bd0500-3d8c-4317-8693-f01b8d155013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@ Instruction 1: \n",
      "### Instruction:\n",
      "What type of data structure would you use to store key-value pairs in a Python program? Write corresponding code in Python.\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@@@@@ Expected response 1: \n",
      "\n",
      "The data structure to use for key-value pairs in Python is a dictionary. A dictionary is a data type that consists of key-value pairs, and is denoted by {} in Python. Each key has a unique value associated with it that can be accessed using the key. For example, a dictionary called \"person\" could look like this: \n",
      "\n",
      "person = {'name':'John', 'age': 32} \n",
      "\n",
      "The value of the key \"name\" can be accessed using person['name'] which returns \"John\".\n",
      "\n",
      "\n",
      "\n",
      "@@@@@ Generated response 1: \n",
      "What type of data structure would you use to store key-value pairs\n",
      "\n",
      "\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@ Instruction 2: \n",
      "### Instruction:\n",
      "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@@@@@ Expected response 2: \n",
      "\n",
      "The equation ax + b = 0 can be solved by subtracting b from both sides and then dividing both sides by a. This will yield the solution x = -b/a.\n",
      "\n",
      "\n",
      "\n",
      "@@@@@ Generated response 2: \n",
      "This is an example of a method to solve an equation of the form\n",
      "\n",
      "\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@ Instruction 3: \n",
      "### Instruction:\n",
      "Write a CSS rule to set the text size of all elements with the class “big-header” to 24px.\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@@@@@ Expected response 3: \n",
      "\n",
      ".big-header {\n",
      "    font-size: 24px;\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "@@@@@ Generated response 3: \n",
      "The CSS rule is set to “big-header”\n",
      "\n",
      "\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('@@@@@@@@@@@@@@@@@@@@')\n",
    "    print('@@@@@ Instruction '+ str(i+1) +': ')\n",
    "    print(instructions[i])\n",
    "    print('\\n\\n')\n",
    "    print('@@@@@ Expected response '+ str(i+1) +': ')\n",
    "    print(expected_outputs[i])\n",
    "    print('\\n\\n')\n",
    "    print('@@@@@ Generated response '+ str(i+1) +': ')\n",
    "    print(generated_outputs_base[i])\n",
    "    print('\\n\\n')\n",
    "    print('@@@@@@@@@@@@@@@@@@@@')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ce1df9-4ac0-4ffd-8c7d-037ac4d8e84d",
   "metadata": {},
   "source": [
    "You can see that the responses generated by the base model are not up to the mark. Also, the responses have the tendency to extend and repeat the answers until they generate the maximum number of tokens. Later on, you can see that the instruction-tuning can fix both of these issues. First, the instruction fine-tuned model will be able to provide more meaningful responses. Second, because, you appended the `eos` token `<\\s>` to the output, you will teach the model via instruction fine-tuning to not generate responses without bound.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157e240-02f3-4833-9bb2-d3af69c7defa",
   "metadata": {},
   "source": [
    "## BLEU score\n",
    "\n",
    "Let's set up a metric that compares the generated responses and the expected responses in the test environment. In this lab, let's use the [BLEU score](https://en.wikipedia.org/wiki/BLEU), a metric originally intended to check the quality of translations made by translation models. You can calculate the BLEU scores for individual generated segments by comparing them with a set of expected outputs and average the scores for the individual segments. Depending on the implementation, BLEU scores range from 0 to 1 or from 0 to 100 (as in the implementation used herein), with higher scores indicating a better match between the model generated output and the expected output.\n",
    "\n",
    "_**Note:**_ \n",
    "1. The BLEU score was originally implemented for assessing the quality of translations. However, it may not necessarily be the best metric for instruction fine-tuning in general, but it is nonetheless a useful metric that gives a sense of the alignment between the model generated output and the expected output.\n",
    "2. BLEU scores are very challenging to compare from one study to the next because it is a parametrized metric. As a result, you can employ a variant of BLEU called [SacreBLEU](https://aclanthology.org/W18-6319/) invariant to the metric's parametrization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6841c64a-a51c-45d7-809b-10550b5c223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 3\n",
      "Number of references: 3\n",
      "['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load sacrebleu metric\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "# Debugging: Print lengths of predictions and references\n",
    "print(f\"Number of predictions: {len(generated_outputs_base)}\")\n",
    "print(f\"Number of references: {len(expected_outputs)}\")\n",
    "\n",
    "# Align lengths if necessary\n",
    "if len(generated_outputs_base) != len(expected_outputs):\n",
    "    min_len = min(len(generated_outputs_base), len(expected_outputs))\n",
    "    generated_outputs_base = generated_outputs_base[:min_len]\n",
    "    expected_outputs = expected_outputs[:min_len]\n",
    "\n",
    "# Ensure references are wrapped in a list\n",
    "expected_outputs = [[ref] for ref in expected_outputs]\n",
    "\n",
    "# Compute sacrebleu metric\n",
    "results_base = sacrebleu.compute(predictions=generated_outputs_base,\n",
    "                                 references=expected_outputs)\n",
    "\n",
    "print(list(results_base.keys()))\n",
    "print(round(results_base[\"score\"], 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c9697-06a9-41ec-940c-e9643a1b8e8f",
   "metadata": {},
   "source": [
    "The SacreBLEU score of 0.4/100 indicates that there is very little alignment between the base model's generated responses and the expected responses for the examples in the test dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c216d5-da8a-4ec4-8047-4ce9a9740dde",
   "metadata": {},
   "source": [
    "## Perform instruction fine-tuning with LoRA\n",
    "\n",
    "To save time, let's perform instruction fine-tuning using a parameter-efficient fine-tuning (PEFT) method called low-rank adaptation (LoRA).\n",
    "First, convert the model into a PEFT model suitable for LoRA fine-tuning by defining a `LoraConfig` object from the `peft` library that outlines LoRA parameters, such as the LoRA rank and the target modules. Next, apply LoRA configuration on the model using `get_peft_model()`, which effectively converts `model` into a LoRA `model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93e70b95-5060-4b38-9e1e-5b031678e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,  # Low-rank dimension\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    task_type=TaskType.CAUSAL_LM  # Task type should be causal language model\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f734d8c-22f8-4c04-b1e4-d95f0f114ede",
   "metadata": {},
   "source": [
    "Instruction fine-tuning using the `SFTTrainer` has the effect of generating the instructions *and* the responses. However, for the purposes of assessing the quality of the generated text, consider only the quality of the response and not the quality of the instruction. For the purposes of calculating the BLEU score, eliminate the length of tokens corresponding to the instruction from the beginning of the tokenized model output. \n",
    "\n",
    "For example, suppose the tokenized instruction had a length of ten, but the generated text had a length of fourteen. Then the tokenized response that was kept for the purposes of calculating the BLEU score was just the four tokens at the end of the tokenized generated text because the first ten tokens represent the model's generation of the tokenized instruction.\n",
    "\n",
    "Although eliminating the first few tokens of the tokenized output worked well for the purposes of calculating BLEU. However, during fine-tuning, the first few tokens won't have an impact on the loss function. You can mask those tokens using -100 by ignoring the value of PyTorch loss functions such as [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). By masking the tokens corresponding to the instruction with -100, only the tokens associated with the response can bear the loss.\n",
    "\n",
    "You can create such a masking manually by defining your own function. However, it is easier to instead use the `DataCollatorForCompletionOnlyLM` class from `trl`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e43671ef-436c-43ac-b16c-a2fc115e7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"### Response:\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d790032-f5d0-4707-a1ff-cebe8253b10c",
   "metadata": {},
   "source": [
    "Now, pass the `collator`, `DataCollatorForCompletionOnlyLM` object to the data collator into `SFTTrainer`, resulting in the generated instructions without bearing on the loss.\n",
    "\n",
    "To perform the training, first configure our `SFTTrainer`, and create the `SFTTrainer` object by passing to the `collator`:\n",
    "\n",
    "```python\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"/tmp\",\n",
    "    num_train_epochs=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=2,  # Reduce batch size\n",
    "    per_device_eval_batch_size=2,  # Reduce batch size\n",
    "    max_seq_length=1024,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    "    data_collator=collator,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad571053-279f-4bd0-ad67-636ad8d215ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Memory-Efficient Features\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"/tmp\",\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,  # Mixed precision training\n",
    "    per_device_train_batch_size=4,  # Reduce if needed\n",
    "    per_device_eval_batch_size=2,  # Reduce if needed\n",
    "    max_seq_length=1024,\n",
    "    do_eval=True,\n",
    "    gradient_accumulation_steps=4,  # Simulates a batch size of 16\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    "    data_collator=collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19eb4d1-caea-41f8-8b61-28071d377b4c",
   "metadata": {},
   "source": [
    "Please ignore the above warning.\n",
    "The below comments, runs the trainer, because this would take a long time on the CPU. Therefore, let's not run the trainer here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d715636b-a327-4596-bec7-f23401bb9796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1464' max='1464' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1464/1464 1:36:14, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.654800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1464, training_loss=1.6827167031543502, metrics={'train_runtime': 5776.1949, 'train_samples_per_second': 4.057, 'train_steps_per_second': 0.253, 'total_flos': 9288178628542464.0, 'train_loss': 1.6827167031543502, 'epoch': 2.998463901689708})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e4545-9a46-4c9b-a3bc-6630939d0849",
   "metadata": {},
   "source": [
    "If you want to train the trainer, the `trainer` object would have a state history for every training step. You would be able to access this state history using the below commented out line:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c072b02b-a976-430b-8b86-da0f87cda215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 1.7696,\n",
       "  'grad_norm': 6.382034778594971,\n",
       "  'learning_rate': 3.2957650273224044e-05,\n",
       "  'epoch': 1.0240655401945724,\n",
       "  'step': 500},\n",
       " {'loss': 1.6548,\n",
       "  'grad_norm': 6.261003017425537,\n",
       "  'learning_rate': 1.5881147540983607e-05,\n",
       "  'epoch': 2.048131080389145,\n",
       "  'step': 1000},\n",
       " {'train_runtime': 5776.1949,\n",
       "  'train_samples_per_second': 4.057,\n",
       "  'train_steps_per_second': 0.253,\n",
       "  'total_flos': 9288178628542464.0,\n",
       "  'train_loss': 1.6827167031543502,\n",
       "  'epoch': 2.998463901689708,\n",
       "  'step': 1464}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_history_lora = trainer.state.log_history\n",
    "log_history_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8656ded7-cab9-4a80-a0c1-1e47447ccadb",
   "metadata": {},
   "source": [
    "Instead of extracting the state history above, let's load the state history of a model that was instruction fine-tuned to the above specifications on a GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afff87a6-6cfc-4142-82e9-368a72881111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 1.964,\n",
       "  'grad_norm': 3.12174391746521,\n",
       "  'learning_rate': 4.936251920122888e-05,\n",
       "  'epoch': 0.12800819252432155,\n",
       "  'step': 500},\n",
       " {'loss': 1.7329,\n",
       "  'grad_norm': 3.3765358924865723,\n",
       "  'learning_rate': 4.872247823860727e-05,\n",
       "  'epoch': 0.2560163850486431,\n",
       "  'step': 1000},\n",
       " {'loss': 1.731,\n",
       "  'grad_norm': 4.632175922393799,\n",
       "  'learning_rate': 4.808371735791091e-05,\n",
       "  'epoch': 0.38402457757296465,\n",
       "  'step': 1500},\n",
       " {'loss': 1.7693,\n",
       "  'grad_norm': 3.1081297397613525,\n",
       "  'learning_rate': 4.74436763952893e-05,\n",
       "  'epoch': 0.5120327700972862,\n",
       "  'step': 2000},\n",
       " {'loss': 1.6826,\n",
       "  'grad_norm': 1.7782740592956543,\n",
       "  'learning_rate': 4.680363543266769e-05,\n",
       "  'epoch': 0.6400409626216078,\n",
       "  'step': 2500},\n",
       " {'loss': 1.7027,\n",
       "  'grad_norm': 1.7686882019042969,\n",
       "  'learning_rate': 4.6163594470046084e-05,\n",
       "  'epoch': 0.7680491551459293,\n",
       "  'step': 3000},\n",
       " {'loss': 1.6579,\n",
       "  'grad_norm': 8.598638534545898,\n",
       "  'learning_rate': 4.5523553507424476e-05,\n",
       "  'epoch': 0.8960573476702509,\n",
       "  'step': 3500},\n",
       " {'loss': 1.6234,\n",
       "  'grad_norm': 3.587522506713867,\n",
       "  'learning_rate': 4.488351254480287e-05,\n",
       "  'epoch': 1.0240655401945724,\n",
       "  'step': 4000},\n",
       " {'loss': 1.6217,\n",
       "  'grad_norm': 2.751554012298584,\n",
       "  'learning_rate': 4.424347158218126e-05,\n",
       "  'epoch': 1.1520737327188941,\n",
       "  'step': 4500},\n",
       " {'loss': 1.6084,\n",
       "  'grad_norm': 4.4422502517700195,\n",
       "  'learning_rate': 4.360343061955965e-05,\n",
       "  'epoch': 1.2800819252432156,\n",
       "  'step': 5000},\n",
       " {'loss': 1.595,\n",
       "  'grad_norm': 2.6668152809143066,\n",
       "  'learning_rate': 4.2963389656938045e-05,\n",
       "  'epoch': 1.4080901177675371,\n",
       "  'step': 5500},\n",
       " {'loss': 1.59,\n",
       "  'grad_norm': 8.379820823669434,\n",
       "  'learning_rate': 4.232334869431644e-05,\n",
       "  'epoch': 1.5360983102918588,\n",
       "  'step': 6000},\n",
       " {'loss': 1.5955,\n",
       "  'grad_norm': 7.10015344619751,\n",
       "  'learning_rate': 4.1684587813620074e-05,\n",
       "  'epoch': 1.66410650281618,\n",
       "  'step': 6500},\n",
       " {'loss': 1.5308,\n",
       "  'grad_norm': 4.479045391082764,\n",
       "  'learning_rate': 4.1044546850998466e-05,\n",
       "  'epoch': 1.7921146953405018,\n",
       "  'step': 7000},\n",
       " {'loss': 1.4971,\n",
       "  'grad_norm': 6.752324104309082,\n",
       "  'learning_rate': 4.040450588837686e-05,\n",
       "  'epoch': 1.9201228878648233,\n",
       "  'step': 7500},\n",
       " {'loss': 1.5255,\n",
       "  'grad_norm': 2.9181501865386963,\n",
       "  'learning_rate': 3.976446492575525e-05,\n",
       "  'epoch': 2.048131080389145,\n",
       "  'step': 8000},\n",
       " {'loss': 1.5236,\n",
       "  'grad_norm': 2.2777297496795654,\n",
       "  'learning_rate': 3.912442396313364e-05,\n",
       "  'epoch': 2.1761392729134665,\n",
       "  'step': 8500},\n",
       " {'loss': 1.5108,\n",
       "  'grad_norm': 4.440834999084473,\n",
       "  'learning_rate': 3.8484383000512035e-05,\n",
       "  'epoch': 2.3041474654377883,\n",
       "  'step': 9000},\n",
       " {'loss': 1.4926,\n",
       "  'grad_norm': 2.7909915447235107,\n",
       "  'learning_rate': 3.784690220174091e-05,\n",
       "  'epoch': 2.4321556579621095,\n",
       "  'step': 9500},\n",
       " {'loss': 1.5447,\n",
       "  'grad_norm': 9.184590339660645,\n",
       "  'learning_rate': 3.72068612391193e-05,\n",
       "  'epoch': 2.5601638504864312,\n",
       "  'step': 10000},\n",
       " {'loss': 1.478,\n",
       "  'grad_norm': 4.387234687805176,\n",
       "  'learning_rate': 3.65668202764977e-05,\n",
       "  'epoch': 2.688172043010753,\n",
       "  'step': 10500},\n",
       " {'loss': 1.5011,\n",
       "  'grad_norm': 10.248931884765625,\n",
       "  'learning_rate': 3.5926779313876094e-05,\n",
       "  'epoch': 2.8161802355350742,\n",
       "  'step': 11000},\n",
       " {'loss': 1.448,\n",
       "  'grad_norm': 2.0514562129974365,\n",
       "  'learning_rate': 3.5286738351254486e-05,\n",
       "  'epoch': 2.944188428059396,\n",
       "  'step': 11500},\n",
       " {'loss': 1.46,\n",
       "  'grad_norm': 6.339014053344727,\n",
       "  'learning_rate': 3.464669738863288e-05,\n",
       "  'epoch': 3.0721966205837172,\n",
       "  'step': 12000},\n",
       " {'loss': 1.4608,\n",
       "  'grad_norm': 7.504676342010498,\n",
       "  'learning_rate': 3.400665642601127e-05,\n",
       "  'epoch': 3.200204813108039,\n",
       "  'step': 12500},\n",
       " {'loss': 1.4655,\n",
       "  'grad_norm': 3.5388448238372803,\n",
       "  'learning_rate': 3.336661546338966e-05,\n",
       "  'epoch': 3.32821300563236,\n",
       "  'step': 13000},\n",
       " {'loss': 1.4318,\n",
       "  'grad_norm': 3.131237745285034,\n",
       "  'learning_rate': 3.272785458269329e-05,\n",
       "  'epoch': 3.456221198156682,\n",
       "  'step': 13500},\n",
       " {'loss': 1.404,\n",
       "  'grad_norm': 4.16353702545166,\n",
       "  'learning_rate': 3.208909370199693e-05,\n",
       "  'epoch': 3.5842293906810037,\n",
       "  'step': 14000},\n",
       " {'loss': 1.4679,\n",
       "  'grad_norm': 5.939687252044678,\n",
       "  'learning_rate': 3.144905273937532e-05,\n",
       "  'epoch': 3.712237583205325,\n",
       "  'step': 14500},\n",
       " {'loss': 1.4433,\n",
       "  'grad_norm': 4.750664234161377,\n",
       "  'learning_rate': 3.0809011776753715e-05,\n",
       "  'epoch': 3.8402457757296466,\n",
       "  'step': 15000},\n",
       " {'loss': 1.4287,\n",
       "  'grad_norm': 2.0621659755706787,\n",
       "  'learning_rate': 3.0168970814132107e-05,\n",
       "  'epoch': 3.9682539682539684,\n",
       "  'step': 15500},\n",
       " {'loss': 1.4652,\n",
       "  'grad_norm': 3.1704111099243164,\n",
       "  'learning_rate': 2.95289298515105e-05,\n",
       "  'epoch': 4.09626216077829,\n",
       "  'step': 16000},\n",
       " {'loss': 1.4149,\n",
       "  'grad_norm': 5.702635288238525,\n",
       "  'learning_rate': 2.8888888888888888e-05,\n",
       "  'epoch': 4.224270353302611,\n",
       "  'step': 16500},\n",
       " {'loss': 1.4037,\n",
       "  'grad_norm': 3.3385021686553955,\n",
       "  'learning_rate': 2.824884792626728e-05,\n",
       "  'epoch': 4.352278545826933,\n",
       "  'step': 17000},\n",
       " {'loss': 1.3626,\n",
       "  'grad_norm': 6.326888084411621,\n",
       "  'learning_rate': 2.761008704557092e-05,\n",
       "  'epoch': 4.480286738351254,\n",
       "  'step': 17500},\n",
       " {'loss': 1.4159,\n",
       "  'grad_norm': 5.192262649536133,\n",
       "  'learning_rate': 2.6970046082949306e-05,\n",
       "  'epoch': 4.6082949308755765,\n",
       "  'step': 18000},\n",
       " {'loss': 1.4483,\n",
       "  'grad_norm': 3.092099666595459,\n",
       "  'learning_rate': 2.63300051203277e-05,\n",
       "  'epoch': 4.736303123399898,\n",
       "  'step': 18500},\n",
       " {'loss': 1.4334,\n",
       "  'grad_norm': 4.8964691162109375,\n",
       "  'learning_rate': 2.568996415770609e-05,\n",
       "  'epoch': 4.864311315924219,\n",
       "  'step': 19000},\n",
       " {'loss': 1.3837,\n",
       "  'grad_norm': 2.806743621826172,\n",
       "  'learning_rate': 2.5051203277009728e-05,\n",
       "  'epoch': 4.99231950844854,\n",
       "  'step': 19500},\n",
       " {'loss': 1.3824,\n",
       "  'grad_norm': 10.990423202514648,\n",
       "  'learning_rate': 2.4411162314388124e-05,\n",
       "  'epoch': 5.1203277009728625,\n",
       "  'step': 20000},\n",
       " {'loss': 1.3665,\n",
       "  'grad_norm': 3.0947678089141846,\n",
       "  'learning_rate': 2.3771121351766516e-05,\n",
       "  'epoch': 5.248335893497184,\n",
       "  'step': 20500},\n",
       " {'loss': 1.3848,\n",
       "  'grad_norm': 7.325835704803467,\n",
       "  'learning_rate': 2.3131080389144908e-05,\n",
       "  'epoch': 5.376344086021505,\n",
       "  'step': 21000},\n",
       " {'loss': 1.3975,\n",
       "  'grad_norm': 8.700212478637695,\n",
       "  'learning_rate': 2.24910394265233e-05,\n",
       "  'epoch': 5.504352278545827,\n",
       "  'step': 21500},\n",
       " {'loss': 1.378,\n",
       "  'grad_norm': 4.010356426239014,\n",
       "  'learning_rate': 2.185099846390169e-05,\n",
       "  'epoch': 5.6323604710701485,\n",
       "  'step': 22000},\n",
       " {'loss': 1.367,\n",
       "  'grad_norm': 7.513988971710205,\n",
       "  'learning_rate': 2.121095750128008e-05,\n",
       "  'epoch': 5.76036866359447,\n",
       "  'step': 22500},\n",
       " {'loss': 1.3576,\n",
       "  'grad_norm': 4.6471381187438965,\n",
       "  'learning_rate': 2.057219662058372e-05,\n",
       "  'epoch': 5.888376856118792,\n",
       "  'step': 23000},\n",
       " {'loss': 1.3712,\n",
       "  'grad_norm': 7.861447811126709,\n",
       "  'learning_rate': 1.993215565796211e-05,\n",
       "  'epoch': 6.016385048643113,\n",
       "  'step': 23500},\n",
       " {'loss': 1.3552,\n",
       "  'grad_norm': 6.154510498046875,\n",
       "  'learning_rate': 1.9292114695340503e-05,\n",
       "  'epoch': 6.1443932411674345,\n",
       "  'step': 24000},\n",
       " {'loss': 1.3502,\n",
       "  'grad_norm': 4.454662322998047,\n",
       "  'learning_rate': 1.8652073732718895e-05,\n",
       "  'epoch': 6.272401433691757,\n",
       "  'step': 24500},\n",
       " {'loss': 1.355,\n",
       "  'grad_norm': 7.043622970581055,\n",
       "  'learning_rate': 1.801331285202253e-05,\n",
       "  'epoch': 6.400409626216078,\n",
       "  'step': 25000},\n",
       " {'loss': 1.3468,\n",
       "  'grad_norm': 5.059154987335205,\n",
       "  'learning_rate': 1.737327188940092e-05,\n",
       "  'epoch': 6.528417818740399,\n",
       "  'step': 25500},\n",
       " {'loss': 1.3584,\n",
       "  'grad_norm': 12.197810173034668,\n",
       "  'learning_rate': 1.6733230926779313e-05,\n",
       "  'epoch': 6.65642601126472,\n",
       "  'step': 26000},\n",
       " {'loss': 1.3613,\n",
       "  'grad_norm': 3.9760193824768066,\n",
       "  'learning_rate': 1.6093189964157706e-05,\n",
       "  'epoch': 6.784434203789043,\n",
       "  'step': 26500},\n",
       " {'loss': 1.3497,\n",
       "  'grad_norm': 2.869044542312622,\n",
       "  'learning_rate': 1.5453149001536098e-05,\n",
       "  'epoch': 6.912442396313364,\n",
       "  'step': 27000},\n",
       " {'loss': 1.3495,\n",
       "  'grad_norm': 15.580376625061035,\n",
       "  'learning_rate': 1.4814388120839733e-05,\n",
       "  'epoch': 7.040450588837686,\n",
       "  'step': 27500},\n",
       " {'loss': 1.3176,\n",
       "  'grad_norm': 4.617693901062012,\n",
       "  'learning_rate': 1.4174347158218127e-05,\n",
       "  'epoch': 7.168458781362007,\n",
       "  'step': 28000},\n",
       " {'loss': 1.3412,\n",
       "  'grad_norm': 4.001060962677002,\n",
       "  'learning_rate': 1.353430619559652e-05,\n",
       "  'epoch': 7.296466973886329,\n",
       "  'step': 28500},\n",
       " {'loss': 1.3479,\n",
       "  'grad_norm': 6.942239284515381,\n",
       "  'learning_rate': 1.2894265232974912e-05,\n",
       "  'epoch': 7.42447516641065,\n",
       "  'step': 29000},\n",
       " {'loss': 1.3487,\n",
       "  'grad_norm': 4.121782302856445,\n",
       "  'learning_rate': 1.2254224270353302e-05,\n",
       "  'epoch': 7.552483358934972,\n",
       "  'step': 29500},\n",
       " {'loss': 1.3643,\n",
       "  'grad_norm': 4.143219947814941,\n",
       "  'learning_rate': 1.1614183307731694e-05,\n",
       "  'epoch': 7.680491551459293,\n",
       "  'step': 30000},\n",
       " {'loss': 1.3198,\n",
       "  'grad_norm': 3.9026846885681152,\n",
       "  'learning_rate': 1.0974142345110088e-05,\n",
       "  'epoch': 7.808499743983615,\n",
       "  'step': 30500},\n",
       " {'loss': 1.2892,\n",
       "  'grad_norm': 9.509966850280762,\n",
       "  'learning_rate': 1.0335381464413722e-05,\n",
       "  'epoch': 7.936507936507937,\n",
       "  'step': 31000},\n",
       " {'loss': 1.3673,\n",
       "  'grad_norm': 2.903964042663574,\n",
       "  'learning_rate': 9.695340501792114e-06,\n",
       "  'epoch': 8.064516129032258,\n",
       "  'step': 31500},\n",
       " {'loss': 1.3111,\n",
       "  'grad_norm': 11.67066764831543,\n",
       "  'learning_rate': 9.055299539170507e-06,\n",
       "  'epoch': 8.19252432155658,\n",
       "  'step': 32000},\n",
       " {'loss': 1.3659,\n",
       "  'grad_norm': 8.069602012634277,\n",
       "  'learning_rate': 8.4152585765489e-06,\n",
       "  'epoch': 8.3205325140809,\n",
       "  'step': 32500},\n",
       " {'loss': 1.2713,\n",
       "  'grad_norm': 5.875398635864258,\n",
       "  'learning_rate': 7.775217613927293e-06,\n",
       "  'epoch': 8.448540706605222,\n",
       "  'step': 33000},\n",
       " {'loss': 1.3478,\n",
       "  'grad_norm': 3.7769482135772705,\n",
       "  'learning_rate': 7.136456733230927e-06,\n",
       "  'epoch': 8.576548899129545,\n",
       "  'step': 33500},\n",
       " {'loss': 1.3047,\n",
       "  'grad_norm': 3.0720221996307373,\n",
       "  'learning_rate': 6.4964157706093195e-06,\n",
       "  'epoch': 8.704557091653866,\n",
       "  'step': 34000},\n",
       " {'loss': 1.3225,\n",
       "  'grad_norm': 10.471675872802734,\n",
       "  'learning_rate': 5.856374807987712e-06,\n",
       "  'epoch': 8.832565284178187,\n",
       "  'step': 34500},\n",
       " {'loss': 1.3018,\n",
       "  'grad_norm': 9.490659713745117,\n",
       "  'learning_rate': 5.216333845366104e-06,\n",
       "  'epoch': 8.960573476702509,\n",
       "  'step': 35000},\n",
       " {'loss': 1.3171,\n",
       "  'grad_norm': 3.6835718154907227,\n",
       "  'learning_rate': 4.577572964669739e-06,\n",
       "  'epoch': 9.08858166922683,\n",
       "  'step': 35500},\n",
       " {'loss': 1.3228,\n",
       "  'grad_norm': 5.009510517120361,\n",
       "  'learning_rate': 3.937532002048131e-06,\n",
       "  'epoch': 9.216589861751151,\n",
       "  'step': 36000},\n",
       " {'loss': 1.3023,\n",
       "  'grad_norm': 2.638444662094116,\n",
       "  'learning_rate': 3.2974910394265234e-06,\n",
       "  'epoch': 9.344598054275474,\n",
       "  'step': 36500},\n",
       " {'loss': 1.2848,\n",
       "  'grad_norm': 11.018571853637695,\n",
       "  'learning_rate': 2.6574500768049156e-06,\n",
       "  'epoch': 9.472606246799796,\n",
       "  'step': 37000},\n",
       " {'loss': 1.3147,\n",
       "  'grad_norm': 9.507741928100586,\n",
       "  'learning_rate': 2.017409114183308e-06,\n",
       "  'epoch': 9.600614439324117,\n",
       "  'step': 37500},\n",
       " {'loss': 1.3056,\n",
       "  'grad_norm': 4.325129985809326,\n",
       "  'learning_rate': 1.3786482334869433e-06,\n",
       "  'epoch': 9.728622631848438,\n",
       "  'step': 38000},\n",
       " {'loss': 1.316,\n",
       "  'grad_norm': 3.9033706188201904,\n",
       "  'learning_rate': 7.386072708653353e-07,\n",
       "  'epoch': 9.85663082437276,\n",
       "  'step': 38500},\n",
       " {'loss': 1.371,\n",
       "  'grad_norm': 3.2345354557037354,\n",
       "  'learning_rate': 9.856630824372762e-08,\n",
       "  'epoch': 9.98463901689708,\n",
       "  'step': 39000},\n",
       " {'train_runtime': 3414.5774,\n",
       "  'train_samples_per_second': 22.875,\n",
       "  'train_steps_per_second': 11.439,\n",
       "  'total_flos': 2.2898070968303616e+16,\n",
       "  'train_loss': 1.4369741710344461,\n",
       "  'epoch': 10.0,\n",
       "  'step': 39060}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/49I70jQD0-RNRg2v-eOoxg/instruction-tuning-log-history-lora.json')\n",
    "log_history_lora = json.load(io.BytesIO(urlopened.read()))\n",
    "log_history_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4ed4406-43e7-457e-9575-9ffbe9643da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Due to resource limitation, only apply the function on 3 records using \"instructions_torch[:10]\"\n",
    "    pipeline_iterator= gen_pipeline(instructions_torch[:3], \n",
    "                                    max_length=50, # this is set to 50 due to resource constraint, using a GPU, you can increase it to the length of your choice\n",
    "                                    num_beams=5,\n",
    "                                    early_stopping=True,)\n",
    "\n",
    "generated_outputs_lora = []\n",
    "for text in pipeline_iterator:\n",
    "    generated_outputs_lora.append(text[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867bc306-1fac-4f25-b518-e4b4cd19157c",
   "metadata": {},
   "source": [
    "Let's have a look at some of the responses from the instruction fine-tuned model and the expected responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5a861e3-fa3f-47d4-b759-0d03d8ab8094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@ Instruction 1: \n",
      "### Instruction:\n",
      "What type of data structure would you use to store key-value pairs in a Python program? Write corresponding code in Python.\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@@@@@ Expected response 1: \n",
      "[['\\nThe data structure to use for key-value pairs in Python is a dictionary. A dictionary is a data type that consists of key-value pairs, and is denoted by {} in Python. Each key has a unique value associated with it that can be accessed using the key. For example, a dictionary called \"person\" could look like this: \\n\\nperson = {\\'name\\':\\'John\\', \\'age\\': 32} \\n\\nThe value of the key \"name\" can be accessed using person[\\'name\\'] which returns \"John\".']]\n",
      "\n",
      "\n",
      "\n",
      "@@@@@ Generated response 1: \n",
      "The key-value pairs in a Python program can be stored in a\n",
      "\n",
      "\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@ Instruction 2: \n",
      "### Instruction:\n",
      "Describe a method to solve an equation of the form ax + b = 0. Write corresponding code in Python.\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@@@@@ Expected response 2: \n",
      "[['\\nThe equation ax + b = 0 can be solved by subtracting b from both sides and then dividing both sides by a. This will yield the solution x = -b/a.']]\n",
      "\n",
      "\n",
      "\n",
      "@@@@@ Generated response 2: \n",
      "A method to solve an equation of the form ax + b = 0\n",
      "\n",
      "\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@ Instruction 3: \n",
      "### Instruction:\n",
      "Write a CSS rule to set the text size of all elements with the class “big-header” to 24px.\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@@@@@ Expected response 3: \n",
      "[['\\n.big-header {\\n    font-size: 24px;\\n}']]\n",
      "\n",
      "\n",
      "\n",
      "@@@@@ Generated response 3: \n",
      "def big-header(size: 24px):\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('@@@@@@@@@@@@@@@@@@@@')\n",
    "    print('@@@@@ Instruction '+ str(i+1) +': ')\n",
    "    print(instructions[i])\n",
    "    print('\\n\\n')\n",
    "    print('@@@@@ Expected response '+ str(i+1) +': ')\n",
    "    print(expected_outputs[i])\n",
    "    print('\\n\\n')\n",
    "    print('@@@@@ Generated response '+ str(i+1) +': ')\n",
    "    print(generated_outputs_lora[i])\n",
    "    print('\\n\\n')\n",
    "    print('@@@@@@@@@@@@@@@@@@@@')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adbe3f69-6cc4-4e93-bd30-aebf7c7d1dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "results_lora = sacrebleu.compute(predictions=generated_outputs_lora,\n",
    "                                 references=expected_outputs)\n",
    "print(list(results_lora.keys()))\n",
    "print(round(results_lora[\"score\"], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c572ab-7377-4dc6-be97-cccb7e609923",
   "metadata": {},
   "source": [
    "You can see that the fine-tuned model achieves a SacreBLEU score of 14.7/100, significantly better than the 0.4/100 achieved by the base model. \n",
    "\n",
    "Let's conclude. The instruction fine-tuned model generates responses that align much better with the expected responses in the dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814ef07-fd2e-4ebd-b2b5-c3a1009f4ab5",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29094d91-cddd-4328-b958-48b8baebf4a8",
   "metadata": {},
   "source": [
    "### Exercise 1: Try with another response template (Question-Answering)\n",
    "\n",
    "Create a `formatting_prompts_response_template` function to format the train_dataset in the Response Template. \n",
    "\n",
    "Template: `### Question: {question}\\n ### Answer: {answer}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7b92f-d288-4c49-891b-9578701111a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here\n",
    "def formatting_prompts_response_template(mydataset):\n",
    "    output_texts = []\n",
    "    for i in range(len(mydataset['instruction'])):\n",
    "        text = (\n",
    "            f\"### Question:\\n{mydataset['instruction'][i]}\"\n",
    "            f\"\\n\\n### Answer:\\n{mydataset['output'][i]}</s>\"\n",
    "        )\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6571beb4-a42a-4ed2-946f-0d8449c46b54",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "def formatting_prompts_response_template(mydataset):\n",
    "    output_texts = []\n",
    "    for i in range(len(mydataset['instruction'])):\n",
    "        text = (\n",
    "            f\"### Question:\\n{mydataset['instruction'][i]}\"\n",
    "            f\"\\n\\n### Answer:\\n{mydataset['output'][i]}</s>\"\n",
    "        )\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d540c-85ce-4065-84c0-8f5dfb9c76cf",
   "metadata": {},
   "source": [
    "Create a `formatting_prompts_response_template_no_response` function to format the `test_dataset` in the Response Template, excluding the response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ff075-520a-4d8d-9349-34901abe7f9d",
   "metadata": {},
   "source": [
    "Template: `### Question: {question}\\n ### Answer: `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c5a35-f284-406d-b111-215aa4287318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999cef38-5c0b-4f8e-a252-62e8a88d6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_response_template_no_response(mydataset):\n",
    "    output_texts = []\n",
    "    for i in range(len(mydataset['instruction'])):\n",
    "        text = (\n",
    "            f\"### Question:\\n{mydataset['instruction'][i]}\"\n",
    "            f\"\\n\\n### Answer:\\n\"\n",
    "        )\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004c9d4-b7bd-4d7c-919a-3bb04c53f939",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "def formatting_prompts_response_template_no_response(mydataset):\n",
    "    output_texts = []\n",
    "    for i in range(len(mydataset['instruction'])):\n",
    "        text = (\n",
    "            f\"### Question:\\n{mydataset['instruction'][i]}\"\n",
    "            f\"\\n\\n### Answer:\\n\"\n",
    "        )\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c6417-f337-4f16-8007-2a80cc3cf0e9",
   "metadata": {},
   "source": [
    "### Exercise 2: Try with another LLM (EleutherAI/gpt-neo-125m)\n",
    "\n",
    "The EleutherAI/gpt-neo-125m is a smaller variant of the GPT-Neo family of models developed by EleutherAI. With 125 million parameters, it is designed to be computationally efficient while still providing robust performance for various natural language processing tasks.\n",
    "\n",
    "Download and load the `EleutherAI/gpt-neo-125m` model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a68c4-f7f9-444c-a241-8fad6ec8f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here\n",
    "model_name = \"EleutherAI/gpt-neo-125m\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63c6d0-2289-497c-84b7-6154c934c22e",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "model_name = \"EleutherAI/gpt-neo-125m\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d20f9-8ac9-4333-bee4-5aa1206a33f8",
   "metadata": {},
   "source": [
    "Initialize LoRA Configuration:\n",
    "\n",
    "- r: 8 (Low-rank dimension)\n",
    "- lora_alpha: 16 (Scaling factor)\n",
    "- target_modules: [\"q_proj\", \"v_proj\"] (Modules to apply LoRA)\n",
    "- lora_dropout: 0.1 (Dropout rate)\n",
    "- task_type: TaskType.CAUSAL_LM (Task type should be causal language model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63abfa-28c9-4598-b0ae-d69185a06255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    task_type=TaskType.CAUSAL_LM  # Task type should be causal language model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9474a1-f2e5-4bf3-9bda-186ec791bbde",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    task_type=TaskType.CAUSAL_LM  # Task type should be causal language model\n",
    ")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c3e6c-c2fb-4e55-9f40-0ebbc09fe49d",
   "metadata": {},
   "source": [
    "Apply LoRA Configuration to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b3446-b8a7-4f31-988f-299977b019f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c79455-8e14-40bb-b784-463d48d93603",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a50db0-0fe0-47a8-9479-ece0e8a91bf8",
   "metadata": {},
   "source": [
    "## Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac6b56-79a0-4269-8935-04b9be49e866",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "[Wojciech \"Victor\" Fulmyk](https://www.linkedin.com/in/wfulmyk) is a Data Scientist and a PhD Candidate in Economics at the University of Calgary.\n",
    "\n",
    "[Fateme Akbari](https://www.linkedin.com/in/fatemeakbari/) is a Ph.D. candidate in Information Systems at McMaster University with demonstrated research experience in Machine Learning and NLP.\n",
    "\n",
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo) has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "## References\n",
    "\n",
    "[Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/main/en/sft_trainer)\n",
    "\n",
    "[Finetuning To Follow Instructions](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb)\n",
    "\n",
    "[Finetuning with LoRA -- A Hands-On Example](https://lightning.ai/lightning-ai/studios/code-lora-from-scratch)\n",
    "\n",
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393dcaf1-b949-4819-8f57-aef5f97637cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
