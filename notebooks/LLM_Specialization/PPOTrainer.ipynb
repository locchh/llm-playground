{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e099b370-7f9b-42a9-869b-ee3d4e199654",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "# Reinforcement Learning from Human Feedback Using PPO\n",
    "\n",
    "Estimated time needed: **30** minutes\n",
    "\n",
    "\n",
    "Imagine you are an AI engineer who wants to train a \"Happy LLM\" and a \"Pessimistic LLM\" to train customer service agents. You have a reward function trained on the sentiment classifier from the IMDb dataset, and you will now use Reinforcement Learning (RL). RL is a subfield of machine learning where an agent learns to make decisions by performing actions in an environment to maximize a cumulative reward. The agent, in this case, will be the LLM, and the decisions will be about what text to output. Unlike supervised learning, which requires labeled input/output pairs, RL relies on the agent exploring the environment and learning from the feedback it receives in the form of rewards or penalties. This trial-and-error approach enables the agent to improve its decision-making strategy over time.\n",
    "\n",
    "Proximal Policy Optimization (PPO) is one of the most effective and widely used RL algorithms. Introduced by OpenAI, PPO strikes a balance between simplicity and performance, making it a popular choice for training RL agents. PPO optimizes the policy directly and employs mechanisms to ensure the updates are not too drastic, thereby maintaining stability and reliability during training.\n",
    "\n",
    "In this lab, you will be guided through the process of training an RL agent using the PPO algorithm with a focus on sentiment analysis. You will use the IMDb dataset, a large collection of movie reviews, to train your model. By the end of this lab, you will have a solid understanding of how to implement and train an RL agent using PPO, and you will be equipped with practical skills to apply RL techniques to other problems and datasets.\n",
    "This lab is based on [a HF example code titled `Tune GPT2 to generate positive reviews`](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb).\n",
    "\n",
    "\n",
    "\n",
    "![rlhf](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/rlhf.png)\n",
    "\n",
    "\n",
    "This image illustrates the process of **Reinforcement Learning with Human Feedback (RLHF)** applied to fine-tune a language model using a reward model and reinforcement learning techniques, such as Proximal Policy Optimization (PPO). Let me explain its key components step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Prompts Dataset\n",
    "- **Input**: A dataset of prompts $x$, such as \"A dog is...\".\n",
    "- The prompts serve as the starting point for generating model outputs $y$, which are evaluated to improve the model's responses.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Initial Language Model\n",
    "- **Base Model**:\n",
    "  - The pretrained language model generates text $y$ for a given prompt $x$.\n",
    "  - Example output: For the prompt \"A dog is...\", the base model might output \"a furry mammal.\"\n",
    "\n",
    "- **Purpose**:\n",
    "  - This serves as the starting policy $\\pi_{\\text{base}}(y|x)$ before reinforcement learning is applied.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Tuned Language Model (RL Policy)\n",
    "- **RL Fine-Tuning**:\n",
    "  - The tuned language model $\\pi_{\\text{PPO}}(y|x)$ is updated using reinforcement learning. The parameters $\\theta$ are adjusted through an iterative process to improve the model's alignment with human preferences.\n",
    "\n",
    "- **Output**:\n",
    "  - Example tuned response: \"man's best friend,\" which aligns more closely with human expectations.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Reward Model (Preference Model)\n",
    "- **Purpose**:\n",
    "  - The reward model $r_\\theta(y|x)$ evaluates the quality of the outputs $y$ for a given prompt $x$.\n",
    "  - It is trained using human feedback (e.g., rankings, comparisons) to predict which outputs are preferred.\n",
    "\n",
    "- **Input**: \n",
    "  - Text $x, y$ pairs.\n",
    "  - Example: For the prompt \"A dog is...\", the reward model might score \"man's best friend\" higher than \"a furry mammal.\"\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Reinforcement Learning Update\n",
    "- **PPO Update**:\n",
    "  - The policy is updated via **Proximal Policy Optimization (PPO)**, which ensures stable learning by constraining the policy's updates.\n",
    "  - The optimization objective maximizes the expected reward while penalizing large deviations from the base model $\\pi_{\\text{base}}(y|x)$.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. KL Divergence Penalty\n",
    "- **KL Regularization**:\n",
    "  - The term $(-\\lambda_{\\text{KL}} D_{\\text{KL}}(\\pi_{\\text{PPO}}(y|x) \\,||\\, \\pi_{\\text{base}}(y|x))$ penalizes the policy for straying too far from the initial language model's predictions.\n",
    "  - **Why?** This regularization ensures that the fine-tuned model retains fluency and general knowledge from pretraining while still optimizing for the reward model.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Iterative Training Loop\n",
    "- The process is iterative:\n",
    "  1. **Generate Outputs**: The RL-tuned model generates outputs $y$ for prompts $x$.\n",
    "  2. **Evaluate Outputs**: The reward model scores the outputs.\n",
    "  3. **Update Model**: The PPO algorithm updates the model's parameters to maximize reward while staying close to the base model.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Terms in the Formula\n",
    "- **Reward $r_\\theta(y|x)$**:\n",
    "  - Guides the model to produce desirable outputs.\n",
    "  \n",
    "- **KL Divergence $D_{\\text{KL}}$**:\n",
    "  - Keeps the RL policy close to the pretrained base model.\n",
    "\n",
    "- **PPO Update $\\theta \\gets \\theta + \\nabla_\\theta J(\\theta)$**:\n",
    "  - Updates the model using gradients of the objective function.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "This diagram showcases the RLHF process, where:\n",
    "1. A pretrained model is fine-tuned using PPO and a reward model.\n",
    "2. The reward model, trained with human feedback, guides the optimization.\n",
    "3. KL divergence regularization prevents the model from diverging too much from its base behavior, ensuring fluency and safety.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a033d-1843-44f1-bfa6-74231a4d1886",
   "metadata": {},
   "source": [
    "\n",
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "            <li><a href=\"#Defining-helper-functions\">Defining helper functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Initializing-the-PPO-configuration,-model,-and-tokenizer\">Initializing the PPO configuration, model, and tokenizer</a></li>\n",
    "            <li><a href=\"#Dataset-and-dataset-tokenization\">Dataset and dataset tokenization</a></li>\n",
    "            <li><a href=\"#Collator-function\">Collator function</a></li>\n",
    "            <li><a href=\"#Initialize-PPOTrainer\">Initialize PPOTrainer</a></li>\n",
    "            <li><a href=\"#Reward-function\">Reward function</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Generating-responses-using-PPO\">Generating responses using PPO</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Tokenizing-and-preparing-the-input-batch\">Tokenizing and preparing the input batch</a></li>\n",
    "            <li><a href=\"#Scoring-function\">Scoring function</a></li>\n",
    "            <li><a href=\"#Proximal-policy-optimization\">Proximal policy optimization</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Plotting-PPO-training-loss-and-mean\">Plotting PPO training loss and mean</a></li>\n",
    "    <li><a href=\"#Generating-and-analyzing-text-with-PPO-and-reference-models\">Generating and analyzing text with PPO and reference models</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Comparing-PPO-and-reference-models-on\">Comparing PPO and reference models on</a>\n",
    "        <ol>\n",
    "        </ol>\n",
    "    </li>\n",
    "                <li><a href=\"#Running-the-PPO-model-with-negative-sentiment\">Running the PPO model with negative sentiment</a></li>\n",
    "            <li><a href=\"#Comparing-models-with-negative-sentiment\">Comparing models with negative sentiment</a></li>\n",
    "            <li><a href=\"#Exercise:-Comparing-PPO-models\">Exercise: Comparing PPO models</a></li>\n",
    "</ol>\n",
    "\n",
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "- Apply the basics of reinforcement learning and proximal policy optimization (PPO).\n",
    "- Set up the environment and load the IMDb dataset for training.\n",
    "- Define and configure the PPO agent and tokenizer.\n",
    "- Implement the PPO training loop.\n",
    "- Generate and evaluate text responses from the trained model.\n",
    "- Compare the performance of two models on the dataset.\n",
    "- Save and load the trained model for future use.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd837d-3799-414d-b307-400272823a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dfe61b-4c36-42d8-8da2-0939c736a8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01422b1-599a-4ff8-9c18-69683d86fe36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b5be3-a87d-4daa-8082-783c6980800a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ad706-1796-4892-9b5c-c36a1f42810d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2591b41-c38b-41f0-a3f5-d90d9da650db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fefc9-c492-4c7e-94d4-592a10d1dcb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce808773-6d84-4092-8c01-e25f1600ab8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410c8913-fc44-4f06-887b-4c383b4c7b88",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo) has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "[Ashutosh Sagar](https://www.linkedin.com/in/ashutoshsagar/) is completing his MS in CS from Dalhousie University. He has previous experience working with Natural Language Processing and as a Data Scientist.\n",
    "\n",
    "## Contributors\n",
    "\n",
    "[Hailey Quach](https://author.skills.network/instructors/hailey_quach) is a Data Scientist at IBM. She's completing her Bsc, Honors in Computer Science at Concordia University, Montreal.\n",
    "\n",
    "## References\n",
    "\n",
    "[RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)\n",
    "\n",
    "[Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)\n",
    "\n",
    "[TEXT CLASSIFICATION WITH THE TORCHTEXT LIBRARY](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)\n",
    "\n",
    "[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)\n",
    "\n",
    "[Simple, Scalable Adaptation for Neural Machine Translation](https://arxiv.org/pdf/1909.08478)\n",
    "\n",
    "```{|Date (YYYY-MM-DD)|Version|Changed By|Change Description||-|-|-|-||2024-06-27|0.1|Kang Wang|Create the lab|}\n",
    "```\n",
    "\n",
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
