{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e099b370-7f9b-42a9-869b-ee3d4e199654",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "# Reinforcement Learning from Human Feedback Using PPO\n",
    "\n",
    "Estimated time needed: **30** minutes\n",
    "\n",
    "\n",
    "Imagine you are an AI engineer who wants to train a \"Happy LLM\" and a \"Pessimistic LLM\" to train customer service agents. You have a reward function trained on the sentiment classifier from the IMDb dataset, and you will now use Reinforcement Learning (RL). RL is a subfield of machine learning where an agent learns to make decisions by performing actions in an environment to maximize a cumulative reward. The agent, in this case, will be the LLM, and the decisions will be about what text to output. Unlike supervised learning, which requires labeled input/output pairs, RL relies on the agent exploring the environment and learning from the feedback it receives in the form of rewards or penalties. This trial-and-error approach enables the agent to improve its decision-making strategy over time.\n",
    "\n",
    "Proximal Policy Optimization (PPO) is one of the most effective and widely used RL algorithms. Introduced by OpenAI, PPO strikes a balance between simplicity and performance, making it a popular choice for training RL agents. PPO optimizes the policy directly and employs mechanisms to ensure the updates are not too drastic, thereby maintaining stability and reliability during training.\n",
    "\n",
    "In this lab, you will be guided through the process of training an RL agent using the PPO algorithm with a focus on sentiment analysis. You will use the IMDb dataset, a large collection of movie reviews, to train your model. By the end of this lab, you will have a solid understanding of how to implement and train an RL agent using PPO, and you will be equipped with practical skills to apply RL techniques to other problems and datasets.\n",
    "This lab is based on [a HF example code titled `Tune GPT2 to generate positive reviews`](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb).\n",
    "\n",
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "            <li><a href=\"#Defining-helper-functions\">Defining helper functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Initializing-the-PPO-configuration,-model,-and-tokenizer\">Initializing the PPO configuration, model, and tokenizer</a></li>\n",
    "            <li><a href=\"#Dataset-and-dataset-tokenization\">Dataset and dataset tokenization</a></li>\n",
    "            <li><a href=\"#Collator-function\">Collator function</a></li>\n",
    "            <li><a href=\"#Initialize-PPOTrainer\">Initialize PPOTrainer</a></li>\n",
    "            <li><a href=\"#Reward-function\">Reward function</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Generating-responses-using-PPO\">Generating responses using PPO</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Tokenizing-and-preparing-the-input-batch\">Tokenizing and preparing the input batch</a></li>\n",
    "            <li><a href=\"#Scoring-function\">Scoring function</a></li>\n",
    "            <li><a href=\"#Proximal-policy-optimization\">Proximal policy optimization</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Plotting-PPO-training-loss-and-mean\">Plotting PPO training loss and mean</a></li>\n",
    "    <li><a href=\"#Generating-and-analyzing-text-with-PPO-and-reference-models\">Generating and analyzing text with PPO and reference models</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Comparing-PPO-and-reference-models-on\">Comparing PPO and reference models on</a>\n",
    "        <ol>\n",
    "        </ol>\n",
    "    </li>\n",
    "                <li><a href=\"#Running-the-PPO-model-with-negative-sentiment\">Running the PPO model with negative sentiment</a></li>\n",
    "            <li><a href=\"#Comparing-models-with-negative-sentiment\">Comparing models with negative sentiment</a></li>\n",
    "            <li><a href=\"#Exercise:-Comparing-PPO-models\">Exercise: Comparing PPO models</a></li>\n",
    "</ol>\n",
    "\n",
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "- Apply the basics of reinforcement learning and proximal policy optimization (PPO).\n",
    "- Set up the environment and load the IMDb dataset for training.\n",
    "- Define and configure the PPO agent and tokenizer.\n",
    "- Implement the PPO training loop.\n",
    "- Generate and evaluate text responses from the trained model.\n",
    "- Compare the performance of two models on the dataset.\n",
    "- Save and load the trained model for future use.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c29d56-124b-4087-b84a-743cca87fce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd837d-3799-414d-b307-400272823a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2591b41-c38b-41f0-a3f5-d90d9da650db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fefc9-c492-4c7e-94d4-592a10d1dcb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce808773-6d84-4092-8c01-e25f1600ab8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410c8913-fc44-4f06-887b-4c383b4c7b88",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo) has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "[Ashutosh Sagar](https://www.linkedin.com/in/ashutoshsagar/) is completing his MS in CS from Dalhousie University. He has previous experience working with Natural Language Processing and as a Data Scientist.\n",
    "\n",
    "## Contributors\n",
    "\n",
    "[Hailey Quach](https://author.skills.network/instructors/hailey_quach) is a Data Scientist at IBM. She's completing her Bsc, Honors in Computer Science at Concordia University, Montreal.\n",
    "\n",
    "## References\n",
    "\n",
    "[RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)\n",
    "\n",
    "[Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)\n",
    "\n",
    "[TEXT CLASSIFICATION WITH THE TORCHTEXT LIBRARY](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)\n",
    "\n",
    "[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)\n",
    "\n",
    "[Simple, Scalable Adaptation for Neural Machine Translation](https://arxiv.org/pdf/1909.08478)\n",
    "\n",
    "```{|Date (YYYY-MM-DD)|Version|Changed By|Change Description||-|-|-|-||2024-06-27|0.1|Kang Wang|Create the lab|}\n",
    "```\n",
    "\n",
    "Â© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
